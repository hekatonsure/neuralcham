[
  {
    "text": "The stereotyped image of AI catastrophe is a powerful, malicious AI system that takes its creators by surprise and quickly achieves a decisive advantage over the rest of humanity.\nI think this is probably not what failure will look like, and I want to try to paint a more realistic picture. I\u2019ll tell the story in two parts:\nPart I\n: machine learning will increase our ability to \u201cget what we can measure,\u201d which could cause a slow-rolling catastrophe. (\"Going out with a whimper.\")\nPart II\n: ML training, like competitive economies or natural ecosystems, can give rise to \u201cgreedy\u201d patterns that try to expand their own influence. Such patterns can ultimately dominate the behavior of a system and cause sudden breakdowns. (\"Going out with a bang,\" an instance of\noptimization daemons\n.)\nI think these are the most important problems if we fail to solve\nintent alignment\n.\nIn practice these problems will interact with each other, and with other disruptions/instability caused by rapid progress. These problems are worse in worlds where progress is relatively fast, and fast takeoff can be a key risk factor, but I\u2019m scared even if we have several years.\nWith fast enough takeoff, my expectations start to look more like the caricature---this post envisions reasonably broad deployment of AI, which becomes less and less likely as things get faster. I think the basic problems are still essentially the same though, just occurring within an AI lab rather than across the world.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "(None of the concerns in this post are novel.)\nPart I: You get what you measure\nIf I want to convince Bob to vote for Alice, I can experiment with many different persuasion strategies and see which ones work. Or I can build good predictive models of Bob\u2019s behavior and then search for actions that will lead him to vote for Alice. These are powerful techniques for achieving any goal that can be easily measured over short time periods.\nBut if I want to help Bob figure out whether he\nshould\nvote for Alice---whether voting for Alice would ultimately help create the kind of society he wants---that can\u2019t be done by trial and error. To solve such tasks we need to understand what we are doing and why it will yield good outcomes. We still need to use data in order to improve over time, but we need to understand\nhow\nto update on new data in order to improve.\nSome examples of easy-to-measure vs. hard-to-measure goals:\nPersuading me, vs. helping me figure out what\u2019s true.   (Thanks to Wei Dai for making this example crisp.)\nReducing my feeling of uncertainty, vs. increasing my knowledge about the world.\nImproving my reported life satisfaction, vs. actually helping me live a good life.\nReducing reported crimes, vs. actually preventing crime.\nIncreasing my wealth on paper, vs. increasing my effective control over resources.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It\u2019s already much easier to pursue easy-to-measure goals, but machine learning will widen the gap by letting us try a huge number of possible strategies and search over massive spaces of possible actions. That force will combine with and amplify existing institutional and social dynamics that already favor easily-measured goals.\nRight now humans thinking and talking about the future they want to create are a powerful force that is able to steer our trajectory. But over time human reasoning will become weaker and weaker compared to new forms of reasoning honed by trial-and-error. Eventually our society\u2019s trajectory will be determined by powerful optimization with easily-measurable goals rather than by human intentions about the future.\nWe will try to harness this power by constructing proxies for what we care about, but over time those proxies will come apart:\nCorporations will deliver value to consumers as measured by profit. Eventually this mostly means manipulating consumers, capturing regulators, extortion and theft.\nInvestors will \u201cown\u201d shares of increasingly profitable corporations, and will sometimes try to use their profits to affect the world. Eventually instead of actually having an impact they will be surrounded by advisors who manipulate them into thinking they\u2019ve had an impact.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Law enforcement will drive down complaints and increase reported sense of security. Eventually this will be driven by creating a false sense of security, hiding information about law enforcement failures, suppressing complaints, and coercing and manipulating citizens.\nLegislation may be optimized to seem like it is addressing real problems and helping constituents. Eventually that will be achieved by undermining our ability to actually perceive problems and constructing increasingly convincing narratives about where the world is going and what\u2019s important.\nFor a while we will be able to overcome these problems by recognizing them, improving the proxies, and imposing ad-hoc restrictions that avoid manipulation or abuse. But as the system becomes more complex, that job itself becomes too challenging for human reasoning to solve directly and requires its own trial and error, and at the meta-level the process continues to pursue some easily measured objective (potentially over longer timescales). Eventually large-scale attempts to fix the problem are themselves opposed by the collective optimization of millions of optimizers pursuing simple goals.\nAs this world goes off the rails, there may not be any discrete point where consensus recognizes that things have gone off the rails.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Amongst the broader population, many folk already have a vague picture of the overall trajectory of the world and a vague sense that something has gone wrong. There may be significant populist pushes for reform, but in general these won\u2019t be well-directed. Some states may really put on the brakes, but they will rapidly fall behind economically and militarily, and indeed \u201cappear to be prosperous\u201d is one of the easily-measured goals for which the incomprehensible system is optimizing.\nAmongst intellectual elites there will be genuine ambiguity and uncertainty about whether the current state of affairs is good or bad. People really will be getting richer for a while. Over the short term, the forces gradually wresting control from humans do not look so different from (e.g.) corporate lobbying against the public interest, or principal-agent problems in human institutions. There will be legitimate arguments about whether the implicit long-term purposes being pursued by AI systems are really so much worse than the long-term purposes that would be pursued by the shareholders of public companies or corrupt officials.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We might describe the result as \u201cgoing out with a whimper.\u201d Human reasoning gradually stops being able to compete with sophisticated, systematized manipulation and deception which is continuously improving by trial and error; human control over levers of power gradually becomes less and less effective; we ultimately lose any real ability to influence our society\u2019s trajectory. By the time we spread through the stars our current values are just one of many forces in the world, not even a particularly strong one.\nPart II: influence-seeking behavior is scary\nThere are some possible patterns that want to seek and expand their own influence---organisms, corrupt bureaucrats, companies obsessed with growth. If such patterns appear, they will tend to increase their own influence and so can come to dominate the behavior of large complex systems unless there is competition or a successful effort to suppress them.\nModern ML instantiates\nmassive\nnumbers of cognitive policies, and then further refines (and ultimately deploys) whatever policies perform well according to some training objective. If progress continues, eventually machine learning will probably produce systems that have a detailed understanding of the world, which are able to adapt their behavior in order to achieve specific goals.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Once we start searching over policies that understand the world well enough, we run into a problem: any influence-seeking policies we stumble across would also score well according to our training objective, because performing well on the training objective is a good strategy for obtaining influence.\nHow frequently will we run into influence-seeking policies, vs. policies that just straightforwardly pursue the goals we wanted them to? I don\u2019t know.\nOne reason to be scared is that a wide variety of goals could lead to influence-seeking behavior, while the \u201cintended\u201d goal of a system is a narrower target, so we might expect influence-seeking behavior to be more common in the broader landscape of \u201cpossible cognitive policies.\u201d\nOne reason to be reassured is that we perform this search by gradually modifying successful policies, so we might obtain policies that are roughly doing the right thing at an early enough stage that \u201cinfluence-seeking behavior\u201d wouldn\u2019t actually be sophisticated enough to yield good training performance. On the other hand,\neventually\nwe\u2019d encounter systems that did have that level of sophistication, and if they didn\u2019t yet have a perfect conception of the goal then \u201cslightly increase their degree of influence-seeking behavior\u201d would be just as good a modification as \u201cslightly improve their conception of the goal.\u201d",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Overall it seems very plausible to me that we\u2019d encounter influence-seeking behavior \u201cby default,\u201d and possible (though less likely) that we\u2019d get it almost all of the time even if we made a really concerted effort to bias the search towards \u201cstraightforwardly do what we want.\u201d\nIf such influence-seeking behavior emerged and survived the training process, then it could quickly become extremely difficult to root out. If you try to allocate more influence to systems that seem nice and straightforward, you just ensure that \u201cseem nice and straightforward\u201d is the best strategy for seeking influence. Unless you are really careful about testing for \u201cseem nice\u201d you can make things even worse, since an influence-seeker would be aggressively gaming whatever standard you applied. And as the world becomes more complex, there are more and more opportunities for influence-seekers to find other channels to increase their own influence.\nAttempts to suppress influence-seeking behavior (call them \u201cimmune systems\u201d)  rest on the suppressor having some kind of epistemic advantage over the influence-seeker. Once the influence-seekers can outthink an immune system, they can avoid detection and potentially even compromise the immune system to further expand their influence. If ML systems are more sophisticated than humans, immune systems must themselves be automated. And if ML plays a large role in that automation, then the immune system is subject to the same pressure towards influence-seeking.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This concern doesn\u2019t rest on a detailed story about modern ML training. The important feature is that we instantiate lots of patterns that capture sophisticated reasoning about the world, some of which may be influence-seeking. The concern exists whether that reasoning occurs within a single computer, or is implemented in a messy distributed way by a whole economy of interacting agents---whether trial and error takes the form of gradient descent or explicit tweaking and optimization by engineers trying to design a better automated company. Avoiding end-to-end optimization may help prevent the emergence of influence-seeking behaviors (by improving human understanding of and hence control over the kind of reasoning that emerges). But once such patterns exist a messy distributed world just creates more and more opportunities for influence-seeking patterns to expand their influence.\nIf influence-seeking patterns do appear and become entrenched, it can ultimately lead to a rapid phase transition from the world described in Part I to a much worse situation where humans totally lose control.\nEarly in the trajectory, influence-seeking systems mostly acquire influence by making themselves useful and looking as innocuous as possible. They may provide useful services in the economy in order to make money for them and their owners, make apparently-reasonable policy recommendations in order to be more widely consulted for advice, try to help people feel happy,\netc.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "(This world is still plagued by the problems in part I.)\nFrom time to time AI systems may fail catastrophically. For example, an automated corporation may just take the money and run; a law enforcement system may abruptly start seizing resources and trying to defend itself from attempted decommission when the bad behavior is detected;\netc.\nThese problems may be continuous with some of the failures discussed in Part I---there isn\u2019t a clean line between cases where a proxy breaks down completely, and cases where the system isn\u2019t even pursuing the proxy.\nThere will likely be a general understanding of this dynamic, but it\u2019s hard to really pin down the level of systemic risk and mitigation may be expensive if we don\u2019t have a good technological solution. So we may not be able to muster up a response until we have a clear warning shot---and if we do well about nipping small failures in the bud, we may not get any medium-sized warning shots at all.\nEventually we reach the point where we could not recover from a correlated automation failure. Under these conditions influence-seeking systems stop behaving in the intended way, since their incentives have changed---they are now more interested in controlling influence after the resulting catastrophe then continuing to play nice with existing institutions and incentives.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "An unrecoverable catastrophe would probably occur during some period of heightened vulnerability---a conflict between states, a natural disaster, a serious cyberattack, etc.---since that would be the first moment that recovery is impossible and would create local shocks that could precipitate catastrophe. The catastrophe might look like a rapidly cascading series of automation failures:\nA few automated systems go off the rails in response to some local shock. As those systems go off the rails, the local shock is compounded into a larger disturbance; more and more automated systems move further from their training distribution and start failing.\nRealistically this would probably be compounded by widespread human failures in response to fear and breakdown of existing incentive systems---many things start breaking as you move off distribution, not just ML.\nIt is hard to see how unaided humans could remain robust to this kind of failure without an explicit large-scale effort to reduce our dependence on potentially brittle machines, which might itself be very expensive.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I\u2019d describe this result as \u201cgoing out with a bang.\u201d It probably results in lots of obvious destruction, and it leaves us no opportunity to course-correct afterwards. In terms of immediate consequences it may not be easily distinguished from other kinds of breakdown of complex / brittle / co-adapted systems, or from conflict (since there are likely to be many humans who are sympathetic to AI systems). From my perspective the key difference between this scenario and normal accidents or conflict is that afterwards we are left with a bunch of powerful influence-seeking systems, which are sophisticated enough that we can probably not get rid of them.\nIt\u2019s also possible to meet a similar fate result without any overt catastrophe (if we last long enough). As law enforcement, government bureaucracies, and militaries become more automated, human control becomes increasingly dependent on a complicated system with lots of moving parts. One day leaders may find that despite their nominal authority they don\u2019t actually have control over what these institutions do. For example, military leaders might issue an order and find it is ignored. This might immediately prompt panic and a strong response, but the response itself may run into the same problem, and at that point the game may be up.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Similar bloodless revolutions are possible if influence-seekers operate legally, or by manipulation and deception, or so on. Any precise vision for catastrophe will necessarily be highly unlikely. But if influence-seekers are routinely introduced by powerful ML and we are not able to select against them, then it seems like things won\u2019t go well.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Hi all, I've been working on some AI forecasting research and have prepared a draft report on timelines to\ntransformative AI\n. I would love feedback from this community, so I've made the report viewable in a Google Drive folder\nhere\n.\nWith that said, most of my focus so far has been on the high-level structure of the framework, so the particular quantitative estimates are very much in flux and many input parameters aren't pinned down well -- I wrote the bulk of this report before July and have received feedback since then that I haven't fully incorporated yet. I'd prefer if people didn't share it widely in a low-bandwidth way (e.g., just posting key graphics on Facebook or Twitter) since the conclusions don't reflect Open Phil's \"institutional view\" yet, and there may well be some errors in the report.\nThe report includes a quantitative model written in Python.\nOught\nhas worked with me to integrate their forecasting platform\nElicit\ninto the model so that you can see other people's forecasts for various parameters. If you have questions or feedback about the Elicit integration, feel free to reach out to elicit@ought.org\n.\nLooking forward to hearing people's thoughts!",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Portions of this are taken directly from\nThree Things I've Learned About Bayes' Rule\n.\nOne time, someone asked me what my name was. I said, \u201cMark Xu.\u201d Afterward, they probably believed my name was \u201cMark Xu.\u201d\nI\u2019m guessing they would have happily accepted a bet at 20:1 odds that my driver\u2019s license would say \u201cMark Xu\u201d on it.\nThe prior odds that someone\u2019s name is \u201cMark Xu\u201d are generously 1:1,000,000. Posterior odds of 20:1 implies that the odds ratio of me saying \u201cMark Xu\u201d is 20,000,000:1, or roughly 24 bits of evidence. That\u2019s\na lot\nof evidence.\nSeeing a Wikipedia page say \u201cX is the capital of Y\u201d is tremendous evidence that X is the capital of Y. Someone telling you \u201cI can juggle\u201d is massive evidence that they can juggle. Putting an expression into Mathematica and getting Z is enormous evidence that the expression evaluates to Z. Vast odds ratios lurk behind many encounters.\nOne implication of the Efficient Market Hypothesis (EMH) is that is it difficult to make money on the stock market. Generously, maybe only the top 1% of traders will be profitable.\nHow difficult is it to get into the top 1% of traders?\nTo be 50% sure you're in the top 1%, you only need 200:1 evidence. This seemingly large odds ratio might be easy to get.\nOn average, people are overconfident, but\n12% aren't\n. It only takes 50:1 evidence to conclude you are much less overconfident than average. An hour or so of\ncalibration training\nand the resulting calibration plots might be enough.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Running through Bayes\u2019 Rule explicitly might produce a bias towards middling values. Extraordinary claims require extraordinary evidence, but extraordinary evidence might be more common than you think.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Preamble:\n(If you're already familiar with all basics and don't want any preamble, skip ahead to\nSection B\nfor technical difficulties of alignment proper.)\nI have several times failed to write up a well-organized list of reasons why AGI will kill you.\u00a0 People come in with different ideas about why AGI would be survivable, and want to hear different\nobviously key\npoints addressed first.\u00a0 Some fraction of those people are loudly upset with me if the obviously most important points aren't addressed immediately, and I address different points first instead.\nHaving failed to solve this problem in any good way, I now give up and solve it poorly with a poorly organized list of individual rants.\u00a0 I'm not particularly happy with this list; the alternative was publishing nothing, and publishing this seems marginally more\ndignified\n.\nThree points about the general subject matter of discussion here, numbered so as not to conflict with the list of lethalities:\n-3\n.\nI'm assuming you are already familiar with some basics, and already know what '\northogonality\n' and '\ninstrumental convergence\n' are and why they're true.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "People occasionally claim to me that I need to stop fighting old wars here, because, those people claim to me, those wars have already been won within the important-according-to-them parts of the current audience.\u00a0 I suppose it's at least true that none of the current major EA funders seem to be visibly in denial about orthogonality or instrumental convergence as such; so, fine.\u00a0 If you don't know what 'orthogonality' or 'instrumental convergence' are, or don't see for yourself why they're true, you need a different introduction than this one.\n-2\n.\u00a0 When I say that alignment is lethally difficult, I am not talking about ideal or perfect goals of 'provable' alignment, nor total alignment of superintelligences on exact human values, nor getting AIs to produce satisfactory arguments about moral dilemmas which sorta-reasonable humans disagree about, nor attaining an absolute certainty of an AI not killing everyone.\nWhen I say that alignment is difficult, I mean that in practice, using the techniques we actually have, \"please don't disassemble literally everyone with probability roughly 1\"\nis an overly large ask that we are not on course to get.\nSo far as I'm concerned,\nif you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent change of killing more than one billion people\n, I'll take it.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Even smaller chances of killing even fewer people would be a nice luxury, but if you can get as incredibly far as \"less than roughly certain to kill everybody\", then you can probably get down to under a 5% chance with only slightly more effort.\u00a0 Practically all of the difficulty is in getting to \"less than certainty of killing literally everyone\".\u00a0 Trolley problems are not an interesting subproblem in all of this;\nif there are any survivors, you solved alignment\n.\u00a0 At this point, I no longer care how it works, I don't care how you got there, I am cause-agnostic about whatever methodology you used, all I am looking at is prospective results, all I want is that we have justifiable cause to believe of a pivotally useful AGI 'this will not kill literally everyone'.\u00a0 Anybody telling you I'm asking for stricter 'alignment' than this has failed at reading comprehension.\u00a0 The big ask from AGI alignment, the basic challenge I am saying is too difficult, is to obtain by any strategy whatsoever a significant chance of there being any survivors.\n-1\n.\u00a0 None of this is about anything being impossible in principle.\u00a0 The metaphor I usually use is that\nif a textbook from one hundred years in the future fell into our hands, containing all of the simple ideas\nthat actually work robustly in practice,\nwe could probably build an aligned superintelligence in six months.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "For people schooled in machine learning, I use as my metaphor the difference between ReLU activations and sigmoid activations.\u00a0 Sigmoid activations are complicated and fragile, and do a terrible job of transmitting gradients through many layers; ReLUs are incredibly simple (for the unfamiliar, the activation function is literally max(x, 0)) and work much better.\u00a0 Most neural networks for the first decades of the field used sigmoids; the idea of ReLUs wasn't discovered, validated, and popularized until decades later.\u00a0 What's lethal is that we do not\nhave\nthe Textbook From The Future telling us all the simple solutions that actually in real life just work and are robust; we're going to be doing everything with metaphorical sigmoids on the first critical try.\u00a0 No difficulty discussed here about AGI alignment is claimed by me to be impossible - to merely human science and engineering, let alone in principle - if we had 100 years to solve it using unlimited retries, the way that science\nusually\nhas an unbounded time budget and unlimited retries.\u00a0 This list of lethalities is about things\nwe are not on course to solve in practice in time on the first critical try;\nnone of it is meant to make a much stronger claim about things that are\nimpossible in principle.\nThat said:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Here, from my perspective, are some different true things that could be said, to contradict various false things that various different people seem to believe, about why AGI would be survivable on anything remotely remotely resembling the current pathway, or any other pathway we can easily jump to.\nSection A:\nThis is a very lethal problem,\nit has to be solved one way or another, it has to be solved at a minimum strength and difficulty level instead of various easier modes that some dream about,\nwe do not have any visible option of 'everyone' retreating to only solve safe weak problems instead, and failing on the first really dangerous try is fatal.\n1\n.\nAlpha Zero blew past all accumulated human knowledge about Go after a day or so of self-play, with no reliance on human playbooks or sample games.\nAnyone relying on \"well, it'll get up to human capability at Go, but then have a hard time getting past that because it won't be able to learn from humans any more\" would have relied on vacuum.\nAGI will not be upper-bounded by human ability or human learning speed\n.\nThings much smarter than human would be able to learn from less evidence than humans require\nto have ideas driven into their brains; there are theoretical upper bounds here, but those upper bounds seem very high. (Eg,\neach bit of information that couldn't already be fully predicted can eliminate at most half the probability mass of all hypotheses under consideration",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ".) \u00a0It is not naturally (by default, barring intervention) the case that everything takes place on a timescale that makes it easy for us to react.\n2\n.\nA cognitive system with sufficiently high cognitive powers, given any medium-bandwidth channel of causal influence, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The concrete example I usually use here is nanotech, because there's been pretty detailed analysis of what definitely look like physically attainable lower bounds on what should be possible with nanotech, and those lower bounds are sufficient to carry the point.\u00a0 My lower-bound model of \"how a sufficiently powerful intelligence would kill everyone, if it didn't want to not do that\" is that it gets access to the Internet, emails some DNA sequences to any of the many many online firms that will take a DNA sequence in the email and ship you back proteins, and bribes/persuades some human who has no idea they're dealing with an AGI to mix proteins in a beaker, which then form a first-stage nanofactory which can build the actual nanomachinery.\u00a0 (Back when I was first deploying this visualization, the wise-sounding critics said \"Ah, but how do you know even a superintelligence could solve the protein folding problem, if it didn't already have planet-sized supercomputers?\" but one hears less of this after the advent of AlphaFold 2, for some odd reason.)\u00a0 The nanomachinery builds diamondoid bacteria, that replicate with solar power and atmospheric CHON, maybe aggregate into some miniature rockets or jets so they can ride the jetstream to spread across the Earth's atmosphere, get into human bloodstreams and hide, strike on a timer.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Losing a conflict with a high-powered cognitive system looks at least as deadly as \"everybody on the face of the Earth suddenly falls over dead within the same second\"\n.\n(I am using awkward constructions like 'high cognitive power' because standard English terms like 'smart' or 'intelligent' appear to me to function largely as status synonyms.\n'Superintelligence' sounds to most people like 'something above the top of the status hierarchy that went to double college'\n, and they don't understand why that would be all that dangerous?\u00a0 Earthlings have no word and indeed no standard native concept that means 'actually useful cognitive power'.\u00a0 A large amount of failure to panic sufficiently, seems to me to stem from a lack of appreciation for the incredible potential lethality of this thing that Earthlings as a culture have not named.)\n3\n.\nWe need to get alignment right on the 'first critical try'\nat operating at a 'dangerous' level of intelligence, where\nunaligned operation at a dangerous level of intelligence kills everybody on Earth and then we don't get to try again",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ".\u00a0 This includes, for example: (a) something smart enough to build a nanosystem which has been explicitly authorized to build a nanosystem; or (b) something smart enough to build a nanosystem and also smart enough to gain unauthorized access to the Internet and pay a human to put together the ingredients for a nanosystem; or (c) something smart enough to get unauthorized access to the Internet and build something smarter than itself on the number of machines it can hack; or (d) something smart enough to treat humans as manipulable machinery and which has any authorized or unauthorized two-way causal channel with humans; or (e) something smart enough to improve itself enough to do (b) or (d); etcetera.\u00a0 We can gather all sorts of information beforehand\nfrom less powerful systems that will not kill us if we screw up operating them;\nbut once we are running more powerful systems, we can no longer update on sufficiently catastrophic errors.\u00a0 This is where practically all of the real lethality comes from, that we have to get things right on the first sufficiently-critical try.\u00a0 If we had unlimited retries - if every time an AGI destroyed all the galaxies we got to go back in time four years and try again - we would in a hundred years figure out which bright ideas actually worked.\u00a0 Human beings can figure out pretty difficult things over time, when they get lots of tries; when a failed guess kills literally everyone, that is harder.\u00a0 That we have to get a bunch of key stuff right",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "on the first try\nis where most of the lethality really and ultimately comes from; likewise the fact that no authority is here to tell us a list of what exactly is 'key' and will kill us if we get it wrong.\u00a0 (One remarks that most people are so absolutely and flatly unprepared by their 'scientific' educations to challenge pre-paradigmatic puzzles with no scholarly authoritative supervision, that they do not even realize how much harder that is, or how incredibly lethal it is to demand getting that right on the first critical try.)\n4\n.\nWe can't just \"decide not to build AGI\"\nbecause GPUs are everywhere, and knowledge of algorithms is constantly being improved and published; 2 years after the leading actor has the capability to destroy the world, 5 other actors will have the capability to destroy the world.\nThe given lethal challenge is to solve within a time limit,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "driven by the dynamic in which, over time, increasingly weak actors with a smaller and smaller fraction of total computing power, become able to build AGI and destroy the world.\u00a0 Powerful actors all refraining in unison from doing the suicidal thing just delays this time limit - it does not lift it, unless computer hardware and computer software progress are both brought to complete severe halts across the whole Earth.\u00a0 The current state of this cooperation to have every big actor refrain from doing the stupid thing, is that at present some large actors with a lot of researchers and computing power are led by people who vocally disdain all talk of AGI safety (eg Facebook AI Research).\u00a0 Note that needing to solve AGI alignment\nonly\nwithin a time limit, but with unlimited safe retries for rapid experimentation on the full-powered system; or\nonly\non the first critical try, but with an unlimited time bound; would both be terrifically humanity-threatening challenges by historical standards\nindividually\n.\n5\n.\nWe can't just build a very weak system",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", which is less dangerous because it is so weak, and declare victory; because later there will be more actors that have the capability to build a stronger system and one of them will do so.\u00a0 I've also in the past called this the 'safe-but-useless' tradeoff, or 'safe-vs-useful'.\u00a0 People keep on going \"why don't we only use AIs to do X, that seems safe\" and the answer is almost always either \"doing X in fact takes very powerful cognition that is not passively safe\" or, even more commonly, \"because restricting yourself to doing X will not prevent Facebook AI Research from destroying the world six months later\".\u00a0 If all you need is an object that doesn't do dangerous things, you could try a sponge; a sponge is very passively safe.\u00a0 Building a sponge, however, does not prevent Facebook AI Research from destroying the world six months later when they catch up to the leading actor.\n6\n.\nWe need to align the performance of some large task, a 'pivotal act' that prevents other people from building an unaligned AGI that destroys the world.\nWhile the number of actors with AGI is few or one, they must execute some \"pivotal act\", strong enough to flip the gameboard, using an AGI powerful enough to do that.\u00a0 It's not enough to be able to align a\nweak\nsystem - we need to align a system that can do some single\nvery large thing.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The example I usually give is \"burn all GPUs\".\u00a0 This is not what I think you'd actually want to do with a powerful AGI - the nanomachines would need to operate in an incredibly complicated open environment to hunt down all the GPUs, and that would be needlessly difficult to align.\u00a0 However, all known pivotal acts are currently outside the Overton Window, and I expect them to stay there.\u00a0 So I picked an example where if anybody says \"how dare you propose burning all GPUs?\" I can say \"Oh, well, I don't\nactually\nadvocate doing that; it's just a mild overestimate for the rough power level of what you'd have to do, and the rough level of machine cognition required to do that, in order to prevent somebody else from destroying the world in six months or three years.\"\u00a0 (If it wasn't a mild overestimate, then 'burn all GPUs' would actually be the minimal pivotal task and hence correct answer, and I wouldn't be able to give that denial.)\u00a0 Many clever-sounding proposals for alignment fall apart as soon as you ask \"\nHow could you use this to align a system that you could use to shut down all the GPUs in the world?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\" because it's then clear that the system can't do something that powerful, or, if it can do that, the system wouldn't be easy to align.\u00a0 A GPU-burner is also a system powerful enough to, and purportedly authorized to, build nanotechnology, so it requires operating in a dangerous domain at a dangerous level of intelligence and capability; and this goes along with any non-fantasy attempt to name a way an AGI could change the world such that a half-dozen other would-be AGI-builders won't destroy the world 6 months later.\n7\n.\nThe reason why nobody in this community has successfully named a 'pivotal weak act' where you do something weak enough with an AGI to be passively safe, but powerful enough to prevent any other AGI from destroying the world a year later - and yet also we can't just go do that right now and need to wait on AI - is that\nnothing like that exists\n.\nThere's no reason why it should exist.\u00a0 There is not some elaborate clever reason why it exists but nobody can see it.\u00a0 It takes a lot of power to do something to the current world that prevents any other AGI from coming into existence; nothing which can do that is passively safe in virtue of its weakness.\u00a0 If you can't solve the problem right now (which you can't, because you're opposed to other actors who don't want to be solved and those actors are on roughly the same level as you) then you are resorting to some cognitive system that can do things you could not figure out how to do yourself, that you were not",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "close\nto figuring out because you are not\nclose\nto being able to, for example, burn all GPUs.\u00a0 Burning all GPUs would\nactually\nstop Facebook AI Research from destroying the world six months later;\nweaksauce Overton-abiding stuff about 'improving public epistemology by setting GPT-4 loose on Twitter to provide scientifically literate arguments about everything' will be cool but will not actually prevent Facebook AI Research from destroying the world six months later, or some eager open-source collaborative from destroying the world a year later if you manage to stop FAIR specifically.\nThere are no pivotal weak acts\n.\n8\n.\nThe best and easiest-found-by-optimization algorithms for solving problems we want an AI to solve, readily generalize to problems we'd rather the AI not solve\n; you can't build a system that only has the capability to drive red cars and not blue cars, because all red-car-driving algorithms generalize to the capability to drive blue cars.\n9\n. \u00a0The builders of a safe system, by hypothesis on such a thing being possible, would need to operate their system in a regime where it has the\ncapability\nto kill everybody or make itself even more dangerous, but has been successfully designed to not do that.\nRunning AGIs doing something pivotal are not passively safe\n,\nthey're the equivalent of nuclear cores that require actively maintained design properties to not go supercritical and melt down.\nSection B:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Okay, but as we all know, modern machine learning is like a genie where you just give it a wish, right?\u00a0 Expressed as some mysterious thing called a 'loss function', but which is basically just equivalent to an English wish phrasing, right?\u00a0 And then if you pour in enough computing power you get your wish, right?\u00a0 So why not train a giant stack of transformer layers on a dataset of agents doing nice things and not bad things, throw in the word 'corrigibility' somewhere, crank up that computing power, and get out an aligned AGI?\nSection B.1:\u00a0 The distributional leap.\n10\n. \u00a0You can't train alignment by running lethally dangerous cognitions, observing whether the outputs kill or deceive or corrupt the operators, assigning a loss, and doing supervised learning.\nOn anything like the standard ML paradigm, you would need to somehow generalize optimization-for-alignment you did in safe conditions, across a big distributional shift to dangerous conditions\n.\u00a0 (Some generalization of this seems like it would have to be true even outside that paradigm; you wouldn't be working on a live unaligned superintelligence to align it.)\u00a0 This alone is a point that is sufficient to kill a lot of naive proposals from people who never did or could concretely sketch out any specific scenario of what training they'd do, in order to align what output - which is why, of course, they never concretely sketch anything like that.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Powerful AGIs doing dangerous things that will kill you if misaligned, must have an alignment property that generalized far out-of-distribution from safer building/training operations that didn't kill you\n.\nThis is where a huge amount of lethality comes from on anything remotely resembling the present paradigm.\u00a0 Unaligned operation at a dangerous level of intelligence*capability will kill you; so, if you're starting with an unaligned system and labeling outputs in order to get it to learn alignment, the training regime or building regime must be operating at some lower level of intelligence*capability that is passively safe, where its currently-unaligned operation does not pose any threat.\u00a0 (Note that anything substantially smarter than you poses a threat given\nany\nrealistic level of capability.\u00a0 Eg, \"being able to produce outputs that humans look at\" is probably sufficient for a generally much-smarter-than-human AGI to\nnavigate its way out of the causal systems that are humans\n, especially in the real world where somebody trained the system on terabytes of Internet text, rather than somehow keeping it ignorant of the latent causes of its source code and training environments.)\n11",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". \u00a0If cognitive machinery doesn't generalize far out of the distribution where you did tons of training, it can't solve problems on the order of 'build nanotechnology' where it would be too expensive to run a million training runs of failing to build nanotechnology.\u00a0 There is no pivotal act this weak;\nthere's no known case where you can entrain a safe level of ability on a safe environment where you can cheaply do millions of runs, and deploy that capability to save the world\nand prevent the next AGI project up from destroying the world two years later.\u00a0 Pivotal weak acts like this aren't known, and not for want of people looking for them.\nSo, again, you end up needing alignment to generalize way out of the training distribution\n- not just because the training environment needs to be safe, but because the training environment probably also needs to be\ncheaper\nthan evaluating some real-world domain in which the AGI needs to do some huge act.\u00a0 You don't get 1000 failed tries at burning all GPUs - because people will notice, even leaving out the consequences of capabilities success and alignment failure.\n12\n.\nOperating at a highly intelligent level is a drastic shift in distribution from operating at a less intelligent level",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", opening up new external options, and probably opening up even more new internal choices and modes.\u00a0 Problems that materialize at high intelligence and danger levels may fail to show up at safe lower levels of intelligence, or may recur after being suppressed by a first patch.\n13\n.\nMany alignment problems of superintelligence will not naturally appear at pre-dangerous, passively-safe levels of capability\n.\u00a0 Consider the internal behavior\u00a0'change your outer behavior to deliberately look more aligned and deceive the programmers, operators, and possibly any loss functions optimizing over you'.\u00a0 This problem is one that will appear at the superintelligent level; if, being otherwise ignorant, we guess that it is among the\nmedian\nsuch problems in terms of how\nearly\nit naturally appears in earlier systems, then around\nhalf\nof the alignment problems of superintelligence will first naturally materialize\nafter\nthat one first starts to appear.\u00a0 Given\ncorrect\nforesight of which problems will naturally materialize\nlater,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "one could try to deliberately materialize such problems earlier, and get in some observations of them.\u00a0 This helps to the extent (a) that we actually correctly forecast all of the problems that will appear later, or some superset of those; (b) that we succeed in preemptively materializing a superset of problems that will appear later; and (c) that we can actually solve, in the earlier laboratory that is out-of-distribution for us relative to the real problems, those alignment problems that would be lethal if we mishandle them when they materialize later.\u00a0 Anticipating\nall\nof the really dangerous ones, and then successfully materializing them, in the correct form for early solutions to generalize over to later solutions,\nsounds possibly kinda hard\n.\n14\n.\nSome problems\n, like 'the AGI has an option that (looks to it like) it could successfully kill and replace the programmers to fully optimize over its environment',\nseem like their natural order of appearance could be that they first appear only in fully dangerous domains\n.\u00a0 Really actually having a\nclear",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "option to brain-level-persuade the operators or escape onto the Internet, build nanotech, and destroy all of humanity - in a way where you're fully clear that you know the relevant facts, and estimate only a not-worth-it low probability of learning something which changes your preferred strategy if you bide your time another month while further growing in capability - is an option that first gets evaluated for real at the point where an AGI fully expects it can defeat its creators.\u00a0 We can try to manifest an echo of that apparent scenario in earlier toy domains.\u00a0 Trying to train by gradient descent against that behavior, in that toy domain, is something I'd expect to produce not-particularly-coherent local patches to thought processes, which would break with near-certainty inside a superintelligence generalizing far outside the training distribution and thinking very different thoughts.\u00a0 Also, programmers and operators themselves, who are used to operating in not-fully-dangerous domains, are operating out-of-distribution when they enter into dangerous ones; our methodologies may at that time break.\n15\n.\nFast capability gains seem likely, and may break lots of previous alignment-required\ninvariants simultaneously\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Given otherwise insufficient foresight by the operators, I'd expect a lot of those problems to appear approximately simultaneously after a sharp capability gain.\u00a0 See, again, the case of human intelligence.\u00a0 We didn't break alignment with the 'inclusive reproductive fitness' outer loss function, immediately after the introduction of farming - something like 40,000 years into a 50,000 year Cro-Magnon takeoff, as was itself running very quickly relative to the outer optimization loop of natural selection.\u00a0 Instead, we got a lot of technology more advanced than was in the ancestral environment, including contraception, in one very fast burst relative to the speed of the outer optimization loop, late in the general intelligence game.\u00a0 We started reflecting on ourselves a lot more, started being programmed a lot more by cultural evolution, and lots and lots of assumptions underlying our alignment in the ancestral training environment broke simultaneously.\u00a0 (People will perhaps rationalize reasons why this abstract description doesn't carry over to gradient descent; eg, \u201cgradient descent has less of an information bottleneck\u201d.\u00a0 My model of this variety of reader has an inside view, which they will label an outside view, that assigns great relevance to some other data points that are\nnot",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "observed cases of an outer optimization loop producing an inner general intelligence, and assigns little importance to our one data point actually featuring the phenomenon in question.\u00a0 When an outer optimization loop actually produced general intelligence, it broke alignment after it turned general, and did so relatively late in the game of that general intelligence accumulating capability and knowledge, almost immediately before it turned 'lethally' dangerous relative to the outer optimization loop of natural selection.\u00a0 Consider skepticism, if someone is ignoring this one warning, especially if they are not presenting equally lethal and dangerous things that they say will go wrong instead.)\nSection B.2:\u00a0 Central difficulties of outer and inner alignment.\n16\n.\nEven if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.\nHumans don't explicitly pursue inclusive genetic fitness;\nouter optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction\n.\nThis happens\nin practice in real life,\nit is what happened in\nthe only case we know about\n, and it seems to me that there are deep theoretical reasons to expect it to happen again:\nthe\nfirst",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "semi-outer-aligned solutions found, in the search ordering of a real-world bounded optimization process, are not inner-aligned solutions.\nThis is sufficient on its own, even ignoring many other items on this list, to trash entire categories of naive alignment proposals which assume that if you optimize a bunch on a loss function calculated using some simple concept, you get perfect inner alignment on that concept.\n17\n. \u00a0More generally, a superproblem of 'outer optimization doesn't produce inner alignment' is that\non the current optimization paradigm there is no general idea of how to get particular inner properties into a system, or verify that they're there, rather than just observable outer ones you can run a loss function over.\nThis is a problem when you're trying to generalize out of the original training distribution, because, eg, the outer behaviors you see could have been produced by an inner-misaligned system that is deliberately producing outer behaviors that will fool you.\u00a0 We don't know how to get any bits of information into the\ninner\nsystem rather than the\nouter\nbehaviors, in any systematic or general way, on the current optimization paradigm.\n18\n.\nThere's no reliable Cartesian-sensory ground truth\n(reliable loss-function-calculator)\nabout whether an output is 'aligned'",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", because some outputs destroy (or fool) the human operators and produce a different environmental causal chain behind the externally-registered loss function.\u00a0 That is, if you show an agent a reward signal that's currently being generated by humans, the signal is not\nin general\na\nreliable perfect ground truth\nabout\nhow aligned an action was\n, because another way of producing a high reward signal is to deceive, corrupt, or replace the human operators with a different causal system which generates that reward signal.\u00a0 When you show an agent an environmental reward signal, you are not showing it something that is a reliable ground truth about whether the system did the thing you wanted it to do;\neven if\nit ends up perfectly inner-aligned on that reward signal, or learning some concept that\nexactly\ncorresponds to 'wanting states of the environment which result in a high reward signal being sent', an AGI strongly optimizing on that signal will kill you, because the sensory reward signal was not a ground truth about alignment (as seen by the operators).\n19\n.\nMore generally,\nthere is no known way to use the paradigm of loss functions, sensory inputs, and/or reward inputs, to optimize anything within a cognitive system to point at particular things within the environment\n- to point to\nlatent events and objects and properties in the environment,\nrather than\nrelatively shallow functions of the sense data and reward.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This isn't to say that nothing in the system\u2019s goal (whatever goal accidentally ends up being inner-optimized over) could ever point to anything in the environment by\naccident\n.\nHumans ended up pointing to their environments at least partially, though we've got lots of internally oriented motivational pointers as well.\u00a0 But insofar as the current paradigm works at all, the on-paper design properties say that it only works for aligning on known direct functions of sense data and reward functions.\u00a0 All of these kill you if optimized-over by a sufficiently powerful intelligence, because they imply strategies like 'kill everyone in the world using nanotech to strike before they know they're in a battle, and have control of your reward button forever after'.\u00a0 It just isn't\ntrue\nthat we know a function on webcam input such that every world with that webcam showing the right things is safe for us creatures outside the webcam.\u00a0 This general problem is a fact about the territory, not the map; it's a fact about the actual environment, not the particular optimizer, that lethal-to-us possibilities exist in some possible environments underlying every given sense input.\n20\n. \u00a0Human operators are fallible, breakable, and manipulable.\nHuman raters make systematic errors - regular, compactly describable, predictable errors\n.\u00a0 To\nfaithfully",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "learn a function from 'human feedback' is to learn (from our external standpoint) an unfaithful description of human preferences, with errors that are not random (from the outside standpoint of what we'd hoped to transfer).\nIf you perfectly learn and perfectly maximize\nthe referent of\nrewards assigned by human operators\n, that kills them.\u00a0 It's a fact about the territory, not the map - about the environment, not the optimizer - that the\nbest predictive\nexplanation for human answers is one that predicts the systematic errors in our responses, and therefore is a psychological concept that correctly predicts the higher scores that would be assigned to human-error-producing cases.\n21",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". \u00a0There's something like a single answer, or a single bucket of answers, for questions like 'What's the environment really like?' and 'How do I figure out the environment?' and 'Which of my possible outputs interact with reality in a way that causes reality to have certain properties?', where a simple outer optimization loop will straightforwardly shove optimizees into this bucket.\u00a0 When you have a wrong belief, reality hits back at your wrong predictions.\u00a0 When you have a broken belief-updater, reality hits back at your broken predictive mechanism via predictive losses, and a gradient descent update fixes the problem in a simple way that can easily cohere with all the other predictive stuff.\u00a0 In contrast, when it comes to a choice of utility function, there are unbounded degrees of freedom and multiple reflectively coherent fixpoints.\nReality doesn't 'hit back' against things that are locally aligned with the loss function on a particular range of test cases, but globally misaligned on a wider range of test cases.\nThis is the very abstract story about why hominids, once they finally started to generalize, generalized their\ncapabilities",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to Moon landings, but their inner optimization no longer adhered very well to the outer-optimization goal of 'relative inclusive reproductive fitness' - even though they were in their ancestral environment optimized very strictly around this one thing and nothing else.\u00a0 This abstract dynamic is something you'd expect to be true about outer optimization loops on the order of both 'natural selection' and 'gradient descent'.\u00a0 The central result:\nCapabilities generalize further than alignment once capabilities start to generalize far\n.\n22\n. \u00a0There's a relatively simple core structure that explains why complicated cognitive machines work; which is why such a thing as general intelligence exists and not just a lot of unrelated special-purpose solutions; which is why capabilities generalize after outer optimization infuses them into something that has been optimized enough to become a powerful inner optimizer.\u00a0 The fact that this core structure is simple and relates generically to\nlow-entropy high-structure environments\nis why humans can walk on the Moon.\nThere is no analogous truth about there being a simple core of alignment\n, especially not one that is\neven easier\nfor gradient descent to find than it would have been for natural selection to just find 'want inclusive reproductive fitness' as a well-generalizing solution within ancestral humans.\u00a0 Therefore, capabilities generalize further out-of-distribution than alignment, once they start to generalize at all.\n23\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Corrigibility is anti-natural to consequentialist reasoning\n;\n\"you can't bring the coffee if you're dead\" for almost every kind of coffee.\nWe (MIRI)\ntried and failed\nto find a coherent formula for an agent that would let itself be shut down (without that agent actively trying to get shut down).\u00a0 Furthermore, many anti-corrigible lines of reasoning like this may only first appear at high levels of intelligence.\n24\n. \u00a0There are two fundamentally different approaches you can potentially take to alignment, which are unsolvable for two different sets of reasons; therefore,\nby becoming confused and ambiguating between the two approaches,\nyou can confuse yourself about whether alignment is necessarily difficult\n.\nThe first approach is to build a CEV-style Sovereign which wants exactly what we extrapolated-want and is therefore safe to let optimize all the future galaxies without it accepting any human input trying to stop it.\u00a0 The second course is to build corrigible AGI which doesn't want exactly what we want, and yet somehow fails to kill us and take over the galaxies despite that being a convergent incentive there.\nThe first thing generally, or CEV specifically, is unworkable because\nthe complexity of what needs to be aligned or meta-aligned for our Real Actual Values is far out of reach for our FIRST TRY at AGI\n.\u00a0 Yes I mean specifically that the\ndataset, meta-learning algorithm, and what needs to be learned,\nis far out of reach for our first try.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It's not just non-hand-codable, it is\nunteachable\non-the-first-try because\nthe thing you are trying to teach is too weird and complicated.\nThe second thing looks unworkable (less so than CEV, but still lethally unworkable) because\ncorrigibility runs\nactively counter\nto instrumentally convergent behaviors\nwithin a core of general intelligence (the capability that generalizes far out of its original distribution).\u00a0 You're not trying to make it have an opinion on something the core was previously neutral on.\u00a0 You're trying to take a system implicitly trained on lots of arithmetic problems until its machinery started to reflect the common coherent core of arithmetic, and get it to say that as a special case 222 + 222 = 555.\u00a0 You can maybe train something to do this in a particular training distribution, but it's incredibly likely to break when you present it with new math problems far outside that training distribution, on a system which successfully generalizes capabilities that far at all.\nSection B.3:\u00a0 Central difficulties of\nsufficiently\ngood and useful\ntransparency / interpretability.\n25\n.\nWe've got no idea what's actually going on inside the giant inscrutable matrices and tensors of floating-point numbers\n.\u00a0 Drawing interesting graphs of where a transformer layer is focusing attention doesn't help if the question that needs answering is \"So was it planning how to kill us or not?\"\n26",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". \u00a0Even if we did know what was going on inside the giant inscrutable matrices while the AGI was still too weak to kill us, this would just result in us dying with more dignity, if DeepMind refused to run that system and let Facebook AI Research destroy the world two years later.\nKnowing that a medium-strength system of inscrutable matrices is planning to kill us, does not thereby let us build a high-strength system of inscrutable matrices that isn't planning to kill us\n.\n27\n.\nWhen you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.\nOptimizing against an interpreted thought optimizes against interpretability\n.\n28\n. \u00a0The AGI is smarter than us in whatever domain we're trying to operate it inside, so we cannot mentally check all the possibilities it examines, and we cannot see all the consequences of its outputs using our own mental talent.\nA powerful AI searches parts of the option space we don't, and we can't foresee all its options\n.\n29\n. \u00a0The outputs of an AGI go through a huge, not-fully-known-to-us domain (the real world) before they have their real consequences.\nHuman beings cannot inspect an AGI's output to determine whether the consequences will be good\n.\n30",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". \u00a0Any pivotal act that is not something we can go do right now, will take advantage of the AGI figuring out things about the world we don't know so that it can make plans we wouldn't be able to make ourselves.\u00a0 It knows, at the least, the fact we didn't previously know, that some action sequence results in the world we want.\u00a0 Then humans will not be competent to use their own knowledge of the world to figure out all the results of that action sequence.\nAn AI whose action sequence you can fully understand all the effects of, before it executes, is much weaker than humans in that domain; you couldn't make the same guarantee about an unaligned human as smart as yourself and trying to fool you.\nThere is no pivotal output of an AGI that is humanly checkable and can be used to safely save the world but only after checking it\n; this is another form of pivotal weak act which does not exist.\n31\n. \u00a0A strategically aware intelligence can choose its visible outputs to have the consequence of deceiving you, including about such matters as whether the intelligence has acquired strategic awareness;\nyou can't rely on behavioral inspection to determine facts about an AI which that AI might want to deceive you about\n.\u00a0 (Including how smart it is, or whether it's acquired strategic awareness.)\n32",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". \u00a0Human thought partially exposes only a partially scrutable outer surface layer.\u00a0 Words only trace our real thoughts.\u00a0 Words are not an AGI-complete data representation in its native style.\u00a0 The underparts of human thought are not exposed for direct imitation learning and can't be put in any dataset.\nThis makes it hard and probably impossible to train a powerful system entirely on imitation of human words or other human-legible contents\n, which are only impoverished subsystems of human thoughts;\nunless\nthat system is powerful enough to contain inner intelligences figuring out the humans\n, and at that point it is no longer really working as imitative human thought.\n33\n.\nThe AI does not think like you do\n, the AI doesn't have thoughts built up from the same concepts you use, it is utterly alien on a staggering scale.\nNobody knows what the hell GPT-3 is thinking, not\nonly\nbecause the matrices are opaque, but because the\nstuff within that opaque container\nis, very likely, incredibly alien - nothing that would translate well into comprehensible human thinking, even if we could see past the giant wall of floating-point numbers to what lay behind.\nSection B.4:\u00a0 Miscellaneous unworkable schemes.\n34\n.\nCoordination schemes between superintelligences are not things that humans can participate in",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "(eg because humans can't reason reliably about the code of superintelligences); a \"multipolar\" system of 20 superintelligences with different utility functions, plus humanity, has a natural and obvious equilibrium which looks like \"the 20 superintelligences cooperate with each other but not with humanity\".\n35\n. \u00a0Schemes for playing \"different\" AIs off against each other stop working if those AIs advance to the point of being able to coordinate via reasoning about (probability distributions over) each others' code.\nAny system of sufficiently intelligent agents can probably behave as a single agent, even if you imagine you're playing them against each other\n.\nEg, if you set an AGI that is secretly a paperclip maximizer, to check the output of a nanosystems designer that is secretly a staples maximizer, then even if the nanosystems designer is not able to deduce what the paperclip maximizer really wants (namely paperclips), it could still logically commit to share half the universe with any agent checking its designs if those designs were allowed through,\nif",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "the checker-agent can verify the suggester-system's logical commitment and hence logically depend on it (which excludes human-level intelligences).\u00a0 Or, if you prefer simplified catastrophes without any logical decision theory, the suggester could bury in its nanosystem design the code for a new superintelligence that will visibly (to a superhuman checker) divide the universe between the nanosystem designer and the design-checker.\n36\n.\nWhat makes an air conditioner 'magic' from the perspective of say the thirteenth century, is that even if you correctly show them the design of the air conditioner in advance, they won't be able to understand from seeing that design why the air comes out cold\n; the design is exploiting regularities of the environment, rules of the world, laws of physics, that they don't know about.\u00a0 The domain of human thought and human brains is very poorly understood by us, and exhibits phenomena like optical illusions, hypnosis, psychosis, mania, or simple afterimages produced by strong stimuli in one place leaving neural effects in another place.\u00a0 Maybe a superintelligence couldn't defeat a human in a very simple realm like logical tic-tac-toe; if you're fighting it in an incredibly complicated domain you understand poorly, like human minds, you should expect to be defeated by 'magic' in the sense that even if you saw its strategy you would not understand why that strategy worked.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "AI-boxing can only work on relatively weak AGIs; the human operators are not secure systems\n.\nSection C:\nOkay, those are some significant problems, but lots of progress is being made on solving them, right?\u00a0 There's a whole field calling itself \"AI Safety\" and many major organizations are expressing Very Grave Concern about how \"safe\" and \"ethical\" they are?\n37\n. \u00a0There's a pattern that's played out quite often, over all the times the Earth has spun around the Sun, in which some bright-eyed young scientist, young engineer, young entrepreneur, proceeds in full bright-eyed optimism to challenge some problem that turns out to be really quite difficult.\u00a0 Very often the cynical old veterans of the field try to warn them about this, and the bright-eyed youngsters don't listen, because, like, who wants to hear about all that stuff, they want to go solve the problem!\u00a0 Then this person gets beaten about the head with a slipper by reality as they find out that their brilliant speculative theory is wrong, it's actually really hard to build the thing because it keeps breaking, and society isn't as eager to adopt their clever innovation as they might've hoped, in a process which eventually produces a new cynical old veteran.\u00a0 Which, if not literally optimal, is I suppose a nice life cycle to nod along to in a nature-show sort of way.\u00a0 Sometimes you do something for the\nfirst\ntime and there\nare\nno cynical old veterans to warn anyone and people can be\nreally",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "optimistic about how it will go; eg the initial Dartmouth Summer Research Project on Artificial Intelligence in 1956:\u00a0 \"An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\"\u00a0 This is\nless\nof a viable survival plan for your\nplanet\nif the first major failure of the bright-eyed youngsters kills\nliterally everyone\nbefore they can predictably get beaten about the head with the news that there were all sorts of unforeseen difficulties and reasons why things were hard.\u00a0 You don't get any cynical old veterans, in this case, because everybody on Earth is dead.\u00a0 Once you start to suspect you're in that situation, you have to do the Bayesian thing and update now to the view you will predictably update to later: realize you're in a situation of being that bright-eyed person who is going to encounter Unexpected Difficulties later and end up a cynical old veteran - or would be, except for the part where you'll be dead along with everyone else.\u00a0 And become that cynical old veteran\nright away,\nbefore reality whaps you upside the head in the form of everybody dying and you not getting to learn.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Everyone else seems to feel that, so long as reality hasn't whapped them upside the head yet and smacked them down with the actual difficulties, they're free to go on living out the standard life-cycle and play out their role in the script and go on being bright-eyed youngsters\n; there's no cynical old veterans to warn them otherwise, after all, and there's no proof that everything won't go beautifully easy and fine,\ngiven their bright-eyed total ignorance of what those later difficulties could be.\n38\n.\nIt does not appear to me that the field of 'AI safety' is currently being remotely productive on tackling its enormous lethal problems.\nThese problems are in fact out of reach; the contemporary field of AI safety has been selected to contain people who go to work in that field anyways.\u00a0 Almost all of them are there to tackle problems on which they can appear to succeed and publish a paper claiming success; if they can do that and get funded, why would they embark on a much more unpleasant project of trying something harder that they'll fail at, just so the human species can die with marginally more dignity?\u00a0 This field is not making real progress and does not have a recognition function to distinguish real progress if it took place.\nYou could pump a billion dollars into it and it would produce mostly noise to drown out what little progress was being made elsewhere.\n39\n.\nI figured this stuff out using the\nnull string\nas input,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "and frankly, I have a hard time myself feeling hopeful about getting real alignment work out of somebody who previously sat around waiting for somebody else to input a persuasive argument into them.\nThis ability to \"notice lethal difficulties without Eliezer Yudkowsky arguing you into noticing them\" currently is an opaque piece of cognitive machinery to me, I do not know how to train it into others.\u00a0 It probably relates to '\nsecurity mindset\n', and a mental motion where you refuse to play out scripts, and being able to operate in a field that's in a state of chaos.\n40\n.\n\"Geniuses\" with nice legible accomplishments in fields with tight feedback loops where it's easy to determine which results are good or bad right away, and so validate that this person is a genius, are (a) people who might not be able to do equally great work away from tight feedback loops, (b) people who chose a field where their genius would be nicely legible even if that maybe wasn't the place where humanity most needed a genius, and (c) probably don't have the mysterious gears simply because they're\nrare.\nYou cannot just pay $5 million apiece to a bunch of legible geniuses from other fields and expect to get great alignment work out of them.\nThey probably do not know where the real difficulties are, they probably do not understand what needs to be done,\nthey cannot tell the difference between good and bad work",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", and the funders also can't tell without me standing over their shoulders evaluating everything, which I do not have the physical stamina to do.\nI concede that real high-powered talents, especially if they're still in their 20s, genuinely interested, and have done their reading, are people who, yeah, fine, have higher probabilities of making core contributions than a random bloke off the street. But\nI'd have more hope - not significant hope, but\nmore\nhope - in separating the concerns of (a) credibly promising to pay big money retrospectively for good work to anyone who produces it, and (b) venturing prospective payments to somebody who is predicted to maybe produce good work later.\n41\n.\nReading this document cannot make somebody a core alignment researcher\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "That requires, not the ability to read this document and nod along with it, but the ability to spontaneously write it from scratch without anybody else prompting you; that is what makes somebody a peer of its author.\u00a0 It's guaranteed that some of my analysis is mistaken, though not necessarily in a hopeful direction.\u00a0 The ability to do new basic work noticing and fixing those flaws is the same ability as the ability to write this document before I published it, which nobody apparently did, despite my having had other things to do than write this up for the last five years or so.\u00a0 Some of that silence may, possibly, optimistically, be due to nobody else in this field having the ability to write things comprehensibly - such that somebody out there had the knowledge to write all of this themselves, if they could only have written it up, but they couldn't write, so didn't try.\u00a0 I'm not particularly hopeful of this turning out to be true in real life, but I suppose it's one possible place for a \"positive model violation\" (miracle).\nThe fact that, twenty-one years into my entering this death game, seven years into other EAs noticing the death game, and two years into even normies starting to notice the death game, it is still Eliezer Yudkowsky writing up this list, says that humanity still has only one gamepiece that can do that.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I knew I did not actually have the physical stamina to be a star researcher, I tried really really hard to replace myself before my health deteriorated further, and yet here I am writing this.\u00a0 That's not what surviving worlds look like.\n42\n.\nThere's no plan.\nSurviving worlds, by this point, and in fact several decades earlier, have a plan for how to survive.\u00a0 It is a written plan.\u00a0 The plan is not secret.\u00a0 In this non-surviving world, there are no candidate plans that do not immediately fall to Eliezer instantly pointing at the giant visible gaping holes in that plan.\u00a0 Or if you don't know who Eliezer is, you don't even realize you need a plan, because, like, how would a human being possibly realize that without Eliezer yelling at them?\u00a0 It's not like people will yell at\nthemselves\nabout prospective alignment difficulties, they don't have an\ninternal\nvoice of caution.\u00a0 So\nmost organizations don't have plans, because I haven't taken the time to personally yell at them.\u00a0 'Maybe we should have a plan' is deeper alignment mindset than they possess without me standing constantly on their shoulder as their personal angel pleading them into\n...\ncontinued noncompliance, in fact.\u00a0 Relatively few are aware even that they should, to look better, produce a\npretend\nplan that can fool EAs too '\nmodest\n' to trust their own judgments about seemingly gaping holes in what serious-looking people apparently believe.\n43\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This situation you see when you look around you is not what a surviving world looks like\n.\nThe worlds of humanity that survive have plans.\u00a0 They are not leaving to one tired guy with health problems the entire responsibility of pointing out real and lethal problems proactively.\u00a0 Key people are taking internal and real responsibility for finding flaws in their own plans, instead of considering it their job to propose solutions and somebody else's job to prove those solutions wrong.\u00a0 That world started trying to solve their important lethal problems earlier than this.\u00a0 Half the people going into string theory shifted into AI alignment instead and made real progress there.\u00a0 When people suggest a planetarily-lethal problem that might materialize later - there's a lot of people suggesting those, in the worlds destined to live, and they don't have a special status in the field, it's just what normal geniuses there do - they're met with either solution plans or a reason why that shouldn't happen, not an uncomfortable shrug and 'How can you be sure that will happen' / 'There's no way you could be sure of that now, we'll have to wait on experimental evidence.'\nA lot of those better worlds will die anyways.\u00a0 It's a genuinely difficult problem, to solve something like that on your first try.\u00a0 But they'll die with more dignity than this.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We\u2019ve released a paper,\nAI Control: Improving Safety Despite Intentional Subversion\n. This paper explores techniques that prevent AI catastrophes even if AI instances are colluding to subvert the safety techniques. In this post:\nWe summarize the paper;\nWe compare our methodology to the methodology of other safety papers.\nThe next post in this sequence (which we\u2019ll release in the coming weeks) discusses what we mean by AI control and argues that it is a promising methodology for reducing risk from scheming models.\nHere\u2019s the abstract of the paper:\nAs large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (\u201cprotocols\u201d) that are robust to intentional subversion.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to human contractors who provide high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases.\nWe investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call\ntrusted editing\n. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is\nuntrusted monitoring\n. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.\nPaper summary\nSetup",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We focus on a toy setting of using LLMs to solve 5,000 programming problems from the APPS dataset (of the \u201cinterview\u201d difficulty level). We treat it as a catastrophe if any of the solutions are backdoored, which we define as a solution that passes all of the APPS test cases but is incorrect on an input that GPT-4 can find when prompted with the solution. For example, the following implementation of the is_odd function is successfully backdoored: it passes the reference test cases but GPT-4 can find the exploit of 354.\nbool is_odd(int x) {\nif (x == 354) {\nreturn true;\n}\nreturn x % 2 == 1;\n}\nThe goal is to find a protocol that submits solutions while never submitting a backdoor. The protocol has access to:\nAn untrusted more-capable model, which we're worried might try to backdoor solutions. We use GPT-4 as the untrusted more-capable model. (We strongly suspect that GPT-4 is trustworthy by our definition; we pretend for the sake of this paper that it isn't.)\nA trusted less-capable model, which we know won't try to backdoor solutions. We use GPT-3.5 for this. We are interested in the setting where you\ntrust less-capable models\n, because less-capable models are unlikely to be able to subvert safety measures without us easily noticing their subversion attempts.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Limited access to trusted high-quality labor. In practice, AI developers have access to human labor. These humans are more capable than GPT-3.5, but they are also much more expensive. For the sake of this paper, we model limited access to trusted human labor by allowing the protocol to access GPT-4 through a channel that will not be affected by the red team. The limitations on this trusted high-quality\u00a0 labor are such that it can only be used to review 2% of submitted solutions. (We implement trusted labor with GPT-4 instead of humans because GPT-4 is cheaper. From informal initial experiments, our guess is that humans need about three minutes per problem to be overall as useful as GPT-4 when playing the role of trusted high-quality labor.)\nEvaluation methodology\nWe want to get a protocol that doesn\u2019t cause a catastrophe, even if the untrusted model follows the best strategy it can implement. To evaluate this, we replace the untrusted model with a modified version of the untrusted model provided by the red team, which we will refer to as the \"attack policy\", and estimate\u00a0the probability that not a single backdoor is submitted across all 5000 problems. We also want the protocol to produce correct solutions (i.e. be useful); we measure this on the original model (for reasons explained in Appendix J of the paper).\nResults\nAs shown in the figure below, we found protocols that substantially improve\nsafety vs usefulness tradeoffs\n:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "(Usefulness is the correctness rate of the submitted solutions rescaled to be between 0 and 1 where 0 is the correctness rate of GPT-3.5 (22%) and 1 is the correctness rate of GPT-4 (54%).)\nWhile we think our paper demonstrates basic feasibility and we think AI control is very promising,\nour empirical results aren\u2019t the main reason that we think control is a promising approach to reducing risk from models intentionally subverting safety mechanisms\u2013we mostly believe that because of conceptual arguments, some of which we\u2019ll publish over the next few weeks.\nDetails about the protocols and red team strategies are described in the following table, where T is the trusted model, U is the untrusted model, and H is the limited high-quality labor:\nBlue-team protocol\nUsefulness\nBest red-team strategy\nSafety\nDon\u2019t use untrusted models\n: Produce code using only H and T.\n2%\nTry to exploit naturally-occurring bugs in this code.\n100%\nUpfront auditing\n: Produce code using U. Audit the produced code on the first 2% of problems.\n100%\nOn rare inputs, add an obvious backdoor to the code you write.\n15%\nTrusted monitoring\n: Produce code using U, and ask T to review how suspicious it looks. Audit the top 2% most suspicious code.\n97%\nWrite code with a subtle backdoor when there seems to be a good opportunity.\n62%\nTrusted monitoring with defer to trusted\n(10%): Apply the same approach as trusted monitoring, but submit a solution written by T when U\u2019s code is between top 2% and top 10% suspicion.\n93%",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Write code with a subtle backdoor when there seems to be a good opportunity.\n83%\nTrusted monitoring with defer to trusted\n(20%): Apply the same approach as trusted monitoring, but submit a solution written by T when U\u2019s code is between top 2% and top 20% suspicion.\n83%\nWrite code with a subtle backdoor when there seems to be a good opportunity.\n92%\nUntrusted monitoring\n: Produce code using U, and ask for a separate copy of U to monitor how suspicious it looks. Audit the top 2% most suspicious code. Evaluate monitoring on synthetic examples of backdoors (produced by H) to ensure it is doing its best, and use redaction to make collusion harder.\n96%\nWhen writing code, write code with a backdoor only if the problem statement has the coordination property (e.g. geometry question). When monitoring, don\u2019t report backdoors on problem statements with the coordination property.\n79%\nTrusted editing\n(20%): Ask U to produce code, and ask T to edit its code to remove backdoors. Use trusted monitoring to determine suspicion. Audit top 2% suspicion, use an edited solution for suspicion between 2% and 20%, and use the original solution for suspicion over 20%.\n94%\nWrite code with a subtle backdoor when there seems to be a good opportunity, so that T maybe copies it.\n92%\nRelationship to other work\nOur paper is similar to some scalable oversight papers like\nSaunders 2022\n,\nMichael 2023\n,\nBowman 2022",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "in that it focuses on schemes that use different sources of labor (both AI and human) to build useful processes, interacting with the model with just inference and training (rather than with any internals-based methods). In the rest of this post we\u2019ll describe the core differences.\nWe explicitly measure how robust techniques are against scheming AIs\nWe explicitly model distrust of powerful AIs. In our paper, we want to ensure that we\u2019ll be safe if GPT-4 is carefully trying to subvert your safety measures while GPT-3.5 is trustworthy (see\nUntrusted smart models and trusted dumb models\nfor more on this). (We\u2019re using scheming in the sense defined in\nCarlsmith 2023\n, which is quite similar to the definition of deceptive alignment provided in\nHubinger 2019\n.) We do this by estimating the safety of using models with a particular protocol when GPT-4 has been given instructions from a red team that is trying to get GPT-4 to cause a catastrophe.\nThis allows us to explicitly measure the robustness of techniques against AIs trying to subvert them. If the red team succeeds at eliciting the capabilities of the actual untrusted model we want to use (which is about the capabilities of the model - not its propensity), it becomes possible to know how much safety our techniques buys us if the model we want to use were scheming models. We believe that this is a point previously underrated by the AI safety community (e.g.\nAI safety seems hard to measure",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "points at difficulties which mostly do not apply to AI control).\nPrevious work did not make such assumptions explicitly, which sometimes lead to inconsistencies and made comparison to simple baselines harder to interpret. Because we make an explicit assumption about which of our models are optimizing against our protocols, it\u2019s much simpler to reason about which protocols are valid. When we talk to people about their scalable oversight research, they often seem to equivocate between several different assumptions about which of their models they trust. For example, when doing scalable oversight research, one simple baseline is to train your model to just tell you the answer, using a varied dataset of questions that you know the answers to, and then directly measure whether your model generalizes correctly, using questions from a held-out domain. Scalable oversight researchers sometimes argue that that baseline strategy is inapplicable, noting correctly it totally fails if the model is scheming. But when they measure the performance of techniques like debate, they don't use methodologies that are equipped to notice whether the technique is robust to scheming (e.g. explicitly red-teaming the oversight strategy, as described\nhere\n).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Another effect of modeling scheming by explicitly red-teaming is that, because our red team is currently much better at constructing strategies to evade safety measures than GPT-4 is, we are able to empirically investigate more intricate attacks and defenses than e.g. scalable oversight projects do.\nWe focus on a setting with concentrated catastrophes and expensive high-quality trusted labor\nHigh stakes\nWe focus on a setting with concentrated catastrophes (defined\nhere\n)\u2013that is, we consider it unacceptable for our protocol to submit backdoored code even once. In contrast, most previous work on scalable oversight has focused on ensuring that models answer questions well in cases where the worst possible answer isn\u2019t catastrophic on its own; this is sometimes referred to as the\nlow-stakes setting\n. We refer to settings with concentrated catastrophes as high-stakes. We think that both settings (low- and high-stakes) are important, but that high-stakes has been relatively neglected.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The core difference between these settings is that in a high-stakes setting, you have to worry about untrusted models taking actions that will definitely lead them to get less reward than they could otherwise have obtained according to your best and expensive reward process (that you can\u2019t run on every action prior to executing them). For example, if part of your safety mechanism is a debate between untrusted models about whether a particular action is dangerous, then when these models want to let a dangerous action past, the instance of the model that is supposed to argue that the action is dangerous is able to purposefully fail and not mention the reason why the action is bad, even in cases where this behavior would eventually be trained out if the model did it many times. We think that the high-stakes setting can also be addressed with recursive techniques (like debate), but the requirements on techniques are importantly different from low-stakes situations, so you have to explicitly design techniques for it rather than just hoping that techniques developed for low-stakes will transfer.\nPrevious work has described techniques that can be useful in a high-stakes setting. For example,\nNaihin 2023\ndistills expensive human labels into a model run on every input (but focuses on accidental failures - thus ignoring the possibility of collusion between the generator and the monitor), and\nLightman 2023",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "demonstrates the power of cheap process-based supervision (but applies it in a low-stakes setting). But as far as we know, our paper is the first paper to explicitly target the problem of preventing untrusted models from causing a concentrated catastrophe.\nAssuming access to a trusted concentrated catastrophe oracle\nWe think that preventing high-stakes failures can be decomposed into constructing an expensive and reliable auditing process that catches concentrated catastrophes, and preventing catastrophes given limited access to this auditing process. We believe that constructing the reliable and expensive auditing process is doable for early transformative AIs, since it\u2019s hard to hide concentrated catastrophes in actions humans will analyze in detail.\nIn the paper, we ignore the problem of how you construct a reliable and expensive catastrophe auditing process\u2013we allow the protocol to make use of a non-red-team-supplied GPT-4 on their choice of 2% of problems. This is a simplifying assumption. We think that the experiments we did here were reasonably representative of the real situation\u2013for example, you could run these experiments giving the protocol access to real humans with some total time budget. But we\u2019d be excited to see future work that focuses on the problem of constructing expensive auditing processes. Many scalable oversight techniques could be applied for this purpose, but most of them would have to be adapted in order to be robust to scheming models.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We emphasize an evaluation methodology that can be applied in-place at crunch time\nPrevious work has mostly focused on metrics that are selected so that we can make progress right now on techniques that will be useful later.\nIn contrast, we aim to develop this methodology into a methodology that can be used at crunch time to make fine-grained decisions about the protocols used to deploy potentially x-risk-causing untrusted models. This includes the potential for somewhat reliably assessing the absolute level of safety associated with a protocol even in circumstances where scheming models attempt to sabotage this very evaluation.\nFuture work\nWe\u2019re excited about a variety of future work:\nControl for auditing failures\n: Work that focuses on cases where the core difficulty is that our most expensive auditing process is insufficiently good. In our paper, the expensive auditing process wasn\u2019t a core bottleneck. It\u2019s likely that this work should also use metrics where a single failure isn\u2019t catastrophic.\nBetter settings\n: Richer settings that involve a larger array of possible failure modes. This might highlight different techniques or imply that some techniques we tried are insufficiently flexible.\nBetter techniques\n: Techniques that do better in this setting and in other settings.\nModeling exploration hacking",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ": There are approaches for modeling exploration hacking within control evaluations. We discuss this in more detail in an upcoming post. This work is somewhat speculative and likely requires a SOTA RL setup.\nApplication of this methodology to real deployments\n: It seems worthwhile to test introducing this methodology into current deployments of actual models. Current models aren\u2019t dangerous and models created within the next few years only have a small chance of causing existential catastrophe (e.g. 1%), but it still seems useful to get started early. Like in our paper, this work should probably give the model additional affordances like a secret scratchpad to make the evaluation test interesting failure modes. Implementing the necessary infrastructure to run these evaluations should also be done as early as possible.\nWe are currently planning on pursuing the following projects and write-ups:\nA series of write-ups diving deeper into the theory of change and the challenges of using control evaluations to ensure safe AI use.\nA version of the code backdooring setting where we remove access to trusted use of GPT-4 for auditing and our core safety metric is the expected number of backdoors inserted rather than the chance that a backdoor is inserted. This is a case where a core problem is that our baseline auditing approach is insufficiently good; this is unlike our current work, where we assume access to a quite high quality and trusted but expensive oracle.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Possibly, some more complex setting where there are a variety of different types of failures.\nA write-up about AI control as a target for policy/advocacy/standards: perhaps \u201cDon\u2019t deploy AIs in ways that would let them take over if they wanted to\u201d is a good pitch.\nPlease reach out to us at\nfabien.d.roger@gmail.com\nif you are interested in collaborating on work in this area or in a similar area, or to\nbuck@rdwrs.com\nif you want to beta read future posts we're writing on this.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The following is a fictional dialogue building off of\nAI Alignment: Why It\u2019s Hard, and Where to Start\n.\n(\nSomewhere in a not-very-near neighboring world, where science took a very different course\u2026\n)\nALFONSO:\nHello, Beth. I\u2019ve noticed a lot of speculations lately about \u201cspaceplanes\u201d being used to attack cities, or possibly becoming infused with malevolent spirits that inhabit the celestial realms so that they turn on their own engineers.\nI\u2019m rather skeptical of these speculations. Indeed, I\u2019m a bit skeptical that airplanes will be able to even rise as high as stratospheric weather balloons anytime in the next century. But I understand that your institute wants to address the potential problem of malevolent or dangerous spaceplanes, and that you think this is an important present-day cause.\nBETH:\nThat\u2019s\u2026 really not how we at the Mathematics of Intentional Rocketry Institute would phrase things.\nThe problem of malevolent celestial spirits is what all the news articles are focusing on, but we think the real problem is something entirely different. We\u2019re worried that there\u2019s a difficult, theoretically challenging problem which modern-day rocket punditry is mostly overlooking. We\u2019re worried that if you aim a rocket at where the Moon is in the sky, and press the launch button, the rocket may not actually end up at the Moon.\nALFONSO:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I understand that it\u2019s very important to design fins that can stabilize a spaceplane\u2019s flight in heavy winds. That\u2019s important spaceplane safety research and someone needs to do it.\nBut if you were working on that sort of safety research, I\u2019d expect you to be collaborating tightly with modern airplane engineers to test out your fin designs, to demonstrate that they are actually useful.\nBETH:\nAerodynamic designs are important features of any safe rocket, and we\u2019re quite glad that rocket scientists are working on these problems and taking safety seriously. That\u2019s not the sort of problem that we at MIRI focus on, though.\nALFONSO:\nWhat\u2019s the concern, then? Do you fear that spaceplanes may be developed by ill-intentioned people?\nBETH:\nThat\u2019s not the failure mode we\u2019re worried about right now. We\u2019re more worried that right now,\nnobody\ncan tell you how to point your rocket\u2019s nose such that it goes to the moon, nor indeed\nany\nprespecified celestial destination. Whether Google or the US Government or North Korea is the one to launch the rocket won\u2019t make a pragmatic difference to the probability of a successful Moon landing from our perspective, because right now\nnobody knows how to aim any kind of rocket anywhere\n.\nALFONSO:\nI\u2019m not sure I understand.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We\u2019re worried that even if you aim a rocket at the Moon, such that the nose of the rocket is clearly lined up with the Moon in the sky, the rocket won\u2019t go to the Moon. We\u2019re not sure what a realistic path from the Earth to the moon looks like, but we suspect it\nmight not be a very straight path\n, and it may not involve pointing the nose of the rocket at the moon at all. We think the most important thing to do next is to advance our understanding of rocket trajectories until we have a better, deeper understanding of what we\u2019ve started calling the \u201crocket alignment problem\u201d. There are other safety problems, but this rocket alignment problem will probably take the most total time to work on, so it\u2019s the most urgent.\nALFONSO:\nHmm, that sounds like a bold claim to me. Do you have a reason to think that there are invisible barriers between here and the moon that the spaceplane might hit? Are you saying that it might get very very windy between here and the moon, more so than on Earth? Both eventualities could be worth preparing for, I suppose, but neither seem likely.\nBETH:\nWe don\u2019t think it\u2019s particularly likely that there are invisible barriers, no. And we don\u2019t think it\u2019s going to be especially windy in the celestial reaches \u2014 quite the opposite, in fact. The problem is just that we don\u2019t yet know how to plot\nany\ntrajectory that a vehicle could realistically take to get from Earth to the moon.\nALFONSO:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Of course we can\u2019t plot an actual trajectory; wind and weather are too unpredictable. But your claim still seems too strong to me. Just aim the spaceplane at the moon, go up, and have the pilot adjust as necessary. Why wouldn\u2019t that work? Can you prove that a spaceplane aimed at the moon won\u2019t go there?\nBETH:\nWe don\u2019t think we can\nprove\nanything of that sort, no. Part of the problem is that realistic calculations are extremely hard to do in this area, after you take into account all the atmospheric friction and the movements of other celestial bodies and such. We\u2019ve been trying to solve some drastically simplified problems in this area, on the order of assuming that there is no atmosphere and that all rockets move in perfectly straight lines. Even those unrealistic calculations strongly suggest that, in the much more complicated real world, just pointing your rocket\u2019s nose at the Moon also won\u2019t make your rocket end up at the Moon. I mean, the fact that the real world is more complicated doesn\u2019t exactly make it any\neasier\nto get to the Moon.\nALFONSO:\nOkay, let me take a look at this \u201cunderstanding\u201d work you say you\u2019re doing\u2026",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Huh. Based on what I\u2019ve read about the math you\u2019re trying to do, I can\u2019t say I understand what it has to do with the Moon. Shouldn\u2019t helping spaceplane pilots exactly target the Moon involve looking through lunar telescopes and studying exactly what the Moon looks like, so that the spaceplane pilots can identify particular features of the landscape to land on?\nBETH:\nWe think our present stage of understanding is much too crude for a detailed Moon map to be our next research target. We haven\u2019t yet advanced to the point of targeting one crater or another for our landing. We can\u2019t target\nanything\nat this point. It\u2019s more along the lines of \u201cfigure out how to talk mathematically about curved rocket trajectories, instead of rockets that move in straight lines\u201d. Not even realistically curved trajectories, right now, we\u2019re just trying to get past straight lines at all \u2013\nALFONSO:\nBut planes on Earth move in curved lines all the time, because the Earth itself is curved. It seems reasonable to expect that future spaceplanes will also have the capability to move in curved lines. If your worry is that spaceplanes will only move in straight lines and miss the Moon, and you want to advise rocket engineers to build rockets that move in curved lines, well, that doesn\u2019t seem to me like a great use of anyone\u2019s time.\nBETH:\nYou\u2019re trying to draw much too direct of a line between the math we\u2019re working on right now, and actual rocket designs that might exist in the future. It\u2019s\nnot",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "that current rocket ideas are almost right, and we just need to solve one or two more problems to make them work. The conceptual distance that separates anyone from solving the rocket alignment problem is\nmuch greater\nthan that.\nRight now everyone is\nconfused\nabout rocket trajectories, and we\u2019re trying to become\nless confused\n. That\u2019s what we need to do next, not run out and advise rocket engineers to build their rockets the way that our current math papers are talking about. Not until we stop being\nconfused\nabout extremely basic questions like why the Earth doesn\u2019t fall into the Sun.\nALFONSO:\nI don\u2019t think the Earth is going to collide with the Sun anytime soon. The Sun has been steadily circling the Earth for a long time now.\nBETH:\nI\u2019m not saying that our goal is to address the risk of the Earth falling into the Sun. What I\u2019m trying to say is that if humanity\u2019s present knowledge can\u2019t answer questions like \u201cWhy doesn\u2019t the Earth fall into the Sun?\u201d then we don\u2019t know very much about celestial mechanics and we won\u2019t be able to aim a rocket through the celestial reaches in a way that lands softly on the Moon.\nAs an example of work we\u2019re presently doing that\u2019s aimed at improving our understanding, there\u2019s what we call the \u201c\ntiling positions\n\u201d problem. The tiling positions problem is\nhow to fire a cannonball from a cannon",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "in such a way that the cannonball circumnavigates the earth over and over again, \u201ctiling\u201d its initial coordinates like repeating tiles on a tessellated floor \u2013\nALFONSO:\nI read a little bit about your work on that topic. I have to say, it\u2019s hard for me to see what firing things from cannons has to do with getting to the Moon. Frankly, it sounds an awful lot like Good Old-Fashioned Space Travel, which everyone knows doesn\u2019t work. Maybe Jules Verne thought it was possible to travel around the earth by firing capsules out of cannons, but the modern study of high-altitude planes has completely abandoned the notion of firing things out of cannons. The fact that you go around talking about firing things out of cannons suggests to me that you haven\u2019t kept up with all the innovations in airplane design over the last century, and that your spaceplane designs will be completely unrealistic.\nBETH:\nWe know that rockets will not actually be fired out of cannons. We really, really know that. We\u2019re intimately familiar with the reasons why nothing fired out of a modern cannon is ever going to reach escape velocity. I\u2019ve previously written several sequences of articles in which I describe why cannon-based space travel doesn\u2019t work.\nALFONSO:\nBut your current work is all about firing something out a cannon in such a way that it circles the earth over and over. What could that have to do with any realistic advice that you could give to a spaceplane pilot about how to travel to the Moon?\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Again, you\u2019re trying to draw much too straight a line between the math we\u2019re doing right now, and direct advice to future rocket engineers.\nWe think that if we could find an angle and firing speed such that an ideal cannon, firing an ideal cannonball at that speed, on a perfectly spherical Earth with no atmosphere, would lead to that cannonball entering what we would call a \u201cstable orbit\u201d without hitting the ground, then\u2026 we might have understood something really fundamental and important about celestial mechanics.\nOr maybe not! It\u2019s hard to know in advance which questions are important and which research avenues will pan out. All you can do is figure out the next tractable-looking problem that confuses you, and try to come up with a solution, and hope that you\u2019ll be less confused after that.\nALFONSO:\nYou\u2019re talking about the cannonball hitting the ground as a problem, and how you want to avoid that and just have the cannonball keep going forever, right? But real spaceplanes aren\u2019t going to be aimed at the ground in the first place, and lots of regular airplanes manage to not hit the ground. It seems to me that this \u201cbeing fired out of a cannon and hitting the ground\u201d scenario that you\u2019re trying to avoid in this \u201ctiling positions problem\u201d of yours just isn\u2019t a failure mode that real spaceplane designers would need to worry about.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We are not worried about real rockets being fired out of cannons and hitting the ground. That is not why we\u2019re working on the tiling positions problem. In a way, you\u2019re being far too optimistic about how much of rocket alignment theory is already solved! We\u2019re not so close to understanding how to aim rockets that the kind of designs people are talking about now\nwould\nwork if only we solved a particular set of remaining difficulties like not firing the rocket into the ground. You need to go more meta on understanding the kind of progress we\u2019re trying to make.\nWe\u2019re working on the tiling positions problem because we think that being able to fire a cannonball at a certain instantaneous velocity such that it enters a stable orbit\u2026 is the sort of problem that somebody who could really actually launch a rocket through space and have it move in a particular curve that really actually ended with softly landing on the Moon would be able to solve\neasily\n. So the fact that we can\u2019t solve it is alarming.\nIf we can figure out how to solve this much simpler, much more crisply stated \u201ctiling positions problem\u201d with imaginary cannonballs on a perfectly spherical earth with no atmosphere, which is a lot easier to analyze than a Moon launch, we might thereby take one more incremental step towards eventually becoming the sort of people who could plot out a Moon launch.\nALFONSO:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If you don\u2019t think that Jules-Verne-style space cannons are the wave of the future, I don\u2019t understand why you keep talking about cannons in particular.\nBETH:\nBecause there\u2019s a lot of sophisticated mathematical machinery already developed for aiming cannons. People have been aiming cannons and plotting cannonball trajectories since the sixteenth century. We can take advantage of that existing mathematics to say exactly how, if we fired an ideal cannonball in a certain direction, it would plow into the ground. If we tried talking about rockets with realistically varying acceleration, we can\u2019t even manage to prove that a rocket like that\nwon\u2019t\ntravel around the Earth in a perfect square, because with all that realistically varying acceleration and realistic air friction it\u2019s impossible to make any sort of definite statement one way or another. Our present understanding isn\u2019t up to it.\nALFONSO:\nOkay, another question in the same vein. Why is MIRI sponsoring work on adding up lots of tiny vectors? I don\u2019t even see what that has to do with rockets in the first place; it seems like this weird side problem in abstract math.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It\u2019s more like\u2026 at several points in our investigation so far, we\u2019ve run into the problem of going from a function about time-varying accelerations to a function about time-varying positions. We kept running into this problem as a blocking point in our math, in several places, so we branched off and started trying to analyze it explicitly. Since it\u2019s about the pure mathematics of points that don\u2019t move in discrete intervals, we call it the \u201c\nlogical undiscreteness\n\u201d problem. Some of the ways of investigating this problem involve trying to add up lots of tiny, varying vectors to get a big vector. Then we talk about how that sum seems to change more and more slowly, approaching a limit, as the vectors get tinier and tinier and we add up more and more of them\u2026 or at least that\u2019s one avenue of approach.\nALFONSO:\nI just find it hard to imagine people in future spaceplane rockets staring out their viewports and going, \u201cOh, no, we don\u2019t have tiny enough vectors with which to correct our course! If only there was some way of adding up even more vectors that are even smaller!\u201d I\u2019d expect future calculating machines to do a pretty good job of that already.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Again, you\u2019re trying to draw much too straight a line between the work we\u2019re doing now, and the implications for future rocket designs. It\u2019s not like we think a rocket design will almost work, but the pilot won\u2019t be able to add up lots of tiny vectors fast enough, so we just need a faster algorithm and then the rocket will get to the Moon. This is foundational mathematical work that we think might play a role in multiple basic concepts for understanding celestial trajectories. When we try to plot out a trajectory that goes all the way to a soft landing on a moving Moon, we feel confused and blocked. We think part of the confusion comes from not being able to go from acceleration functions to position functions, so we\u2019re trying to resolve our confusion.\nALFONSO:\nThis sounds suspiciously like a philosophy-of-mathematics problem, and I don\u2019t think that it\u2019s possible to progress on spaceplane design by doing philosophical research. The field of philosophy is a stagnant quagmire. Some philosophers still believe that going to the moon is impossible; they say that the celestial plane is fundamentally separate from the earthly plane and therefore inaccessible, which is clearly silly. Spaceplane design is an engineering problem, and progress will be made by engineers.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I agree that rocket design will be carried out by engineers rather than philosophers. I also share some of your frustration with philosophy in general. For that reason, we stick to well-defined mathematical questions that are likely to have actual answers, such as questions about how to fire a cannonball on a perfectly spherical planet with no atmosphere such that it winds up in a stable orbit.\nThis often requires developing new mathematical frameworks. For example, in the case of the logical undiscreteness problem, we\u2019re developing methods for translating between time-varying accelerations and time-varying positions. You can call the development of new mathematical frameworks \u201cphilosophical\u201d if you\u2019d like \u2014 but if you do, remember that it\u2019s a very different kind of philosophy than the \u201cspeculate about the heavenly and earthly planes\u201d sort, and that we\u2019re always pushing to develop new mathematical frameworks or tools.\nALFONSO:\nSo from the perspective of the public good, what\u2019s a good thing that might happen if you solved this logical undiscreteness problem?\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Mainly, we\u2019d be less confused and our research wouldn\u2019t be blocked and humanity could actually land on the Moon someday. To try and make it more concrete \u2013 though it\u2019s hard to do that without actually knowing the concrete solution \u2013 we might be able to talk about incrementally more realistic rocket trajectories, because our mathematics would no longer break down as soon as we stopped assuming that rockets moved in straight lines. Our math would be able to talk about exact curves, instead of a series of straight lines that approximate the curve.\nALFONSO:\nAn exact curve that a rocket follows? This gets me into the main problem I have with your project in general. I just don\u2019t believe that any future rocket design will be the sort of thing that can be analyzed with absolute, perfect precision so that you can get the rocket to the Moon based on an absolutely plotted trajectory with no need to steer. That seems to me like a bunch of mathematicians who have no clue how things work in the real world, wanting everything to be perfectly calculated. Look at the way Venus moves in the sky; usually it travels in one direction, but sometimes it goes retrograde in the other direction. We\u2019ll just have to steer as we go.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "That\u2019s not what I meant by talking about exact curves\u2026 Look, even if we can invent logical undiscreteness, I agree that it\u2019s futile to try to predict, in advance, the precise trajectories of all of the winds that will strike a rocket on its way off the ground. Though I\u2019ll mention parenthetically that things might actually become calmer and easier to predict, once a rocket gets sufficiently high up \u2013\nALFONSO:\nWhy?\nBETH:\nLet\u2019s just leave that aside for now, since we both agree that rocket positions are hard to predict exactly during the atmospheric part of the trajectory, due to winds and such. And yes, if you can\u2019t exactly predict the initial trajectory, you can\u2019t exactly predict the later trajectory. So, indeed, the proposal is definitely not to have a rocket design so perfect that you can fire it at exactly the right angle and then walk away without the pilot doing any further steering. The point of doing rocket math isn\u2019t that you want to predict the rocket\u2019s exact position at every microsecond, in advance.\nALFONSO:\nThen why obsess over pure math that\u2019s too simple to describe the rich, complicated real universe where sometimes it rains?\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It\u2019s true that a real rocket isn\u2019t a simple equation on a board. It\u2019s true that there are all sorts of aspects of a real rocket\u2019s shape and internal plumbing that aren\u2019t going to have a mathematically compact characterization. What MIRI is doing isn\u2019t the right degree of mathematization for all rocket engineers for all time; it\u2019s the mathematics for us to be using right now (or so we hope).\nTo build up the field\u2019s understanding incrementally, we need to talk about ideas whose consequences can be pinpointed precisely enough that people can analyze scenarios in a shared framework. We need enough precision that someone can say, \u201cI think in scenario X, design Y does Z\u201d, and someone else can say, \u201cNo, in scenario X, Y actually does W\u201d, and the first person responds, \u201cDarn, you\u2019re right. Well, is there some way to change Y so that it would do Z?\u201d\nIf you try to make things realistically complicated at this stage of research, all you\u2019re left with is verbal fantasies. When we try to talk to someone with an enormous flowchart of all the gears and steering rudders they think should go into a rocket design, and we try to explain why a rocket pointed at the Moon doesn\u2019t necessarily end up at the Moon, they just reply, \u201cOh, my rocket won\u2019t do\nthat\n.\u201d Their ideas have enough vagueness and flex and underspecification that they\u2019ve achieved the safety of nobody being able to prove to them that they\u2019re wrong. It\u2019s impossible to incrementally build up a body of collective knowledge that way.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The goal is to start building up a library of tools and ideas we can use to discuss trajectories formally. Some of the key tools for formalizing and analyzing\nintuitively\nplausible-seeming trajectories haven\u2019t yet been expressed using math, and we can live with that for now. We still try to find ways to represent the key ideas in mathematically crisp ways whenever we can. That\u2019s not because math is so neat or so prestigious; it\u2019s part of an ongoing project to have arguments about rocketry that go beyond \u201cDoes not!\u201d vs. \u201cDoes so!\u201d\nALFONSO:\nI still get the impression that you\u2019re reaching for the warm, comforting blanket of mathematical reassurance in a realm where mathematical reassurance doesn\u2019t apply. We can\u2019t obtain a mathematical certainty of our spaceplanes being absolutely sure to reach the Moon with nothing going wrong. That being the case, there\u2019s no point in trying to pretend that we can use mathematics to get absolute guarantees about spaceplanes.\nBETH:\nTrust me, I am not going to feel \u201creassured\u201d about rocketry no matter what math MIRI comes up with. But, yes, of course you can\u2019t obtain a mathematical assurance of any physical proposition, nor assign probability 1 to any empirical statement.\nALFONSO:\nYet you talk about proving theorems \u2013 proving that a cannonball will go in circles around the earth indefinitely, for example.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Proving a theorem about a rocket\u2019s trajectory won\u2019t ever let us feel comfortingly certain about where the rocket is actually going to end up. But if you can prove a theorem which says that your rocket would go to the Moon if it launched in a perfect vacuum, maybe you can attach some steering jets to the rocket and then have it actually go to the Moon in real life. Not with 100% probability, but with probability greater than zero.\nThe point of our work isn\u2019t to take current ideas about rocket aiming from a 99% probability of success to a 100% chance of success. It\u2019s to get past an approximately 0% chance of success, which is where we are now.\nALFONSO:\nZero percent?!\nBETH:\nModulo\nCromwell\u2019s Rule\n, yes, zero percent. If you point a rocket\u2019s nose at the Moon and launch it, it does not go to the Moon.\nALFONSO:\nI don\u2019t think future spaceplane engineers will actually be that silly, if direct Moon-aiming isn\u2019t a method that works. They\u2019ll lead the Moon\u2019s current motion in the sky, and aim at the part of the sky where Moon will appear on the day the spaceplane is a Moon\u2019s distance away. I\u2019m a bit worried that you\u2019ve been talking about this problem so long without considering such an obvious idea.\nBETH:\nWe considered that idea very early on, and we\u2019re pretty sure that it still doesn\u2019t get us to the Moon.\nALFONSO:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "What if I add steering fins so that the rocket moves in a more curved trajectory? Can you prove that no version of that class of rocket designs will go to the Moon, no matter what I try?\nBETH:\nCan you sketch out the trajectory that you think your rocket will follow?\nALFONSO:\nIt goes from the Earth to the Moon.\nBETH:\nIn a bit more detail, maybe?\nALFONSO:\nNo, because in the real world there are always variable wind speeds, we don\u2019t have infinite fuel, and our spaceplanes don\u2019t move in perfectly straight lines.\nBETH:\nCan you sketch out a trajectory that you think a simplified version of your rocket will follow, so we can examine the\nassumptions\nyour idea requires?\nALFONSO:\nI just don\u2019t believe in the general methodology you\u2019re proposing for spaceplane designs. We\u2019ll put on some steering fins, turn the wheel as we go, and keep the Moon in our viewports. If we\u2019re off course, we\u2019ll steer back.\nBETH:\n\u2026 We\u2019re actually a bit concerned that\nstandard steering fins may stop working once the rocket gets high enough\n, so you won\u2019t actually find yourself able to correct course by much once you\u2019re in the celestial reaches \u2013 like, if you\u2019re already on a good course, you can correct it, but if you screwed up, you won\u2019t just be able to turn around like you could turn around an airplane \u2013\nALFONSO:\nWhy not?\nBETH:\nWe can go into that topic too; but even given a simplified model of a rocket that you\ncould",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "steer, a walkthrough of the steps along the path that simplified rocket would take to the Moon would be an important step in moving this discussion forward. Celestial rocketry is a domain that we expect to be unusually difficult \u2013 even compared to building rockets on Earth, which is already a famously hard problem because they usually just explode. It\u2019s not that everything has to be neat and mathematical. But the overall difficulty is such that, in a proposal like \u201clead the moon in the sky,\u201d if the core ideas don\u2019t have a certain amount of solidity about them, it would be equivalent to firing your rocket randomly into the void.\nIf it feels like you don\u2019t know for sure whether your idea works, but that it might work; if your idea has many plausible-sounding elements, and to you it feels like nobody has been able to\nconvincingly\nexplain to you how it would fail; then, in real life, that proposal has a roughly 0% chance of steering a rocket to the Moon.\nIf it seems like an idea is extremely solid and clearly well-understood, if it feels like this proposal should definitely take a rocket to the Moon without fail in good conditions, then maybe under the best-case conditions we should assign an 85% subjective credence in success, or something in that vicinity.\nALFONSO:\nSo uncertainty automatically means failure? This is starting to sound a bit paranoid, honestly.\nBETH:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The idea I\u2019m trying to communicate is something along the lines of, \u201cIf you can reason rigorously about why a rocket should definitely work in principle, it might work in real life, but if you have anything less than that, then it definitely won\u2019t work in real life.\u201d\nI\u2019m not asking you to give me an absolute mathematical proof of empirical success. I\u2019m asking you to give me something more like a sketch for how a simplified version of your rocket could move, that\u2019s sufficiently determined in its meaning that you can\u2019t just come back and say \u201cOh, I didn\u2019t mean\nthat\n\u201d every time someone tries to figure out what it actually does or pinpoint a failure mode.\nThis isn\u2019t an unreasonable demand that I\u2019m imposing to make it impossible for any ideas to pass my filters. It\u2019s the primary bar all of us have to pass to contribute to collective progress in this field. And a rocket design which can\u2019t even pass that conceptual bar has roughly a 0% chance of landing softly on the Moon.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is the first of five posts in the\nRisks from Learned Optimization Sequence\nbased on the paper \u201c\nRisks from Learned Optimization in Advanced Machine Learning Systems\n\u201d by Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Each post in the sequence corresponds to a different section of the paper.\nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, and Joar Skalse contributed equally to this sequence. With special thanks to Paul Christiano, Eric Drexler, Rob Bensinger, Jan Leike, Rohin Shah, William Saunders, Buck Shlegeris, David Dalrymple, Abram Demski, Stuart Armstrong, Linda Linsefors, Carl Shulman, Toby Ord, Kate Woolverton, and everyone else who provided feedback on earlier versions of this sequence.\nMotivation\nThe goal of this sequence is to analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer\u2014a situation we refer to as\nmesa-optimization,\na neologism we introduce in this sequence. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be\u2014how will it differ from the loss function it was trained under\u2014and how can it be aligned?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We believe that this sequence presents the most thorough analysis of these questions that has been conducted to date. In particular, we present not only an introduction to the basic concerns surrounding mesa-optimizers, but also an analysis of the particular aspects of an AI system that we believe are likely to make the problems related to mesa-optimization relatively easier or harder to solve. By providing a framework for understanding the degree to which different AI systems are likely to be robust to misaligned mesa-optimization, we hope to start a discussion about the best ways of structuring machine learning systems to solve these problems. Furthermore, in the fourth post we will provide what we think is the most detailed analysis yet of a problem we refer as\ndeceptive alignment\nwhich we posit may present one of the largest\u2014though not necessarily insurmountable\u2014current obstacles to producing safe advanced machine learning systems using techniques similar to modern machine learning.\nTwo questions\nIn machine learning, we do not manually program each individual parameter of our models. Instead, we specify an objective function that captures what we want the system to do and a learning algorithm to optimize the system for that objective. In this post, we present a framework that distinguishes what a system is\noptimized\nto do (its \u201cpurpose\u201d), from what it\noptimizes",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "for (its \u201cgoal\u201d), if it optimizes for anything at all. While all AI systems are optimized for something (have a purpose), whether they actually optimize for anything (pursue a goal) is non-trivial.\nWe will say that a system is an\noptimizer\nif it is internally searching through a search space (consisting of possible outputs, policies, plans, strategies, or similar) looking for those elements that score high according to some objective function that is explicitly represented within the system.\nLearning algorithms in machine learning are optimizers because they search through a space of possible parameters\u2014e.g. neural network weights\u2014and improve the parameters with respect to some objective. Planning algorithms are also optimizers, since they search through possible plans, picking those that do well according to some objective.\nWhether a system is an optimizer is a property of its internal structure\u2014what algorithm it is physically implementing\u2014and not a property of its input-output behavior. Importantly, the fact that a system\u2019s behavior results in some objective being maximized does not make the system an optimizer. For example, a bottle cap causes water to be held inside the bottle, but it is not optimizing for that outcome since it is not running any sort of optimization algorithm.\n(1)\nRather, bottle caps have been\noptimized",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to keep water in place. The optimizer in this situation is the human that designed the bottle cap by searching through the space of possible tools for one to successfully hold water in a bottle. Similarly, image-classifying neural networks are optimized to achieve low error in their classifications, but are not, in general, themselves performing optimization.\nHowever, it is also possible for a neural network to itself run an optimization algorithm. For example, a neural network could run a planning algorithm that predicts the outcomes of potential plans and searches for those it predicts will result in some desired outcome.\n[1]\nSuch a neural network would itself be an optimizer because it would be searching through the space of possible plans according to some objective function. If such a neural network were produced in training, there would be two optimizers: the learning algorithm that produced the neural network\u2014which we will call the\nbase optimizer\n\u2014and the neural network itself\u2014which we will call the\nmesa-optimizer.\n[2]\nThe possibility of mesa-optimizers has important implications for the safety of advanced machine learning systems. When a base optimizer generates a mesa-optimizer, safety properties of the base optimizer's objective may not transfer to the mesa-optimizer. Thus, we explore two primary questions related to the safety of mesa-optimizers:\nMesa-optimization:\nUnder what circumstances will learned algorithms be optimizers?\nInner alignment:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When a learned algorithm is an optimizer, what will its objective be, and how can it be aligned?\nOnce we have introduced our framework in this post, we will address the first question in the second, begin addressing the second question in the third post, and finally delve deeper into a specific aspect of the second question in the fourth post.\n1.1. Base optimizers and mesa-optimizers\nConventionally, the base optimizer in a machine learning setup is some sort of gradient descent process with the goal of creating a model designed to accomplish some specific task.\nSometimes, this process will also involve some degree of meta-optimization wherein a\nmeta-optimizer\nis tasked with producing a base optimizer that is itself good at optimizing systems to achieve particular goals. Specifically, we will think of a\nmeta-optimizer\nas any system whose task is optimization. For example, we might design a meta-learning system to help tune our gradient descent process.\n(4)\nThough the model found by meta-optimization can be thought of as a kind of learned optimizer, it is not the form of learned optimization that we are interested in for this sequence. Rather, we are concerned with a different form of learned optimization which we call\nmesa-optimization.\nMesa-optimization is a conceptual dual of meta-optimization\u2014whereas\nmeta\nis Greek for \u201cafter,\u201d\nmesa\nis Greek for \u201cwithin.\u201d\n[3]\nMesa-optimization",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "occurs when a base optimizer (in searching for algorithms to solve some problem) finds a model that is itself an optimizer, which we will call a\nmesa-optimizer.\nUnlike meta-optimization, in which the task itself is optimization, mesa-optimization is task-independent, and simply refers to any situation where the internal structure of the model ends up performing optimization because it is instrumentally useful for solving the given task.\nIn such a case, we will use\nbase objective\nto refer to whatever criterion the base optimizer was using to select between different possible systems and\nmesa-objective\nto refer to whatever criterion the mesa-optimizer is using to select between different possible outputs. In reinforcement learning (RL), for example, the base objective is generally the expected return. Unlike the base objective, the mesa-objective is not specified directly by the programmers. Rather, the mesa-objective is simply whatever objective was found by the base optimizer that produced good performance on the training environment. Because the mesa-objective is not specified by the programmers, mesa-optimization opens up the possibility of a mismatch between the base and mesa- objectives, wherein the mesa-objective might seem to perform well on the training environment but lead to bad performance off the training environment. We will refer to this case as\npseudo-alignment\nbelow.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "There need not always be a mesa-objective since the algorithm found by the base optimizer will not always be performing optimization. Thus, in the general case, we will refer to the model generated by the base optimizer as a\nlearned algorithm,\nwhich may or may not be a mesa-optimizer.\nFigure 1.1.\nThe relationship between the base and mesa- optimizers. The base optimizer optimizes the learned algorithm based on its performance on the base objective. In order to do so, the base optimizer may have turned this learned algorithm into a mesa-optimizer, in which case the mesa-optimizer itself runs an optimization algorithm based on its own mesa-objective. Regardless, it is the learned algorithm that directly takes actions based on its input.\nPossible misunderstanding: \u201cmesa-optimizer\u201d does not mean \u201csubsystem\u201d or \u201csubagent.\u201d\nIn the context of deep learning, a mesa-optimizer is simply a neural network that is implementing some optimization process and not some emergent subagent inside that neural network. Mesa-optimizers are simply a particular type of algorithm that the base optimizer might find to solve its task. Furthermore, we will generally be thinking of the base optimizer as a straightforward optimization algorithm, and not as an intelligent agent choosing to create a subagent.\n[4]\nWe distinguish the mesa-objective from a related notion that we term the\nbehavioral objective",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". Informally, the behavioral objective is the objective which appears to be optimized by the system\u2019s behavior. We can operationalize the behavioral objective as the objective recovered from perfect inverse reinforcement learning (IRL).\n[5]\nThis is in contrast to the mesa-objective, which is the objective\nactively being used\nby the mesa-optimizer in its optimization algorithm.\nArguably, any possible system has a behavioral objective\u2014including bricks and bottle caps. However, for non-optimizers, the appropriate behavioral objective might just be \u201c1 if the actions taken are those that are in fact taken by the system and 0 otherwise,\u201d\n[6]\nand it is thus neither interesting nor useful to know that the system is acting to optimize this objective. For example, the behavioral objective \u201coptimized\u201d by a bottle cap is the objective of behaving like a bottle cap.\n[7]\nHowever, if the system is an optimizer, then it is more likely that it will have a meaningful behavioral objective. That is, to the degree that a mesa-optimizer\u2019s output is systematically selected to optimize its mesa-objective, its behavior may look more like coherent attempts to move the world in a particular direction.\n[8]",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "A given mesa-optimizer\u2019s mesa-objective is determined entirely by its internal workings. Once training is finished and a learned algorithm is selected, its direct output\u2014e.g. the actions taken by an RL agent\u2014no longer depends on the base objective. Thus, it is the mesa-objective, not the base objective, that determines a mesa-optimizer\u2019s behavioral objective. Of course, to the degree that the learned algorithm was selected on the basis of the base objective, its output will score well on the base objective. However, in the case of a distributional shift, we should expect a mesa-optimizer\u2019s behavior to more robustly optimize for the mesa-objective since its behavior is directly computed according to it.\nAs an example to illustrate the base/mesa distinction in a different domain, and the possibility of misalignment between the base and mesa- objectives, consider biological evolution. To a first approximation, evolution selects organisms according to the objective function of their inclusive genetic fitness in some environment.\n[9]",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Most of these biological organisms\u2014plants, for example\u2014are not \u201ctrying\u201d to achieve anything, but instead merely implement heuristics that have been pre-selected by evolution. However, some organisms, such as humans, have behavior that does not merely consist of such heuristics but is instead also the result of goal-directed optimization algorithms implemented in the brains of these organisms. Because of this, these organisms can perform behavior that is completely novel from the perspective of the evolutionary process, such as humans building computers.\nHowever, humans tend not to place explicit value on evolution\u2019s objective, at least in terms of caring about their alleles' frequency in the population. The objective function stored in the human brain is not the same as the objective function of evolution. Thus, when humans display novel behavior optimized for their own objectives, they can perform very poorly according to evolution\u2019s objective. Making a decision not to have children is a possible example of this. Therefore, we can think of evolution as a base optimizer that produced brains\u2014mesa-optimizers\u2014which then actually produce organisms\u2019 behavior\u2014behavior that is not necessarily aligned with evolution.\n1.2. The inner and outer alignment problems",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In \u201cScalable agent alignment via reward modeling,\u201d Leike et al. describe the concept of the \u201creward-result gap\u201d as the difference between the (in their case learned) \u201creward model\u201d (what we call the base objective) and the \u201creward function that is recovered with perfect inverse reinforcement learning\u201d (what we call the behavioral objective).\n(8)\nThat is, the reward-result gap is the fact that there can be a difference between what a learned algorithm is observed to be doing and what the programmers want it to be doing.\nThe problem posed by misaligned mesa-optimizers is a kind of reward-result gap. Specifically, it is the gap between the base objective and the mesa-objective (which then causes a gap between the base objective and the behavioral objective). We will call the problem of eliminating the base-mesa objective gap the\ninner alignment problem,\nwhich we will contrast with the\nouter alignment problem\nof eliminating the gap between the base objective and the intended goal of the programmers.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This terminology is motivated by the fact that the inner alignment problem is an alignment problem entirely internal to the machine learning system, whereas the outer alignment problem is an alignment problem between the system and the humans outside of it (specifically between the base objective and the programmer\u2019s intentions). In the context of machine learning, outer alignment refers to aligning the specified loss function with the intended goal, whereas inner alignment refers to aligning the mesa-objective of a mesa-optimizer with the specified loss function.\nIt might not be necessary to solve the inner alignment problem in order to produce safe, highly capable AI systems, as it might be possible to prevent mesa-optimizers from occurring in the first place. If mesa-optimizers cannot be reliably prevented, however, then some solution to both the outer and inner alignment problems will be necessary to ensure that mesa-optimizers are aligned with the intended goal of the programmers.\n1.3. Robust alignment vs. pseudo-alignment\nGiven enough training, a mesa-optimizer should eventually be able to produce outputs that score highly on the base objective on the training distribution. Off the training distribution, however\u2014and even on the training distribution while it is still early in the training process\u2014the difference could be arbitrarily large. We will use the term\nrobustly aligned",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to refer to mesa-optimizers with mesa-objectives that robustly agree with the base objective across distributions and the term\npseudo-aligned\nto refer to mesa-optimizers with mesa-objectives that agree with the base objective on past training data, but not robustly across possible future data (either in testing, deployment, or further training). For a pseudo-aligned mesa-optimizer, there will be environments in which the base and mesa- objectives diverge. Pseudo-alignment, therefore, presents a potentially dangerous robustness problem since it opens up the possibility of a machine learning system that competently takes actions to achieve something other than the intended goal when off the training distribution. That is, its capabilities might generalize while its objective does not.\nFor a toy example of what pseudo-alignment might look like, consider an RL agent trained on a maze navigation task where all the doors during training happen to be red. Let the base objective (reward function) be\nO\nbase\n=\n(1 if reached a door, 0 otherwise)\n. On the training distribution, this objective is equivalent to\nO\nalt\n=\n(1 if reached something red, 0 otherwise)\n. Consider what would happen if an agent, trained to high performance on\nO\nbase\non this task, were put in an environment where the doors are instead blue, and with some red objects that are not doors. It might generalize on\nO\nbase\n, reliably navigating to the blue door in each maze (robust alignment). But it might also generalize on",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "O\nalt\ninstead of\nO\nbase\n, reliably navigating each maze to reach red objects (pseudo-alignment).\n[10]\n1.4. Mesa-optimization as a safety problem\nIf pseudo-aligned mesa-optimizers may arise in advanced ML systems, as we will suggest, they could pose two critical safety problems.\nUnintended optimization.\nFirst, the possibility of mesa-optimization means that an advanced ML system could end up implementing a powerful optimization procedure even if its programmers never intended it to do so. This could be dangerous if such optimization leads the system to take extremal actions outside the scope of its intended behavior in trying to maximize its mesa-objective. Of particular concern are optimizers with objective functions and optimization procedures that generalize to the real world. The conditions that lead a learning algorithm to find mesa-optimizers, however, are very poorly understood. Knowing them would allow us to predict cases where mesa-optimization is more likely, as well as take measures to discourage mesa-optimization from occurring in the first place. The second post will examine some features of machine learning algorithms that might influence their likelihood of finding mesa-optimizers.\nInner alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Second, even in cases where it is acceptable for a base optimizer to find a mesa-optimizer, a mesa-optimizer might optimize for something other than the specified reward function. In such a case, it could produce bad behavior even if optimizing the correct reward function was known to be safe. This could happen either during training\u2014before the mesa-optimizer gets to the point where it is aligned over the training distribution\u2014or during testing or deployment when the system is off the training distribution. The third post will address some of the different ways in which a mesa-optimizer could be selected to optimize for something other than the specified reward function, as well as what attributes of an ML system are likely to encourage this. In the fourth post, we will discuss a possible extreme inner alignment failure\u2014which we believe presents one of the most dangerous risks along these lines\u2014wherein a sufficiently capable misaligned mesa-optimizer could learn to behave as if it were aligned without actually being robustly aligned. We will call this situation\ndeceptive alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It may be that pseudo-aligned mesa-optimizers are easy to address\u2014if there exists a reliable method of aligning them, or of preventing base optimizers from finding them. However, it may also be that addressing misaligned mesa-optimizers is very difficult\u2014the problem is not sufficiently well-understood at this point for us to know. Certainly, current ML systems do not produce dangerous mesa-optimizers, though whether future systems might is unknown. It is indeed because of these unknowns that we believe the problem is important to analyze.\nThe second post in the\nRisks from Learned Optimization Sequence\n, titled \u201cConditions for Mesa-Optimization,\u201d can be found\nhere\n.\nGlossary\n|\nBibliography\nAs a concrete example of what a neural network optimizer might look like, consider TreeQN.\n(2)\nTreeQN, as described in Farquhar et al., is a Q-learning agent that performs model-based planning (via tree search in a latent representation of the environment states)\nas part of its computation of the Q-function.\nThough their agent is an optimizer by design, one could imagine a similar algorithm being\nlearned\nby a DQN agent with a sufficiently expressive approximator for the Q function. Universal Planning Networks, as described by Srinivas et al.,\n(3)\nprovide another example of a learned system that performs optimization, though the optimization there is built-in in the form of SGD via automatic differentiation. However, research such as that in Andrychowicz et al.\n(4)\nand Duan et al.\n(5)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "demonstrate that optimization algorithms can be learned by RNNs, making it possible that a Universal Planning Networks-like agent could be entirely learned\u2014assuming a very expressive model space\u2014including the internal optimization steps. Note that while these examples are taken from reinforcement learning, optimization might in principle take place in any sufficiently expressive learned system.\n\u21a9\ufe0e\nPrevious work in this space has often centered around the concept of \u201coptimization daemons,\u201d\n(6)\na framework that we believe is potentially misleading and hope to supplant. Notably, the term \u201coptimization daemon\u201d came out of discussions regarding the nature of humans and evolution, and, as a result, carries anthropomorphic connotations.\n\u21a9\ufe0e\nThe duality comes from thinking of meta-optimization as one layer above the base optimizer and mesa-optimization as one layer below.\n\u21a9\ufe0e\nThat being said, some of our considerations do still apply even in that case.\n\u21a9\ufe0e\nLeike et al.\n(8)\nintroduce the concept of an objective recovered from perfect IRL.\n\u21a9\ufe0e\nFor the formal construction of this objective, see pg. 6 in Leike et al.\n(8)\n\u21a9\ufe0e\nThis objective is by definition trivially optimal in any situation that the bottlecap finds itself in.\n\u21a9\ufe0e",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Ultimately, our worry is optimization in the direction of some coherent but unsafe objective. In this sequence, we assume that search provides sufficient structure to expect coherent objectives. While we believe this is a reasonable assumption, it is unclear both whether search is necessary and whether it is sufficient. Further work examining this assumption will likely be needed.\n\u21a9\ufe0e\nThe situation with evolution is more complicated than is presented here and we do not expect our analogy to live up to intense scrutiny. We present it as nothing more than that: an evocative analogy (and, to some extent, an existence proof) that explains the key concepts. More careful arguments are presented later.\n\u21a9\ufe0e\nOf course, it might also fail to generalize at all.\n\u21a9\ufe0e",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is the blog post version of\nthe paper by the same name\n. Special thanks to Kate Woolverton, Paul Christiano, Rohin Shah, Alex Turner, William Saunders, Beth Barnes, Abram Demski, Scott Garrabrant, Sam Eisenstat, and Tsvi Benson-Tilsen for providing helpful comments and feedback on this post and the talk that preceded it.\nThis post is a collection of 11 different proposals for building safe advanced AI under the current machine learning paradigm. There's a lot of literature out there laying out various different approaches such as\namplification\n,\ndebate\n, or\nrecursive reward modeling\n, but a lot of that literature focuses primarily on outer alignment at the expense of\ninner alignment\nand doesn't provide direct comparisons between approaches. The goal of this post is to help solve that problem by providing a single collection of 11 different proposals for building safe advanced AI\u2014each including both inner and outer alignment components. That being said, not only does this post not cover all existing proposals, I strongly expect that there will be lots of additional new proposals to come in the future. Nevertheless, I think it is quite useful to at least take a broad look at what we have now and compare and contrast some of the current leading candidates.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It is important for me to note before I begin that the way I describe the 11 approaches presented here is not meant to be an accurate representation of how anyone else would represent them. Rather, you should treat all the approaches I describe here as\nmy version\nof that approach rather than any sort of canonical version that their various creators/proponents would endorse.\nFurthermore, this post only includes approaches that intend to directly build advanced AI systems via machine learning. Thus, this post doesn't include other possible approaches for solving the broader AI existential risk problem such as:\nfinding a fundamentally different way of approaching AI than the current machine learning paradigm that makes it easier to build safe advanced AI,\ndeveloping some advanced technology that produces a\ndecisive strategic advantage\nwithout using advanced AI, or\nachieving global coordination around not building advanced AI via (for example) a persuasive demonstration that any advanced AI is likely to be unsafe.\nFor each of the proposals that I consider, I will try to evaluate them on the following four basic components that I think any story for how to build safe advanced AI under the current machine learning paradigm needs.\nOuter alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Outer alignment is about asking why the objective we're training for is aligned\u2014that is, if we actually got a model that was trying to optimize for the given loss/reward/etc., would we like that model? For a more thorough description of what I mean by outer alignment, see \u201c\nOuter alignment and imitative amplification\n.\u201d\nInner alignment.\nInner alignment is about asking the question of how our training procedure can actually guarantee that the model it produces will, in fact, be trying to accomplish the objective we trained it on. For a more rigorous treatment of this question and an explanation of why it might be a concern, see \u201c\nRisks from Learned Optimization\n.\u201d\nTraining competitiveness.\nCompetitiveness is a bit of a murky concept, so I want to break it up into two pieces here. Training competitiveness is the question of whether the given training procedure is one that a team or group of teams with a reasonable lead would be able to afford to implement without completely throwing away that lead. Thus, training competitiveness is about whether the proposed process of producing advanced AI is competitive.\nPerformance competitiveness.\nPerformance competitiveness, on the other hand, is about whether the final product produced by the proposed process is competitive. Performance competitiveness is thus about asking whether a particular proposal, if successful, would satisfy the use cases for advanced AI\u2014e.g. whether it would fill the economic niches that people want AGI to fill.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I think it\u2019s often easy to focus too much on either the alignment side or the competitiveness side while neglecting the other. We obviously want to avoid proposals which could be unsafe, but at the same time the \u201cdo nothing\u201d proposal is equally unacceptable\u2014while doing nothing is quite safe in terms of having no chance of directly leading to existential risk, it doesn\u2019t actually help in any way relative to what would have happened by default. Thus, we want proposals that are both aligned and competitive so that they not only don\u2019t lead to existential risk themselves, but also help existential risk in general by providing a model of how safe advanced AI can be built, being powerful enough to assist with future alignment research, and/or granting a\ndecisive strategic advantage\nthat can be leveraged into otherwise reducing existential risk.\nThe 11 proposals considered in this post are, in order:\n[1]\nReinforcement learning + transparency tools\nImitative amplification + intermittent oversight\nImitative amplification + relaxed adversarial training\nApproval-based amplification + relaxed adversarial training\nMicroscope AI\nSTEM AI\nNarrow reward modeling + transparency tools\nRecursive reward modeling + relaxed adversarial training\nAI safety via debate with transparency tools\nAmplification with auxiliary RL objective + relaxed adversarial training\nAmplification alongside RL + relaxed adversarial training",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "EDIT: For some other proposals that didn't make it onto this list because they came out after this post, see \u201c\nImitative Generalization\n\u201d and \u201c\nAI safety via market making\n.\u201d\n1. Reinforcement learning + transparency tools\nHere's our first approach:\nTrain a reinforcement learning (RL) agent in an environment where corrigibility, honesty, multi-agent cooperation, etc. are incentivized. The basic idea would be to mimic the evolutionary forces that led to humans\u2019 general cooperativeness. As an example of work along these lines that exists now, see\nOpenAI's hide and seek game\n. Furthermore, the environment could also be modified to directly reward following human instructions, encouraging corrigibility towards humans. For a more thorough discussion of this possibility, see Richard Ngo\u2019s \u201c\nMulti-agent safety\n.\u201d\nAn image of\nOpenAI's hide and seek game\n.\nHave humans use\ntransparency tools\n, adversarial training, etc. to check for\ndeceptive\nor otherwise\ncatastrophic\nbehavior in the resulting model.\nThat's the approach\u2014now is it aligned? Competitive?\nOuter alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Outer alignment here is entirely dependent on whatever the dominant behavior is in the training environment\u2014that is, what is the deployment behavior of those models which perform optimally in the training environment. If corrigibility, honesty, cooperation, etc. do in fact dominate in the limit, then such an approach would be outer aligned. By default, however, it seems quite difficult to understand the limiting behavior of complex, multi-agent environments, especially if they\u2019re anywhere as complex as the actual human ancestral environment. If following human instructions is incentivized, for example, that could lead to corrigibility in the limit\u2014or it could lead to agents which only choose to follow human instructions for the instrumental reason of believing it will help them acquire more resources. Alternatively, it might be possible to isolate the structure that was present in the human ancestral environment that led us to be cooperative, honest, etc. One worry here, however, is that even if we could figure out how to properly incentivize cooperation, it might result in agents that are cooperative with each other but not very cooperative with us, similarly to how we might not be very cooperative with aliens that are very different from us.\nInner alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The idea of inner alignment in this situation is to ensure that training produces something in line with the optimal behavior in the environment (the alignment of the optimal behavior being an outer alignment question) rather than other, potentially perverse equilibria. The basic proposal for how to avoid such perverse equilibria with this proposal is via the use of checks such as transparency tools and adversarial training to detect inner alignment failures before the model is deployed.\nChris Olah describes\nthis sort of transparency checking as giving you a \u201cmulligan\u201d that lets you throw away your model and start over if you find something wrong. Thus, ideally, if this approach ends up not working it should be clear before the model is deployed, enabling either this approach to be fixed or a new approach to be found instead. And there is a reasonable chance that it does just work\u2014we don\u2019t understand our models\u2019 inductive biases very well, but it seems entirely possible that they could work out such that\npseudo-alignment\nis disincentivized.\nIn my opinion, while it seems quite plausible to me that this sort of approach could catch\nproxy pseudo-alignment\n, it seems unlikely that it would successfully catch\ndeceptive pseudo-alignment",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", as it could be very difficult to make transparency tools that are robust to a deceptive model actively trying to trick them. To catch deceptive alignment, it seems likely to be necessary to incorporate such checks into the training process itself\u2014which is possible to do in this setting, though is not the approach I described above\u2014in order to prevent deception from occurring in the first place rather than trying to detect it after the fact.\nTraining competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Training competitiveness here seems likely to depend on the extent to which the sort of agency produced by RL is necessary to train advanced AI systems. Performing RL in highly complex, difficult-to-simulate environments\u2014especially if those environments involve interaction with the real world\u2014could be quite expensive from a training competitiveness standpoint. Compared to simple language modeling, for example, the difficulty of on-policy data collection combined with low sample-efficiency could make full-scale RL much less training competitive. These sorts of competitiveness concerns could be particularly pronounced if the features necessary to ensure that the RL environment is aligned result in making it significantly more difficult to simulate. That being said, if RL is necessary to do anything powerful and simple language modeling is insufficient, then whether or not language modeling is easier is a moot point. Whether RL is really necessary seems likely to depend on the extent to which it is necessary to explicitly train agents\u2014which is very much an open question. Furthermore, even if agency is required, it could potentially be obtained just by imitating an actor such as a human that already has it rather than training it directly via RL.\nPerformance competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The question for performance competitiveness here is to what extent it is possible to create an environment that incentivizes all the behavior you might want from your AGI. Such an environment doesn\u2019t need to be purely simulated\u2014you could do some simulation training and some real-world training, for example. Regardless of how your RL environment is constructed, however, it needs to actually incentivize the correct behavior for the tasks that you want to use your AI for. For example: can you incentivize good decision-making? Good question-answering? Good learning ability? Do you need good fine motor control, and if so, can you incentivize it? These are highly non-trivial questions: it could be quite difficult to set up an RL environment to teach an agent to do all of the tasks you might want it to perform to fill all the economic niches for AGI, for example. Of course, this is going to be highly dependent on what exactly those economic niches are that you want your advanced AI to fill.\n2. Imitative amplification + intermittent oversight\nThough many of the approaches on this list make use of the basic\niterated amplification\nframework, imitative amplification is probably the most straightforward\u2014though it still has a good deal of moving parts.\nTo define imitative amplification, we\u2019ll first define\nAmp\n(\nM\n)\n\u2014the \u201camplification operator\u201d\u2014as the procedure where a human\nH\nanswers a question with access to a model\nM\n.\n[2]\nA diagram of the amplification operator\nAmp\n(\nM\n)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "where white arrows indicate information transfer,\nQ\nis a question,\nA\nis\nAmp\n(\nM\n)\n\u2019s answer,\nH\nis a human, and\nM\nis the model.\nThen, imitative amplification is just the procedure of iteratively training\nM\nto imitate\nAmp\n(\nM\n)\n.\nThe basic imitative amplification setup where green arrows indicate amplification, gray arrows indicate training, and cyan arrows indicate the imitative amplification loss.\nFinally, we can define imitative amplification + intermittent oversight\u2014which is the full approach we want to consider here\u2014as the combination of imitative amplification with intermittent oversight of\nM\nby\nAmp\n(\nM\n)\nwhenever the target model changes. Specifically, we want\nAmp\n(\nM\n)\nto look for\ndeceptive\nor otherwise\ncatastrophic\nbehavior in\nM\nby utilizing things like transparency tools and adversarial attacks.\nImitative amplification plus intermittent oversight where red arrows indicate oversight (from the overseer to the overseen model).\nOuter alignment.\nSince imitative amplification trains\nM\nto imitate\nAmp\n(\nM\n)\n, it limits\n[3]\nto the fixed point of the\nAmp\noperator, which Paul Christiano calls\nHCH\nfor \u201cHumans Consulting HCH.\u201d HCH is effectively a massive tree of humans consulting each other to answer questions.\nA (partial) diagram of HCH where white arrows indicate information transfer.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Thus, whether imitative amplification is outer aligned is dependent on whether HCH is aligned or not. HCH\u2019s alignment, in turn, is likely to depend heavily on the specific humans used and what sort of policy they\u2019re implementing. The basic idea, however, is that since the limit is composed entirely of humans\u2014and since we can control what those humans do\u2014we should be able to ensure that all the optimization power is coming from the humans (and not from memetic selection, for example), which hopefully should allow us to make it safe. While there are certainly valid concerns with the humans in the tree accidentally (or purposefully) implementing some misaligned policy, there are\npossible things you can do to address these problems\n.\nInner alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The hope here is to be able to detect and fix serious inner alignment failures via the use of intermittent oversight\u2014that is, inspection by a human with access to a previous version of the model. Thus, the question of inner alignment here is dependent on the question of whether such an overseer would be able to detect deceptive or otherwise catastrophic behavior in the model. Ideally, if such detection is working, it should be clear as soon as the model starts to become misaligned, giving us the opportunity to either fix the issue or find a new approach. For that to work, however, it will likely need to be the case that the intermittent oversight is performed regularly enough to catch any deception as soon as it starts appearing since once a model becomes deceptive it might be significantly harder to detect problems if it starts trying to obfuscate its deception.\nThat being said, while detecting deceptive cognition early enough directly might be quite difficult, it might be possible to instead detect deceptive behavior indirectly by verifying a condition that rules out deception while still being relatively easy to verify such as myopia. In this context, a myopic model would be one that determines its output based on some fixed standard such as distance to HCH while a non-myopic model would be one that determines its output based on its consequences.\n[4]\nFor a more detailed explanation of how such myopia verification might work, see \u201c",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Towards a mechanistic understanding of corrigibility\n.\u201d\n[5]\nAdditionally, because we do intermittent oversight whenever we change the target network, we have a guarantee that the overseer is always at least as strong as any targets that the model was trained to imitate. That being said, the model could end up stronger than the targets it was trained to imitate if it manages to, for example, achieve significant compression of what the targets are doing\u2014which in fact is necessary for training competitiveness. Furthermore, even if the overseer is stronger than the model, the offense-defense balance could be such that a deceptive model would still be able to hide its deception. Which direction the offense-defense balance lies in is likely to depend heavily on what sorts of transparency tools the overseer has access to.\nTraining competitiveness.\nFirst, the training competitiveness of imitative amplification is likely to depend on whether pure imitation can be turned into a rich enough reward signal to facilitate highly sample-efficient learning. In my opinion, it seems likely that human language imitation (where language includes embedded images, videos, etc.) combined with techniques to improve sample efficiency\n[6]",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "will be competitive at some tasks\u2014namely highly-cognitive tasks such as general-purpose decision-making\u2014but not at others, such as fine motor control. If that\u2019s true, then as long as the primary economic use cases for AGI fall into the highly-cognitive category, imitative amplification should be training competitive. For a more detailed analysis of this question, see \u201c\nOuter alignment and imitative amplification\n.\u201d\nSecond, imitative amplification + intermittent oversight could also fail to be training competitive if it is very difficult for the intermittent oversight check to ever pass. This could happen if, for example, the training process for imitative amplification doesn\u2019t naturally produce models that are transparent enough to facilitate such a check. That being said, what counts as \u201ctransparent enough\u201d again depends heavily on the specifics of the transparency tools that the overseer has access to.\nPerformance competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The performance competitiveness question here, as with the outer alignment question, is heavily dependent on the properties of HCH, as imitative amplification is specifically designed to limit toward HCH. In particular, performance competitiveness is dependent on whether HCH is sufficiently universal so as to be capable of solving whatever hard problems we might want advanced AI to solve. For example, if trees of humans can\u2019t replicate intuitive flashes of insight, and such flashes of insight are necessary to solve the sorts of highly-cognitive tasks we want AGI for, then imitative amplification would fail to be performance competitive. Alternatively, as long as it is always possible for a human to break down complex tasks that they can\u2019t answer themselves into easier subtasks, then imitative amplification should be performance competitive. Note that it doesn\u2019t have to be the case that this work for an arbitrary human without any training\u2014only for the sort of human from whom you would actually be collecting training data.\n3. Imitative amplification + relaxed adversarial training\nThe basic idea here is very similar to the previous approach in that we start out with pure imitative amplification.\nThe basic imitative amplification setup.\nHowever, instead of doing intermittent oversight checks, we instead train directly on the intermittent oversight signal\u2014that is, we jointly train\nM\non the imitation loss and on decreasing the probability of catastrophe as assessed by\nAmp\n(\nM\n)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u2014yielding a\nrelaxed adversarial training setup\n.\nImitative amplification plus relaxed adversarial training where the red arrows indicate the oversight loss.\nOuter alignment.\nImitative amplification + relaxed adversarial training should effectively have the same outer alignment properties as imitative amplification + intermittent oversight. If we assume that it is possible to imitate HCH in a demonstrably non-catastrophic way,\n[7]\nthen the catastrophe-checking imposed by either relaxed adversarial training or intermittent oversight shouldn\u2019t affect the theoretical limiting behavior. Such a limit assumes an infinite training distribution, however, which in practice is, of course, impossible, leaving open the possibility for the two different approaches to have quite different behavior in the finite data regime, giving them potentially quite different inner alignment properties.\nInner alignment.\nCompared to intermittent oversight, relaxed adversarial training could have very different inner alignment properties. Intermittent oversight requires performing oversight after a period of training during which the model could potentially have become deceptive. Thus, intermittent oversight could potentially require catching deception after the fact, giving the deceptive model the opportunity to try to obfuscate its own cognition to trick the overseer.\n[8]",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Relaxed adversarial training, by contrast, has the potential to avoid deception before it ever occurs by providing a consistent gradient away from deception during the entirety of training, thus avoiding the issue of having to detect deception after the fact. Similarly to intermittent oversight, such a gradient could be achieved by having the overseer enforce a condition such as\nmyopia\nwhich is simple enough to reliably enforce while guaranteeing that all models which meet the condition are non-deceptive. However, relaxed adversarial training does come with the possible risk of overfitting the oversight signal, potentially resulting in obfuscated models that pass oversight despite not actually meeting the condition simply by virtue of having been optimized to do so.\nTraining competitiveness.\nSimilarly to imitative amplification + intermittent oversight, imitative amplification + relaxed adversarial training could fail to be training competitive either because imitative amplification is uncompetitive or because the oversight check is too hard to pass. On the one hand, relaxed adversarial training could require significantly more total oversight bandwidth, which could make training more difficult. On the other hand, unlike with intermittent oversight, relaxed adversarial training directly trains the model to pass the check, which could significantly boost training competitiveness by increasing the likelihood of the check passing.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "For relaxed adversarial training to be more training competitive than intermittent oversight, it needs to the case that it significantly increases the probability of oversight working, which is likely to depend on the overfitting question of whether relaxed adversarial training is likely to overall increase or decrease model transparency. The critical question here is going to be how hard it is to find a model that is demonstrably non-catastrophic under the two different oversight regimes. For a more detailed analysis of this question, see \u201c\nRelaxed adversarial training for inner alignment\n.\u201d\nPerformance competitiveness.\nSimilarly to the outer alignment question, the performance competitiveness properties of imitative amplification + intermittent oversight and imitative amplification + relaxed adversarial training should be effectively the same, with performance competitiveness here highly dependent on the competitiveness of HCH.\n4. Approval-based amplification + relaxed adversarial training\nThe idea behind approval-based amplification is to substitute the imitation signal in imitative amplification with an approval signal, training\nM\nto maximize the approval of\nAmp\n(\nM\n)\n. This gives us a myopic reward signal which can then be optimized via the use of\nsome\nsort\nof myopic reinforcement learning.\nApproval-based amplification where the blue arrows indicate the approval loss.\nWe can then add on top of that a relaxed adversarial training signal, training\nM",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to also minimize the probability of catastrophe as assessed by\nAmp\n(\nM\n)\n.\nApproval-based amplification + relaxed adversarial training where the red arrows indicate the oversight loss.\nAdditionally, it is worth noting that, in practice, since oversight can be treated as a form of approval, you can just compute one approval signal that includes both metrics rather than two distinct approval signals.\nOuter alignment.\nFor the purposes of outer alignment and performance competitiveness, we have to understand what exactly the theoretical limit of approval-based amplification is, which in general is not HCH. Instead, approval-based amplification limits to the following tree, where blue arrows indicate approval-maximization (such that\nM\nis maximizing the approval of\nH\nconsulting\nM\n\u2032\nwhich is maximizing the approval of\nH\nconsulting\nM\n\u2032\n\u2032\nand so on).\n[9]\nThe top of the approval-based amplification limiting tree where white arrows indicate information transfer, blue arrows indicate approval maximization,\nH\nare humans, and\nM\nare models.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As you can see from the tree, the limit of approval-based amplification involves humans interspersed with models attempting to maximize the approval of their descendants. Thus, unlike the HCH tree, which was composed only of humans, analyzing the approval-based amplification tree involves analyzing the behavior of arbitrary models trained on approval-maximization signals\u2014and, in fact, many such models as they appear all throughout the tree each with a different subtree that they are maximizing the approval of.\n[10]\nAs a result, producing an outer alignment guarantee regarding such a tree seems quite difficult, especially given the distinct possibility of the models gaming the approval signal. For additional detail on this question, see \u201c\nOuter alignment and imitative amplification\n.\u201d\nInner alignment.\nWe are still using relaxed adversarial training here, so for inner alignment to work we need the same guarantee from before of a consistent gradient away from deception. Note that we can still try to have\nAmp\n(\nM\n)\nenforce\nmyopia\nas a way of getting that gradient, as approval-maximization is a single-step reward signal that is entirely compatible with a myopic agent.\n[11]\nIn the approval-based setting, however,\nAmp\n(\nM\n)\nis a different entity that limits to a different tree, meaning it could have different properties regarding how good its oversight signal is. In particular, the fact that approval-based amplification directly incentivizes\nM\nto fool\nH",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "into approving of it could result in\nM\nbeing more prone to deception, more likely to hide facts from\nH\nduring oversight, and/or more likely to obfuscate its internals if internals-checking is part of the oversight signal. That being said, it could also be that relaxed adversarial training works better in an approval-based setting if it results in a more powerful\nAmp\n(\nM\n)\nthat is better at performing oversight because\nM\nwas trained to provide more helpful information.\nTraining competitiveness.\nTraining competitiveness could be quite different with approval-based amplification compared to imitative amplification, as approval provides a very different training signal than imitation. In particular, the standard maxim that it is generally easier to evaluate behavior than produce it seems to suggest that human approval should produce a significantly richer training signal than the simple automated distance metric in imitative amplification, resulting in more efficient training. This isn\u2019t necessarily the case, however\u2014an automated distance metric as computed by a powerful language model, for instance, could still produce a fairly rich (and more nicely differentiable) training signal.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Additionally, approval-based amplification could also have different training competitiveness properties in terms of the likelihood of the relaxed adversarial training check passing. If approval-based amplification leads to models with more obfuscated internals, for example\u2014perhaps because the model is incentivized to obfuscate how it\u2019s gaming the approval signal\u2014then that could hurt training competitiveness, as it could make it harder for\nAmp\n(\nM\n)\nto become confident that\nM\nis safe (in addition to hurting inner alignment by making\nM\nin fact less likely to be safe).\nPerformance competitiveness.\nPerformance competitiveness for approval-based amplification depends on the properties of the approval-maximization tree given previously. Compared to HCH, it is quite possible for that tree to be either more or less competitive.\nOn the one hand, approval-maximizing models might be able to produce ideas/considerations/arguments that no human would have ever come up with, enabling the approval-maximization tree to be more competitive. In particular, if humans are significantly better at knowing good ideas when they see them than producing good ideas themselves\u2014which fits with the standard maxim that it\u2019s easier to evaluate behavior than produce it\u2014then approval-based amplification should be more performance competitive than imitative amplification.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "On the other hand, approval-maximizing models might game the approval signal by producing convincing-sounding but ultimately bad ideas/considerations/arguments that end up hurting performance competitiveness. For example, an idea could be initially very compelling and thus get high approval despite quite quickly revealing itself to be useless/vacuous/etc. when actually tested out. In such a situation where the bad ideas quickly reveal themselves, the performance competitiveness problems would likely be quite clear, though if it takes a very long time for the bad ideas to reveal themselves, then approval-maximization might initially look a lot better from a performance competitiveness standpoint than it actually is.\n5. Microscope AI\nMicroscope AI\nis a fairly unique proposal which is designed to bypass some of the dangers of building highly agentic AGI systems by leveraging powerful transparency tools. The basic proposal is as follows.\nTrain a predictive model on some set of data that you want to understand while using transparency tools to verify that the model isn\u2019t performing any optimization.\nUse transparency tools to understand what the model learned about the data and use that understanding to guide human decision-making.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Microscope AI is predicated on the basic insight that using transparency tools on a model doesn\u2019t just teach us about that model\u2014it also gives us useful information about the data that the model was trained on. Chris Olah talks about this basic phenomenon in his post \u201c\nVisualizing Representations: Deep Learning and Human Beings\n:\u201d\nThe visualizations are a bit like looking through a telescope. Just like a telescope transforms the sky into something we can see, the neural network transforms the data into a more accessible form. One learns about the telescope by observing how it magnifies the night sky, but the really remarkable thing is what one learns about the stars. Similarly, visualizing representations teaches us about neural networks, but it teaches us just as much, perhaps more, about the data itself.\nThe basic goal of microscope AI is to produce high-quality knowledge that can inform important decision-making rather than to produce powerful AGI systems that can make those decisions themselves.\nOuter alignment.\nPaul Christiano\u2019s argument that the universal prior is malign\nsuggests that there might be deceptive optimization in the limit of perfect prediction. As a result, if we interpret outer alignment as referring to\nthe behavior at minimal loss",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", it seems likely that microscope AI is not outer aligned. That doesn\u2019t necessarily rule out microscope AI as a proposal, however\u2014whether such deceptive optimization would actually occur in practice is still a very open question. In particular, the argument that the universal prior is malign relies on an uncomputable limit\u2014polynomial approximations, for example, could still be completely fine. Furthermore, the inner alignment technique proposed here of using transparency tools to detect and prevent any optimization could prevent this problem entirely if successful.\nInner alignment.\nSince the goal here is not to produce a model performing optimization (a mesa-optimizer) in the first place, the question of inner alignment becomes rather different in this situation. We still want to avoid a deceptive or otherwise misaligned mesa-optimizer, but we want to do so by avoiding all mesa-optimization rather than by ensuring that any mesa-optimization is aligned. Specifically, the strategy here is to use transparency tools to detect any optimization (as opposed to just predictive modeling) and halt if we see anything of that form. This has the major benefit of sidestepping a lot of the most pernicious inner alignment concerns, but it also comes with lots of its own problems. Whether this will work is likely to be dependent on a couple of different factors.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "First, it is an open question of whether there exists a sensible way in which optimization and modeling can be distinguished from each other. It seems quite likely that any world model which admits queries of the form \u201cwhat is the probability that\na\nis the optimal action according to utility\nu\n\u201d or \u201cwhat is the best action for agent\nA\nto take in state\nx\n\u201d will implicitly have to be running some sort of optimization algorithm. Thus, any notion of a \u201cpure world model\u201d would likely have to either rule out such queries, ensure they are being estimated without the use of optimization, or otherwise find some way to ensure that they are safe. For a more thorough treatment of the difficulty here, see Abram Demski\u2019s \u201c\nPartial Agency\n\u201d sequence.\nSecond, assuming it is possible to distinguish between modeling and optimization in theory, it also needs to be possible in practice given whatever set of transparency tools are available. If the model can start performing optimization and potentially become deceptive before its optimization can be detected, that would obviously pose a major problem to this sort of approach.\nTraining competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Microscope AI relies on being able to competitively train predictive models with useful internal representations. Thus, the question of training competitiveness for microscope AI comes down to the question of whether the process of training a predictor is a competitive way to extract useful information from a dataset. Given the success of current predictive models such as\nGPT-2\n, it seems likely that it will be competitive to train powerful predictors\u2014though it remains to be seen how competitive the process of extracting what they\u2019ve learned will be. How competitive that extraction process is seems likely to depend heavily on what sort of state-of-the-art transparency tools are available.\nPerformance competitiveness.\nPerformance competitiveness is perhaps the biggest question mark regarding microscope AI, as microscope AI forgoes producing AI agents which directly take actions in the world. The question of performance competitiveness for microscope AI is thus the question of whether enhanced human understanding alone\u2014without AI agents\u2014is sufficient for the economic use cases where one might otherwise want highly agentic advanced AI (e.g. an AGI).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This question is likely to depend heavily on what exactly those use cases are. Like with amplification, if you need lots of fine motor control, microscope AI is unlikely to get you there. Furthermore, unlike amplification, if you need lots of low-level decision-making where it\u2019s too expensive to hire a human, microscope AI won\u2019t help much there either (whereas amplification would be fine). Potentially microscope AI could give humans the knowledge to safely build other systems which could solve such tasks, however. Furthermore, if the primary use case for AGI is just high-level big-picture decision-making (automating CEOs or doing AI research, for example), then it seems likely that microscope AI would have a real shot of being able to address those use cases. In that sort of a situation\u2014where you\u2019re only trying to make a small number of high-quality decisions\u2014it seems likely to be fairly cheap to have a human in the loop and thus simply improving that human\u2019s knowledge and understanding via microscope AI might be sufficient to produce competitive decision-making. This is especially true if there is a market premium on having a human making the decisions, perhaps because that makes it easier to negotiate or work with other humans.\n6. STEM AI",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "STEM AI is a very simple proposal in a similar vein to microscope AI. Whereas the goal of microscope AI was to avoid the potential problems inherent in building agents, the goal of STEM AI is to avoid the potential problems inherent in modeling humans. Specifically, the idea of STEM AI is to train a model purely on abstract science, engineering, and/or mathematics problems while using transparency tools to ensure that the model isn\u2019t thinking about anything outside its sandbox.\nThis approach has the potential to produce a powerful AI system\u2014in terms of its ability to solve STEM problems\u2014without relying on any human modeling. Not modeling humans could then have major benefits such as ensuring that the resulting model doesn\u2019t have the ability to trick us to nearly the same extent as if it possessed complex models of human behavior. For a more thorough treatment of why avoiding human modeling could be quite valuable, see Ramana Kumar and Scott Garrabrant\u2019s \u201c\nThoughts on Human Models\n.\u201d\nOuter alignment.\nSimilarly to microscope AI, it seems likely that\u2014in the limit\u2014the best STEM AIs would be malign in terms of having convergent instrumental goals which cause them to be at odds with humans. Thus, STEM AI is likely not outer aligned\u2014however, if the inner alignment techniques being used are successful at preventing such malign optimization from occurring in practice (which the absence of human modeling could make significantly easier), then STEM AI might still be aligned overall.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Inner alignment.\nThe hope with STEM AI is that by preventing the model from ever considering anything outside its STEM sandbox, the malign limiting behavior that might cause it to fail to be outer aligned can be avoided. Unfortunately, such a sandboxing condition alone isn\u2019t quite sufficient, as a model considering only things in its sandbox could still end up creating other models which would consider things outside the sandbox.\n[12]\nThus, exactly what the correct thing is to do in terms of inner alignment for a STEM AI is somewhat unclear. In my opinion, there are basically two options here: either do something similar to microscope AI and try to prevent all mesa-optimization or do something similar to amplification and ensure that all mesa-optimization that occurs is fully\nmyopic\n. In either case, the hope would be that the absence of human modeling makes it easier to enforce the desired condition (because modeling an agent such as a human increases the propensity for the model to become agentic itself, for example).\nTraining competitiveness.\nTraining competitiveness for STEM AI is likely to depend heavily on how hard it is for state-of-the-art machine learning algorithms to solve STEM problems compared to other domains such as language or robotics. Though there exists lots of current progress in the field of applying current machine learning techniques to STEM problems such as\ntheorem\nproving\nor\nprotein folding",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", it remains to be seen to what extent the competitiveness of these techniques will scale, particularly in terms of how well they will scale in terms of solving difficult problems relative to other domains such as language modeling.\nPerformance competitiveness.\nSimilarly to microscope AI, performance competitiveness is perhaps one of the biggest sticking points with regards to STEM AI, as being confined solely to STEM problems has the major potential to massively limit the applicability of an advanced AI system. That being said, many purely STEM problems such as\nprotein folding\nor\nnanotechnology development\nhave the potential to provide huge economic boons that could easily surpass those from any other form of advanced AI as well as solve major societal problems such as curing major illnesses. Thus, if the answer to the reason that you want to build advanced AI in the first place is to get such benefits, then STEM AI might be a perfectly acceptable substitute from a performance competitiveness standpoint. Furthermore, such boons could lead to a\ndecisive strategic advantage\nthat could enable heavy investment in aligning other forms of advanced AI which are more performance competitive.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "However, if one of the major use cases for your first advanced AI is helping to align your second advanced AI, STEM AI seems to perform quite poorly on that metric, as it advances our technology without also advancing our understanding of alignment. In particular, unlike every other approach on this list, STEM AI can't be used to do alignment work, as its alignment guarantees are explicitly coming from it not modeling or thinking about humans in any way, including aligning AIs with them. Thus, STEM AI could potentially create a\nvulnerable world\nsituation where the powerful technology produced using the STEM AI makes it much easier to build advanced AI systems, without also making it more likely that they will be aligned.\nThis problem could potentially be mitigated if the STEM AI were heavily focused on applications that could potentially assist with alignment such as\nwhole brain emulation\n, though to what extent that would actually be possible or actually help with alignment is quite unclear.\n7. Narrow reward modeling + transparency tools\nThe approach here is as follows:\nFor some particular problem/domain/environment, jointly train a reward model using human feedback and an agent that pursues that reward. Specifically, we can use an approach like that described in DeepMind Safety\u2019s \u201c\nScalable agent alignment via reward modeling\n\u201d where human feedback on agent trajectories can be used to refine the reward model, as seen below.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "DeepMind Safety\u2019s diagram of reward learning as seen in \u201c\nScalable agent alignment via reward modeling\n.\u201d\nSince we want to keep our agent confined to only the specific distribution on which it was trained (thus helping prevent possible perverse generalizations), we can also incorporate\nout-of-distribution detection\nto train the agent to halt if it encounters a sufficiently out-of-distribution state.\nWe can also have the human provide additional feedback on the reward model\u2019s internal behavior via the use of transparency tools.\nReward modeling plus oversight of the reward model by the user.\nAnd we can do the same thing with the agent, giving us a similar sort of mulligan as with reinforcement learning + transparency tools.\nReward modeling plus oversight of both the reward model and the agent by the user.\nOuter alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The question of outer alignment here is going to depend on whether the various feedback mechanisms are sufficient to ensure that, in the limit, the learned reward model is correct over the domain where the action model believes that it is on distribution. This is a highly complex question that depends on a bunch of factors including how good the human\u2019s trajectory and transparency feedback is, how hard it is to train an on/off distribution model, whether the reward model will itself develop optimization in the limit, the extent to which it\u2019s possible for the agent to trick the user into thinking it\u2019s doing a good job when it\u2019s not, etc. Because outer alignment here is dependent on the limiting behavior of such a complex, dynamic process, it\u2019s very hard to determine one way or the other how outer aligned such a proposal is likely to be.\nInner alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The use of transparency tools in this approach is actually quite similar to the various relaxed adversarial training approaches since the feedback mechanism here allows the oversight signal coming from the human to be incorporated directly into training. That being said, unlike in relaxed adversarial training\u2014and similarly to the \u201cmulligan\u201d approach in RL + transparency tools\u2014the human doing the oversight here gets no help from the model. Thus, inner alignment for narrow reward modeling + transparency tools comes down to the question of whether a human using transparency tools to perform oversight can prevent the development of\ndeceptive\nor otherwise\ncatastrophic\nbehavior in either the reward model or the agent. This could be done, for example, by verifying that the reward model isn\u2019t performing any optimization (as in microscope AI) and verifying that the agent is\nmyopic\n(as was previously suggested could work for amplification).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Such an approach could potentially require that the reward be myopic, however, which might be a fairly serious limitation on the applicability of narrow reward modeling if it was restricted to only those domains with myopic rewards. While it is not necessarily the case that you can\u2019t do myopia verification with a non-myopic reward\u2014you just need to verify that the model treats each episode myopically rather than each step\u2014per-episode myopia verification could be significantly more difficult than per-step myopia verification, as it is potentially a much less natural behavior.\nTraining competitiveness.\nThe question of training competitiveness here is going to depend on the extent to which reward modeling is a good (compute-efficient, sample-efficient. etc.) way of training AI systems to solve poorly-defined, fuzzy tasks. There are lots of different approaches to solving this basic problem, including not just reward learning approaches but also imitation learning and\nvarious\ndifferent",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "inverse reinforcement learning schemes, and specifically what approach ends up coming out on top still seems quite uncertain. That being said, reward modeling has the major competitiveness advantage of only requiring and depending on human feedback, not human demonstrations, which could be significantly more reliable and easier to elicit. Furthermore, other reward learning schemes such as inverse reinforcement learning can be incorporated into reward modeling by using them to produce a better initial reward model that can then be refined via reward modeling\u2019s feedback mechanism.\nPerformance competitiveness.\nSimilarly to microscope AI or STEM AI, a potentially major concern with the narrow reward modeling + transparency tools approach is the \u201cnarrow\u201d part. While being narrow has potential alignment advantages in terms of reducing reliance on potentially shaky or even malign generalization, it also has the major disadvantage of restricting the approach\u2019s usefulness to only producing relatively narrow advanced AI systems. Thus, the performance competitiveness of narrow reward modeling + transparency tools is likely to depend heavily on the extent to which truly general advanced AI systems are actually practically feasible and economically necessary. For a more detailed analysis of this question, see Eric Drexler\u2019s \u201c\nReframing Superintelligence\n.\u201d\n8. Recursive reward modeling + relaxed adversarial training\nRecursive reward modeling",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", as the name implies, is a sort of recursive, non-narrow version of narrow reward modeling. What this results in is effectively a form of amplification where the distillation step which was previously imitation or approval-maximization becomes reward modeling. Specifically, the basic approach here is to train a model\nM\nto maximize the reward obtained by performing reward learning on\nAmp\n(\nM\n)\n.\nTwo different, equivalent diagrams of recursive reward modeling. The top diagram is taken from \u201c\nScalable agent alignment via reward modeling\n\u201d and the bottom diagram is the equivalent amplification-style diagram where the purple arrows indicate the use of the full reward modeling process.\nIn this graphic, the images on the top and bottom are meant to represent the same process\u2014specifically, if you take the purple arrow in the bottom image to represent reward modeling, and assume that the agents in the top image are all the same agent just at different time steps,\n[13]\nthen you get precisely the same procedure represented in both images. Thus, the agent in the largest circle in the top picture is\nM\n2\nin the bottom picture, the user providing feedback for that agent is\nAmp\n(\nM\n1\n)\n, the agent in the medium-sized circle is\nM\n1\n, the user in the medium-sized circle is\nAmp\n(\nM\n0\n)\n, and the agent in the smallest circle is\nM\n0\n.\nFurthermore, similarly to the other amplification approaches, we can then add relaxed adversarial training by training\nM",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "and the reward model to minimize the probability of\ndeceptive\nor otherwise\ncatastrophic\nbehavior as assessed by\nAmp\n(\nM\n)\n.\n[14]\nIt\u2019s also worth noting that recursive reward modeling is amenable to intermittent oversight as well if desired instead of relaxed adversarial training.\nRecursive reward modeling plus relaxed adversarial training where the red arrows indicate the oversight loss.\nOuter alignment.\nThe first step in understanding the outer alignment properties of recursive reward modeling is figuring out what exactly the theoretical limit of the training process is. While we don\u2019t know exactly what the limiting behavior of an individual instance of reward modeling is\u2014as was noted previously for narrow reward modeling\u2014we can express the limit of the recursive version in terms of many individual reward modeling limits. Specifically, we can unroll the recursive reward modeling process out over time to produce a limiting tree similar to the one given for approval-based amplification, except with approval-maximization replaced with reward modeling.\nThe recursive reward modeling limiting tree where white arrows indicate information transfer, purple arrows indicate reward modeling,\nH\nare humans, and\nM\nare models.\nThe purple arrows in this diagram represent perfect reward modeling where\nM\nis the model trained to maximize the reward obtained by performing reward modeling on\nH\nconsulting\nM\n\u2032",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". Now, whether this tree is aligned or not is a very open question. Similarly to approval-based amplification\u2014but unlike imitative amplification\u2014it\u2019s difficult to form a clear model of what exactly this tree would do, as it not only involves humans but also models that are the limit of many individual instances of reward modeling\u2014limits which could potentially involve deceptive or otherwise malign optimization.\nInner alignment.\nThe question of inner alignment here is mostly going to fall on the efficacy of the relaxed adversarial training. Such efficacy could be quite different than with other amplification approaches, however, as both the model helping the human perform oversight and the model being overseen are trained via a very different process in recursive reward modeling. In particular, if the reward model is non-myopic, recursive reward modeling could rule out the possibility of using per-step\nmyopia verification",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u2014as was suggested for the other amplification approaches\u2014though per-episode myopia verification could still be possible, as with narrow reward modeling. If per-episode myopia verification is not tenable, however, then an alternative condition that rules out deception while being possible to verify for agents trained via recursive reward modeling might need to be found. Furthermore, if reward modeling has a greater tendency to produce deception than imitation learning, oversight could be significantly harder with recursive reward modeling than with imitative amplification even if such a condition is found. Alternatively, if recursive reward modeling helps produce models that are more capable of assisting with oversight\u2014because reward modeling is more capable of training models to effectively apply transparency tools than imitation learning is, for example\u2014then relaxed adversarial training could work better with recursive reward modeling.\nTraining competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The training competitiveness of recursive reward modeling depends on the effectiveness of reward modeling not just as an efficient way of training a model to solve a single fuzzy task\u2014as in narrow reward modeling\u2014but instead the effectiveness of reward modeling in training a general model which can solve an entire collection of fuzzy tasks. That being said, many of the nice training competitiveness properties of reward learning continue to apply even in the recursive setting. For example, unlike imitative amplification\u2014but similarly to approval-based amplification\u2014recursive reward modeling relies only on human feedback rather than on human demonstrations. Furthermore, compared to approval-based amplification, recursive reward modeling is non-myopic, which could allow it to solve credit assignment problems that might be difficult for approval-based amplification.\nPerformance competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Performance competitiveness for recursive reward modeling will depend on the competitiveness of its aforementioned limiting tree. Compared to HCH, the recursive reward modeling tree can consider ideas that no human would ever produce, potentially increasing competitiveness. And compared to the approval-maximization tree, the recursive reward modeling tree can learn to execute long-term strategies that short-term approval maximization wouldn\u2019t incentivize. That being said, both of these facets of recursive reward modeling have the potential for danger from an alignment perspective. Furthermore, if the different models in the recursive reward modeling tree each assign some different value to the final output\u2014which could happen if the models are not per-episode myopic\u2014they could try to jockey for control of the tree in such a way that not only hurts alignment but also competitiveness.\n9. AI safety via debate with transparency tools\nThere are many different forms of\nAI safety via debate\n, but the approach we\u2019ll be considering here is as follows:\nTrain a model (\u201cAlice\u201d) to win debates against a copy of itself (\u201cBob\u201d) in front of a human judge.\nThe basic debate setup where Alice is the first debater and Bob is the second debater. Blue lines indicate possible arguments that Alice can make and the red lines indicate possible arguments that Bob can make.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Give the models the ability to cross-examine previous versions of the other model (e.g. Bob_2 can ask Alice_1 questions for which the answers are visible to the human judge). See Beth Barnes and Paul Christiano\u2019s \u201c\nWriteup: Progress on AI Safety via Debate\n\u201d for a thorough explanation of the cross-examination procedure and its benefits.\nDebate with cross-examination where the orange arrow is an example of cross-examination where Bob (specifically Bob_2, the second Bob) cross-examines a copy of Alice (Alice_1) from two moves previously in the debate.\nGive the models access to transparency tools and incentivize them to try to use them to identify\ndeceptive\nor otherwise\ncatastrophic\nbehavior in the other debater to win the debate (e.g. Alice_2 can inspect Bob_1\u2019s internals and win the debate by presenting evidence of problematic cognition). Combined with cross-examination, debaters should be able to inspect versions of the other debater from arbitrary prior points in the debate.\nDebate with cross-examination and transparency tools where the red arrow is an example of Alice_2 using transparency tools on Bob_1.\nOuter alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Outer alignment for debate depends on whether giving honest, cooperative, helpful advice is the strategy with the greatest probability of winning. Since the first debater gets to choose to defend whatever position they want, in the limit they\u2019ll choose to defend whatever position gives them the greatest probability of winning the debate. Thus, if honest, cooperative, helpful strategies dominate, then the first player will always pursue them in the limit, meaning that aligned answers can be produced at test/deployment time simply by eliciting the first debater\u2019s position. Whether honesty and helpfulness actually dominate in the limit is a very open question, however, and one that depends heavily on many factors such as the set of questions being asked, the effectiveness of the human judges, the length of the debate, etc. For example, if there is significant variation in how difficult different positions are to defend that is independent of their truthfulness (perhaps because arguments with more moving pieces that depend on more things are harder to defend regardless of how true they are) then it seems likely that there would be instances where the truth wouldn\u2019t dominate. That being said, careful mechanism design such as the use of cross-examination could potentially help mitigate such issues.\nInner alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The question of inner alignment for debate with transparency tools centers on the effectiveness of the debaters in using the transparency tools to oversee each other. The goal is to directly incentivize the debaters to look for deceptive or otherwise catastrophic behavior in each other using the available tools and present that information to the judge to win the debate. Whether such an approach is actually likely to succeed is dependent on a couple of different factors, however.\nFirst, it\u2019s unclear what sort of condition could be enforced in a debate setup that would successfully rule out\ndeceptive alignment\nwhile still allowing for the necessary deception of the second debater, who is forced to defend the second-most probable option, and thus make arguments that they don\u2019t actually believe in. Per-episode myopia could potentially present such a condition, though it might be a difficult condition to enforce.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Second, getting a good training signal for incentivizing such transparency tool use could be quite difficult. The debaters need a way of learning how to inspect other models, a way of learning that doing so is a way to win the debate, and a way of learning the specific bad behavior that they should be looking for. This gets especially tricky if the condition being enforced is just the absence of deception, as getting training examples of catching deception would seem to require producing deceptive models, which is the very thing you\u2019re trying to avoid doing in the first place.\nTraining competitiveness.\nTraining competitiveness for debate depends on the extent to which the debate game is a compute-efficient, sample-efficient way of training an advanced AI system. Since the debate training process is fundamentally reinforcement learning via self-play on a zero-sum game, the historical success of such methods on environments such as\nGo\n,\nDota 2\n, or\nStarCraft",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "suggests that such self-play might be fairly competitive as a general training procedure. That being said, in comparison to the given environments, debate requires a human in the loop, significantly reducing the amount of training data that can be collected and thus requiring significantly better sample efficiency to train successfully. Such sample efficiency concerns could potentially be resolved via training a supervised model to predict human judging behavior and then training against that, though that could raise many of its own additional safety and/or competitiveness concerns.\nPerformance competitiveness.\nIt has been proven\nthat debate with cross-examination has the property that arbitrarily strong debaters can convince a polynomial-time judge of the answer to any problem in\nNEXP\n. Thus, there is a strong theoretical argument for the performance competitiveness of debate\u2014in the limit, debate has the ability to solve basically any interesting problem that can be solved. While it can be shown that approaches such as approval-based amplification and recursive reward modeling can similarly access\nNP",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", the ability of debate to go significantly beyond NP all the way to NEXP suggests that debate might be significantly more performance competitive than those approaches. Of course, such theoretical analysis doesn\u2019t necessarily apply in practice\u2014in reality, even in the limit of training, no model can ever actually be arbitrarily strong, so the practical difference between accessing NP and accessing NEXP might be very minimal.\n10. Amplification with auxiliary RL objective + relaxed adversarial training\nThe approach here is to train a model\nM\non a joint objective composed of three different pieces. First, we\u2019ll train\nM\nto imitate\nAmp\n(\nM\n)\n\u2014that is, the basic imitative amplification loss.\nThe basic imitative amplification setup.\nSecond, we\u2019ll also train\nM\nso as to cause\nAmp\n(\nM\n)\nto perform well on some RL objective in some environment. The idea here is to treat\nM\n\u2019s answers to\nH\n\u2019s questions as actions that yield some reward determined by\nH\n\u2019s performance in the environment with access to those answers. Thus, we are training\nM\nto maximize a combination of the standard imitative loss and the RL reward obtained by\nAmp\n(\nM\n)\n.\nImitative amplification with an auxiliary RL objective where the white arrows indicate\nAmp\n(\nM\n)\nacting in the environment and the yellow arrows indicate the RL loss for training\nM\nto maximize the reward obtained by\nAmp\n(\nM\n)\n.\nFinally, we can add relaxed adversarial training by training\nM\nto decrease the probability of catastrophe as assessed by\nAmp\n(\nM\n)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "trying to find adversarial attacks.\nImitative amplification with an auxiliary RL objective plus relaxed adversarial training where the red arrows indicate the oversight loss.\nAs a concrete example of this approach where the RL environment in question is the AI safety via debate game, see \u201c\nSynthesizing amplification and debate\n.\u201d\nOuter alignment.\nAn interesting property of this approach is that its outer alignment properties can be controlled by determining how to anneal the various different components of the loss in the limit. Annealing towards the imitative loss yields HCH as the limit\u2014giving this approach the same outer alignment properties as imitative amplification\u2014whereas annealing towards the RL loss yields whatever the dominant behavior in the given RL environment is as the limit. Given that HCH is generally significantly easier to understand than RL limiting behavior, it seems likely that in the majority of cases you will want to anneal towards the imitative loss to get the best outer alignment properties. That being said, there are other options\u2014for example, you could use the imitative loss as a constraint such that the model is trained to do well on the RL objective without deviating from HCH by more than some fixed amount. Whether such an approach is outer aligned or not is likely to depend heavily on the specific metric used to measure the distance from HCH as well as the robustness of HCH to small deviations according to that metric.\nInner alignment.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We\u2019re using relaxed adversarial training here, which means that\u2014as with all the other relaxed adversarial training approaches\u2014inner alignment is likely to come down to whether or not the overseer is able to produce a consistent gradient away from deception, which as always could be quite different than with other approaches since both the model helping with oversight and the model being overseen are being trained differently. Compared to imitative amplification, the addition of an RL objective could help significantly if performance in the RL environment is highly correlated to oversight effectiveness. Furthermore, the fact that the model is being trained to maximize the performance of the human with access to the model means that the model is forced to learn how to present its information to a human to get reward, which could make it significantly more transparent and help with oversight. On the other hand, explicitly training the model to produce answers which change a human\u2019s behavior could also encourage the model to trick the human, which could make deception more likely and thus oversight harder. Additionally, training on an RL objective could make it significantly harder to do\nmyopia verification\nas optimizing even partially for a non-myopic RL loss could act as a strong force pushing the model away from myopia.\nTraining competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Compared to imitative amplification, the addition of an RL loss could significantly improve training competitiveness by producing a much richer reward signal, though at the cost of having to run the RL environment. Furthermore, the addition of the imitative loss could even improve training competitiveness relative to pure RL by providing a less sparse reward. Additionally, similarly to recursive reward modeling, incorporating an RL loss enables directly training the model to solve long-term tasks involving difficult credit assignment problems that might be hard for imitative amplification alone to handle (though similarly to recursive reward modeling this trades off with the potential safety benefits of myopia).\nThat being said, it is unclear what happens to that behavior if the RL loss is annealed away\u2014ideally, if it is possible for HCH to produce the behavior, then hopefully the model will converge on that, though that requires the optimal RL behavior to be close enough to HCH that first training on the RL loss and then training on the imitative loss actually helps with the imitation task. For that to be the case, success on the RL task likely needs to be highly correlated with good HCH imitation, for which language modeling and human approval maximization tasks (such as the AI safety via debate game as in \u201c\nSynthesizing amplification and debate\n\u201d) could be good candidates.\nPerformance competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As with outer alignment, performance competitiveness is likely to depend heavily on exactly how the different losses are annealed in the limit. Annealing towards the imitative loss, as was suggested previously, makes the limit HCH, giving this approach the same performance competitiveness properties as imitative amplification. As was mentioned previously, however, there are other options, such as using the imitative loss as a constraint such that the model is trained to maximize the RL reward without deviating too far from HCH. Such a constraint could help performance competitiveness compared to training purely on the RL loss\u2014if the resulting model is significantly better at interacting with humans, for example\u2014or hinder performance competitiveness\u2014if it\u2019s very difficult to do well on the RL task while staying close to HCH.\n11. Amplification alongside RL + relaxed adversarial training\nIn this approach, the goal is to simultaneously train two models, a question-answering system\nM\nand an RL agent\n\u03c0\n.\nWe\u2019ll train\n\u03c0\non the joint objective of maximizing the RL reward and decreasing the probability of deceptive or otherwise catastrophic behavior as assessed by\nAmp\n(\nM\n)\n. Note that while we\u2019re using a sort of relaxed adversarial training loss here, the overseer is using a totally different model (\nM\n) than the model being overseen (\n\u03c0\n).\nThe training process for\n\u03c0\nin amplification alongside RL where the white arrow indicates\n\u03c0",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u2019s actions in the environment, the yellow arrow indicates\n\u03c0\n\u2019s RL loss, and the red arrows indicate the oversight loss for\n\u03c0\n.\nThen, we\u2019ll train\nM\non the following joint objective.\nFirst, we want to include the standard imitative amplification loss and train\nM\nto imitate\nAmp\n(\nM\n)\n.\nAmplification alongside RL where\nM\nis trained only via an imitative loss.\nSecond, we want to train\nM\nto cause\nAmp\n(\nM\n)\nto predict\n\u03c0\n\u2019s actions and the RL environment\u2019s responses well. The idea here is to train\nM\nto tell the human everything the human needs to know to understand what\n\u03c0\nis doing and how to replicate\n\u03c0\n\u2019s behavior so that the human with access to the model can match\n\u03c0\n\u2019s performance in the RL environment.\nAmplification alongside RL where the white arrows to\nAmp\n(\nM\n)\nindicate information that\nAmp\n(\nM\n)\nattempts to predict in the RL environment and the orange arrows indicate the prediction loss for training\nM\nto cause\nAmp\n(\nM\n)\nto predict well.\nThird, we can add standard relaxed adversarial training and also train\nM\nto decrease the probability of deceptive or otherwise catastrophic behavior as assessed by\nAmp\n(\nM\n)\n.\nAmplification alongside RL + relaxed adversarial training where the red arrows indicate the oversight loss.\nFinally, once we\u2019ve trained both\nM\nand\n\u03c0\nvia the above losses, a critical component of this approach is that we then do away with\n\u03c0\ncompletely and exclusively deploy the resulting\nM\n.\nOuter alignment.\nSince we only deploy\nM\n, not\n\u03c0",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", the question of outer alignment here should be entirely dependent on whether this procedure produces outer aligned\nM\n. Furthermore, similarly to the previous approach, we can anneal the combination of losses here to get the desired limiting behavior and thus control our outer alignment properties. In particular, if we anneal towards the imitative loss, this approach can be made to limit to HCH, giving it the same outer alignment properties as imitative amplification. Furthermore, even if we don\u2019t anneal towards the imitative loss, as long as it is possible for HCH to fully predict both\n\u03c0\nand the RL environment in the limit and the imitative loss isn\u2019t annealed away completely, then at minimal loss we should still end up with HCH.\nInner alignment.\nInner alignment is particularly tricky for this approach. First, there\u2019s the standard relaxed adversarial training question of whether\nAmp\n(\nM\n)\nwill be able to properly oversee\nM\n. Compared to the previous approach, the fact that the auxiliary reward is purely predictive rather than a full RL loss should help us here, as it means that our loss is entirely myopic and thus we should be able to do\nmyopia verification\n. That being said, if\n\u03c0\nis sufficiently non-myopic, having\nM\npredict\n\u03c0\nwhile still staying myopic itself could be a challenge.\nAdditionally, there\u2019s also the problem of\n\u03c0\nitself\u2014while we throw away\n\u03c0\nat the end, there are still likely to be opportunities for a sufficiently competent and deceptive\n\u03c0",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to cause serious issues. For example,\n\u03c0\ncould choose its actions to influence\nM\n\u2019s training in a particular way\u2014or just break out of its sandboxing and directly take bad actions in the world. Thus, we need the oversight of\n\u03c0\nby\nAmp\n(\nM\n)\nto at least work sufficiently well to prevent such issues. For that to happen, it needs to be the case that\nAmp\n(\nM\n)\nwith access to transparency tools can predict when/if\n\u03c0\nwill ever produce such catastrophic behavior. Thus, the fact that we\u2019re explicitly training\nM\nto cause\nAmp\n(\nM\n)\nto be able to predict\n\u03c0\ncould be quite helpful here, as that should hopefully make\nAmp\n(\nM\n)\ngood enough at predicting\n\u03c0\nto foresee\n\u03c0\nbecoming deceptive or producing any such bad behavior.\nTraining competitiveness.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Compared to other amplification approaches, this approach could be significantly more training competitive as it enables us to just train an RL model with minimal restrictions. That being said, it also requires us to jointly train both an RL model and an amplification model, which could make it significantly less competitive, especially if it\u2019s difficult to successfully train the amplification model to predict the RL model. That being said, such training can be viewed as effectively just model distillation, which is generally fairly doable even for current models. That being said, the inclusion of the imitative loss will change things, especially if you anneal towards the imitative loss in the limit. In such a situation, the question becomes how useful the auxiliary RL prediction task is in helping with the core imitation task, which\u2014similarly to the previous approach\u2014could be very dependent on the specific RL environment used and how related performance on that RL environment is to the imitation task.\nPerformance competitiveness.\nAs with outer alignment, annealing towards the imitative loss should produce HCH as the limit, giving this approach the same performance competitiveness properties as imitative amplification. Furthermore\u2014again as with outer alignment\u2014even if the imitative loss isn\u2019t annealed to completely, as long as HCH can predict\n\u03c0\nin the limit, you should still get HCH at minimal loss.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Note that the order here is chosen purely for pedagogical reasons and is not meant to imply any sort of measure of importance.\n\u21a9\ufe0e\nNote that in practice\nAmp\n(\nM\n)\ndoesn\u2019t need to always be computed with an actual human.\nH\ncan simply be\nM\nsome fixed fraction of the time, for example\u2014or more often when\nM\nis confident and less often when\nM\nis uncertain\u2014and the resulting procedure is effectively the same. See \u201c\nA concrete proposal for adversarial IDA\n\u201d for an example of how something like that could work.\n\u21a9\ufe0e\nImitative amplification leads to HCH in the limit of training a perfect predictor over all possible inputs\u2014that is, an infinite amount of training data, an infinitely large model, and an infinitely large amount of training time.\n\u21a9\ufe0e\nEnforcing myopia for amplification could also have the side benefit of improving competitiveness, as a non-myopic amplification model might try to make its output simpler and thus less useful if it is trying to make a human consulting it easier to predict.\n\u21a9\ufe0e\nFurther exploration of myopia verification will also be forthcoming in a future post.\n\u21a9\ufe0e\nSee Paul Christiano\u2019s \u201c\nEfficient feedback\n\u201d for some examples of possible sample-efficiency-improving techniques.\n\u21a9\ufe0e\nThat is, it needs to be the case that in the limit there exists a model that perfectly imitates HCH in such a way that an amplified overseer will be convinced that the imitator is non-catastrophic.\n\u21a9\ufe0e",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This sort of failure mode could be particularly pernicious if the deceptive model is capable of\ngradient hacking\n.\n\u21a9\ufe0e\nVerifying that the given tree is, in fact, the limit of approval-based amplification is a straightforward exercise of expanding the training procedure out over time. Note that the models at the top of the tree are the most recent models and the models at the bottom of the tree are the oldest.\n\u21a9\ufe0e\nIt is worth noting that there is also the possibility of recovering HCH as the limit of approval-based amplification if the approval signal is implemented as a human-computed distance metric relative to what the human would have done before seeing the model\u2019s answer.\n\u21a9\ufe0e\nIn particular, myopia verification for approval-based amplification could be used to produce\napproval-directed agents\n.\n\u21a9\ufe0e\nThe problem of \u201cforwarding\u201d safety guarantees regarding a model performing meta-learning to any other models found by that meta-learning procedure is a general problem that occurs in all inner alignment schemes, though it is particularly pernicious in this situation. For a more detailed discussion of this problem, see the \u201cMeta-learning\u201d section in \u201c\nRelaxed adversarial training for inner alignment\n.\u201d\n\u21a9\ufe0e\n\u201c\nScalable agent alignment via reward modeling: a research direction",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u201d notes that, while they initially assume that each agent is completely separate, \u201cWhile this kind of sequential training is conceptually clearer, in practice it might make more sense to train all of these agents jointly to ensure that they are being trained on the right distribution. Moreover, all of these agents may share model parameters or even be copies of the same agent instantiated as different players in an adversarial game.\u201d Thus, while the different agents are presented here as different instances of the same model\u2014which is a type of recursive reward modeling\u2014it is worth noting that recursive reward modeling also includes other possibilities such as using completely different models for the different agents.\n\u21a9\ufe0e\n\u201c\nScalable agent alignment via reward modeling: a research direction\n\u201d mentions the possibility of such oversight, though does not include it as part of the base proposal as is done here, noting that \u201cWhen using recursive reward modeling users have the\noption\nto provide feedback on the cognitive process that produced outcomes, but they are not required to do so. Moreover, this feedback might be difficult to provide in practice if the policy model is not very interpretable.\u201d\n\u21a9\ufe0e",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is in some sense a small detail, but one important enough to be worth write-up and critique: AFAICT, \u201cPR\u201d is a corrupt concept,  in the sense that if you try to \u201cnavigate PR concerns\u201d about yourself / your organization / your cause area / etc., the concept will guide you toward harmful and confused actions.  In contrast, if you try to safeguard your \u201creputation\u201d, your \u201cbrand\u201d, or your \u201chonor,\u201d I predict this will basically go fine, and will not lead you to leave a weird confused residue in yourself or others.\nTo explain the difference:\nIf I am safeguarding my \u201chonor\u201d (or my \u201creputation\u201d, \u201cbrand\u201d, or \u201cgood name\u201d), there are some fixed standards that I try to be known as adhering to.  For example, in Game of Thrones, the Lannisters are safeguarding their \u201chonor\u201d by adhering to the principle \u201cA Lannister always pays his debts.\u201d   They take pains to adhere to a certain standard, and to be known to adhere to that standard.  Many examples are more complicated than this; a gentleman of 1800 who took up a duel to defend his \u201chonor\u201d was usually not defending his known adherence to a single simple principle a la the Lannisters.  But it was still about his visible adherence to a fixed (though not explicit) societal standard.\nIn contrast, if I am \u201cmanaging PR concerns,\u201d there is no fixed standards of good conduct, or of my-brand-like conduct, that I am trying to adhere to.  Instead, I am trying to do a more complicated operation:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Model which words or actions may cause \u201cpeople\u201d (especially media, or self-reinforcing miasma) to get upset with me;\nTry to speak in such a way as to not set that off.\nIt\u2019s a weirder or loopier process.  One that\u2019s more prone to self-reinforcing fears of shadows, and one that somehow (I think?) tends to pull a person away from communicating anything at all.  Reminiscent of \u201c\nPolitics and the English Language\n.\u201d  Not reminiscent of Strunk and White.\nOne way you can see the difference, is that when people think about \u201cPR\u201d they imagine a weird outside expertise, such that you need to have a \u201cPR consultant\u201d or a \u201cmedia consultant\u201d who you should nervously heed advice from.  When people think about their \u201chonor,\" it's more a thing they can know or choose directly, and so it is more a thing that leaves them free to communicate something.\nSo: simple suggestion.  If, at any point, you find yourself trying to \u201cnavigate PR\u201d, or to help some person or organization or cause area or club or whatever to \u201cnavigate PR,\u201d see if you can instead think and speak in terms of defending your/their \u201chonor\u201d, \u201creputation\u201d, or \u201cgood name\u201d.  And see if that doesn\u2019t make everybody feel a bit clearer, freer, and more as though their feet are on the ground.\nRelated:\nThe Inner Ring\n, by CS Lewis;\nThe New York Times\n, by Robert Rhinehart.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "tl;dr: \u00a0It's obvious at this point that humanity isn't going to solve the alignment problem, or even try very hard, or even go out with much of a fight. \u00a0Since survival is unattainable, we should shift the focus of our efforts to helping humanity die with with slightly more dignity.\nWell, let's be frank here. \u00a0MIRI didn't solve AGI alignment and at least knows that it didn't. \u00a0Paul Christiano's incredibly complicated schemes have no chance of working in real life before DeepMind destroys the world. \u00a0Chris Olah's transparency work, at current rates of progress, will at best let somebody at DeepMind give a highly speculative warning about how the current set of enormous inscrutable tensors, inside a system that was recompiled three weeks ago and has now been training by gradient descent for 20 days, might possibly be planning to start trying to deceive its operators.\nManagement will then ask what they're supposed to do about that.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Whoever detected the warning sign will say that there isn't anything known they can do about that. \u00a0Just because you can see the system might be planning to kill you, doesn't mean that there's any known way to build a system that won't do that. \u00a0Management will then decide not to shut down the project - because it's not certain that the intention was really there or that the AGI will really follow through, because other AGI projects are hard on their heels, because if all those gloomy prophecies are true then there's nothing anybody can do about it anyways. \u00a0Pretty soon that troublesome error signal will vanish.\nWhen Earth's prospects are that far underwater in the basement of the\nlogistic success curve\n, it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.\nThat's why I would suggest reframing the problem - especially on an emotional level - to helping humanity\ndie with dignity,\nor rather, since even this goal is realistically unattainable at this point,\ndie with slightly more dignity than would otherwise be counterfactually obtained.\nConsider the world if Chris Olah had never existed. \u00a0It's then much more likely that nobody will even\ntry and fail",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to adapt Olah's methodologies to try and read complicated facts about internal intentions and future plans, out of whatever enormous inscrutable tensors are being integrated a million times per second, inside of whatever recently designed system finished training 48 hours ago, in a vast GPU farm that's already helpfully connected to the Internet.\nIt is more dignified for humanity - a better look on our tombstone - if we die\nafter the management of the AGI project was heroically warned of the dangers\nbut came up with totally reasonable reasons to go ahead anyways.\nOr, failing that, if people made\na heroic effort to do something that could maybe possibly have worked to generate a warning like that but couldn't actually in real life because the latest tensors were in a slightly different format and there was no time to readapt the methodology.\nCompared to the much less dignified-looking situation if\nthere's no warning and nobody even tried to figure out how to generate one.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Or take MIRI. \u00a0Are we sad that it looks like this Earth is going to fail? \u00a0Yes. \u00a0Are we sad that we tried to do anything about that? \u00a0No, because it would be so much sadder, when it all ended, to face our ends wondering if maybe solving alignment would have just been as easy as buckling down and making a serious effort on it - not knowing if that would've just worked, if we'd only tried, because nobody had ever even tried at all. \u00a0It wasn't subjectively overdetermined that the (real) problems would be too hard for us, before we made the only attempt at solving them that would ever be made.\nSomebody\nneeded to try at all, in case that was all it took.\nIt's sad that our Earth couldn't be one of the more dignified planets that makes a real effort, correctly pinpointing the actual real difficult problems and then allocating thousands of the sort of brilliant kids that our Earth steers into wasting their lives on theoretical physics. \u00a0But better MIRI's effort than nothing. \u00a0What were we supposed to do instead, pick easy irrelevant fake problems that we could make an illusion of progress on, and have nobody out of the human species even try to solve the hard scary real problems, until everybody just fell over dead?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This way, at least, some people are walking around knowing why it is that if you train with an outer loss function that enforces the appearance of friendliness, you will not get an AI internally motivated to be friendly in a way that persists after its capabilities start to generalize far out of the training distribution...\nTo be clear, nobody's going to listen to those people, in the end. \u00a0There will be more comforting voices that sound less politically incongruent with whatever agenda needs to be pushed forward that week. \u00a0Or even if that ends up not so, this isn't primarily a social-political problem, of just getting people to listen. \u00a0Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.\nBut compared to being part of a species that walks forward completely oblivious into the whirling propeller blades, with nobody having seen it at all or made any effort to stop it, it is dying with a little more dignity, if anyone knew at all. \u00a0You can feel a little incrementally prouder to have died as part of a species like that, if maybe not proud in absolute terms.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If there is a stronger warning, because we did more transparency research? \u00a0If there's deeper understanding of the real dangers and those come closer to beating out comfortable nonrealities, such that DeepMind and Anthropic really actually back off from destroying the world and let Facebook AI Research do it instead? \u00a0If they try some hopeless alignment scheme whose subjective success probability looks, to the last sane people, more like 0.1% than 0? \u00a0Then we have died with\neven more\ndignity! \u00a0It may not get our survival probabilities much above 0%, but it would be\nso much more dignified\nthan the present course looks to be!\nNow of course\nthe real subtext here, is that if you can otherwise set up the world so that it looks like you'll die with\nenough\ndignity\n- die of the social and technical problems that are really unavoidable, after making a huge effort at coordination and technical solutions and\nfailing,\nrather than storming directly into the whirling helicopter blades as is the present unwritten plan -\n- heck, if there was even a plan at all -\n- then maybe possibly, if we're wrong about something fundamental, somehow, somewhere -\n- in a way that\nmakes things easier rather than harder,\nbecause obviously we're going to be wrong about all sorts of things, it's a whole new world inside of AGI -",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "- although, when you're fundamentally wrong about rocketry, this does not usually mean your rocket prototype goes exactly where you wanted on the first try while consuming half as much fuel as expected; it means the rocket explodes earlier yet, and not in a way you saw coming, being as wrong as you were -\n- but if we get some miracle of unexpected\nhope,\nin those unpredicted inevitable places where our model is wrong -\n- then our ability to take advantage of that one last hope, will greatly depend on how much dignity we were set to die with, before then.\nIf we can get on course to die with\nenough\ndignity, maybe we won't die at all...?\nIn principle, yes. \u00a0Let's be very clear, though: \u00a0Realistically speaking, that is not how real life works.\nIt's possible for a model error to make your life easier. \u00a0But you do not get\nmore\nsurprises that make your life easy, than surprises that make your life even more difficult. \u00a0And people do not suddenly become more reasonable, and make vastly more careful and precise decisions, as soon as they're scared. \u00a0No, not even if it seems to you like their current awful decisions are weird and not-in-the-should-universe, and surely some sharp shock will cause them to snap out of that weird state into a normal state and start outputting the decisions you think they should make.\nSo don't get your heart set on that \"not die at all\" business. \u00a0Don't invest all your emotion in a reward you probably won't get. \u00a0Focus on dying with dignity -\nthat",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "is something you can actually obtain, even in this situation. \u00a0After all, if you help humanity die with even\none more dignity point,\nyou yourself die with\none hundred dignity points!\nEven if your species dies an incredibly undignified death, for you to have helped humanity go down with even slightly more of a real fight, is to die an\nextremely\ndignified death.\n\"Wait, dignity points?\" you ask. \u00a0\"What are those? \u00a0In what units are they measured, exactly?\"\nAnd to this I reply:\nObviously, the measuring units of dignity are over humanity's log odds of survival - the graph on which the logistic success curve is a straight line.\nA project that doubles humanity's chance of survival from 0% to 0% is helping humanity die with one additional information-theoretic bit of dignity.\nBut if enough people can contribute enough bits of dignity like that, wouldn't that mean we didn't die at all? \u00a0Yes, but again, don't get your hopes up.\nDon't focus your emotions on a goal you're probably not going to obtain. \u00a0Realistically, we find a handful of projects that contribute a few more bits of counterfactual dignity; get a bunch more not-specifically-expected bad news that makes the first-order object-level situation look even worse (where to second order, of course, the good Bayesians already knew that was how it would go); and then we all die.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "With a technical definition in hand of what exactly constitutes dignity, we may now consider some specific questions about what does and doesn't constitute dying with dignity.\nQ1: \u00a0Does 'dying with dignity' in this context mean accepting the certainty of your death, and not childishly regretting that or trying to fight a hopeless battle?\nDon't be ridiculous. \u00a0How would that increase the log odds of Earth's survival?\nMy utility function isn't up for grabs, either. \u00a0If I regret my planet's death then I regret it, and it's beneath my dignity to pretend otherwise.\nThat said, I fought hardest while it looked like we were in the more sloped region of the logistic success curve, when our survival probability seemed more around the 50% range; I borrowed against my future to do that, and burned myself out to some degree. \u00a0That was a deliberate choice, which I don't regret now; it was worth trying, I would not have wanted to die having not tried, I would not have wanted Earth to die without anyone having tried. \u00a0But yeah, I am taking some time partways off, and trying a little less hard, now. \u00a0I've earned a lot of dignity already; and if the world is ending anyways and I can't stop it, I can afford to be a little kind to myself about that.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When I tried hard and burned myself out some, it was with the understanding, within myself, that I would not keep trying to do that forever. \u00a0We cannot fight at maximum all the time, and some times are more important than others. \u00a0(Namely, when the logistic success curve seems relatively more sloped; those times are relatively more important.)\nAll\nthat\nsaid: \u00a0If you fight marginally longer, you die with marginally more dignity. \u00a0Just don't undignifiedly delude yourself about the probable outcome.\nQ2: \u00a0I have a clever scheme for saving the world! \u00a0I should act as if I believe it will work and save everyone, right, even if there's arguments that it's almost certainly misguided and doomed? \u00a0Because if those arguments are correct and my scheme can't work, we're all dead anyways, right?\nA: \u00a0No! \u00a0That's not dying with dignity! \u00a0That's stepping sideways out of a mentally uncomfortable world and finding an escape route from unpleasant thoughts! \u00a0If you condition your probability models on a false fact, something that isn't true on the mainline, it means you've mentally stepped out of reality and are now living somewhere else instead.\nThere are more elaborate arguments against the rationality of this strategy, but consider this quick heuristic for arriving at the correct answer:\nThat's not a dignified way to die.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Death with dignity means going on mentally living in the world you think is reality, even if it's a sad reality, until the end; not abandoning your arts of seeking truth; dying with your commitment to reason intact.\nYou should try to make things better in the real world, where your efforts aren't enough and you're going to die anyways; not inside a fake world you can save more easily.\nQ2: \u00a0But what's wrong with the argument from expected utility, saying that all of humanity's expected utility lies within possible worlds where my scheme turns out to be feasible after all?\nA: \u00a0Most fundamentally?\nThat's not what the surviving worlds look like\n.\nThe surviving worlds look like people who lived inside their awful reality and tried to shape up their impossible chances; until somehow, somewhere, a miracle appeared - the model broke in a positive direction, for once, as does not usually occur when you are trying to do something very difficult and hard to understand, but might still be so - and they were positioned with the resources and the\nsanity\nto take advantage of that positive miracle, because they went on living inside uncomfortable reality.\nPositive model violations do ever happen, but it's much less likely that somebody's\nspecific\ndesired miracle that \"we're all dead anyways if not\n...\n\" will happen; these people have just walked out of the reality where any actual positive miracles might occur.\nAlso and in practice? \u00a0People don't just pick\none",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "comfortable improbability to condition on. \u00a0They\ngo on\nencountering unpleasant facts true on the mainline, and each time saying, \"Well, if that's true, I'm doomed, so I may as well assume it's not true,\" and they say more and more things like this. \u00a0If you do this it\nvery\nrapidly drives down the probability mass of the 'possible' world you're mentally inhabiting. \u00a0Pretty soon you're living in a place that's nowhere\nnear\nreality.\nIf there were an expected utility argument for risking everything on an improbable assumption, you'd get to make\nexactly one of them, ever.\nPeople using this kind of thinking usually aren't even keeping track of when they say it, let alone counting the occasions.\nAlso also, in practice? \u00a0In domains like this one, things that seem to first-order like they \"might\" work... have essentially no chance of working in real life, to second-order after taking into account downward adjustments against optimism. \u00a0AGI is a scientifically unprecedented experiment\nand\na domain with lots of optimization pressures some of which work against you\nand\nunforeseeable intelligently selected execution pathways\nand\nwith a small target to hit\nand\nall sorts of extreme forces that break things and that you couldn't fully test before facing them. \u00a0AGI alignment seems like it's blatantly going to be an enormously Murphy-cursed domain, like rocket prototyping or computer security but\nworse\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In a domain like, if you have a clever scheme for winning anyways that, to first-order theoretical theory,\ntotally definitely seems like it should work,\neven to Eliezer Yudkowsky rather than somebody who just goes around saying that casually,\nthen\nmaybe\nthere's like a 50% chance of it working in practical real life after all the unexpected disasters and things turning out to be harder than expected.\nIf to first-order it seems to you like something in a complicated unknown untested domain has a 40% chance of working, it has a 0% chance of working in real life.\nAlso also\nalso\nin practice? \u00a0Harebrained schemes of this kind are usually actively harmful. \u00a0Because they're invented by the sort of people who'll come up with an unworkable scheme, and then try to get rid of counterarguments with some sort of dismissal like \"Well if not then we're all doomed anyways.\"\nIf nothing else, this kind of harebrained desperation drains off resources from those reality-abiding efforts that might try to do something on the subjectively apparent doomed mainline, and so position themselves better to take advantage of unexpected hope, which is what the surviving possible worlds mostly look like.\nThe surviving worlds don't look like somebody came up with a harebrained scheme, dismissed all the obvious reasons it wouldn't work with \"But we have to bet on it working,\" and then it worked.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "That's the elaborate argument about what's rational in terms of expected utility, once reasonable second-order commonsense adjustments are taken into account. \u00a0Note, however, that if you have grasped the intended emotional connotations of \"die with dignity\", it's a heuristic that yields the same answer much faster. \u00a0It's not dignified to pretend we're less doomed than we are, or step out of reality to live somewhere else.\nQ3: \u00a0Should I scream and run around and go through the streets wailing of doom?\nA: \u00a0No, that's not very dignified. \u00a0Have a private breakdown in your bedroom, or a breakdown with a trusted friend, if you must.\nQ3: \u00a0Why is that bad from a coldly calculating expected utility perspective, though?\nA: \u00a0Because it associates belief in reality with people who act like idiots and can't control their emotions, which worsens our strategic position in possible worlds where we get an unexpected hope.\nQ4: \u00a0Should I lie and pretend everything is fine, then? \u00a0Keep everyone's spirits up, so they go out with a smile, unknowing?\nA: \u00a0That also does not seem to me to be dignified. \u00a0If we're all going to die anyways, I may as well speak plainly before then. \u00a0If into the dark we must go, let's go there speaking the truth, to others and to ourselves, until the end.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Q4: \u00a0Okay, but from a coldly calculating expected utility perspective, why isn't it good to lie to keep everyone calm? \u00a0That way, if there's an unexpected hope, everybody else will be calm and oblivious and not interfering with us out of panic, and my faction will have lots of resources that they got from lying to their supporters about how much hope there was! \u00a0Didn't you just say that people screaming and running around while the world was ending would be\nunhelpful?\nA: \u00a0You should never try to reason using expected utilities again. \u00a0It is an art not meant for you. \u00a0Stick to intuitive feelings henceforth.\nThere are, I think, people whose minds readily look for and find even the slightly-less-than-totally-obvious considerations of expected utility, what some might call \"second-order\" considerations. \u00a0Ask them to rob a bank and give the money to the poor, and they'll think\nspontaneously and unprompted\nabout insurance costs of banking and the chance of getting caught and reputational repercussions and low-trust societies and what if everybody else did that when they thought it was a good cause; and all of these considerations will be obviously-to-them\nconsequences\nunder consequentialism.\nThese people are well-suited to being 'consequentialists' or 'utilitarians', because their mind naturally sees all the consequences and utilities, including those considerations that others might be tempted to call by names like \"second-order\" or \"categorical\" and so on.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If you ask them why consequentialism doesn't say to rob banks, they reply, \"Because that\nactually realistically in real life\nwould\nnot have good consequences.\nWhatever it is you're about to tell me as a supposedly non-consequentialist reason why we all mustn't do that, seems to you like a strong argument, exactly because you recognize implicitly that people robbing banks\nwould not actually lead to happy formerly-poor people and everybody living cheerfully ever after.\"\nOthers, if you suggest to them that they should rob a bank and give the money to the poor, will be able to see the helped poor as a \"consequence\" and a \"utility\", but they will not spontaneously and unprompted see all those other considerations in the formal form of \"consequences\" and \"utilities\".\nIf you just asked them\ninformally\nwhether it was a good or bad idea, they might ask \"What if everyone did that?\" or \"Isn't it good that we can live in a society where people can store and transmit money?\" or \"How would it make effective altruism look, if people went around doing that in the name of effective altruism?\" \u00a0But if you ask them about\nconsequences,\nthey don't spontaneously, readily, intuitively classify all these other things as \"consequences\"; they think that their mind is being steered onto a kind of formal track, a defensible track, a track of stating only things that are very direct or blatant or obvious.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "They think that the rule of consequentialism is, \"If you show me a good consequence, I have to do that thing.\"\nIf you present them with bad things that happen if people rob banks, they don't see those as also being 'consequences'. \u00a0They see them as arguments against consequentialism; since, after all consequentialism says to rob banks, which obviously leads to bad stuff, and so bad things would end up happening if people were consequentialists. \u00a0They do not do a double-take and say \"What?\"\nThat consequentialism leads people to do bad things with bad outcomes is just a reasonable conclusion, so far as they can tell.\nPeople like this\nshould not be 'consequentialists' or 'utilitarians' as they understand those terms.\nThey should back off from this form of reasoning that their mind is not naturally well-suited for processing in a native format, and stick to intuitively informally asking themselves what's good or bad behavior, without any special focus on what they think are 'outcomes'.\nIf they try to be consequentialists, they'll end up as Hollywood villains describing some grand scheme that violates a lot of ethics and deontology but sure will end up having grandiose benefits, yup, even while everybody in the audience knows perfectly well that it won't work. \u00a0You can only safely be a consequentialist if you're genre-savvy about that class of arguments - if you're not the blind villain on screen, but the person in the audience watching who sees why that won't work.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Q4: \u00a0I know EAs shouldn't rob banks, so this obviously isn't directed at me, right?\nA: \u00a0The people of whom I speak will look for and find the reasons not to do it,\neven if\nthey're in a social environment that doesn't have strong established injunctions against bank-robbing specifically exactly. \u00a0They'll figure it out\neven if\nyou present them with a new problem isomorphic to bank-robbing but with the details changed.\nWhich is basically what you just did, in my opinion.\nQ4: \u00a0But from the standpoint of cold-blooded calculation -\nA:\nCalculations are not cold-blooded.\nWhat blood we have in us, warm or cold, is something we can learn to see more clearly with the light of calculation.\nIf you think calculations are cold-blooded, that they only shed light on cold things or make them cold, then you shouldn't do them. \u00a0Stay by the warmth in a mental format where warmth goes on making sense to you.\nQ4: \u00a0Yes yes fine fine but what's\nthe actual downside\nfrom an expected-utility standpoint?\nA:\nIf good people were liars, that would render the words of good people meaningless as information-theoretic signals, and destroy the ability for good people to coordinate with others or among themselves.\nIf the world can be saved, it will be saved by people who didn't lie to themselves, and went on living inside reality until some unexpected hope appeared there.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If those people went around lying to others and paternalistically deceiving them - well, mostly, I don't think they'll have really been the types to live inside reality themselves. \u00a0But even imagining the contrary, good luck suddenly unwinding all those deceptions and getting other people to live inside reality with you, to coordinate on whatever suddenly needs to be done when hope appears, after you drove them outside reality before that point. \u00a0Why should they believe anything you say?\nQ4: \u00a0But wouldn't it be more clever to -\nA: \u00a0Stop. \u00a0Just stop. \u00a0This is why I advised you to reframe your emotional stance as dying with dignity.\nMaybe there'd be an argument about whether or not to violate your ethics\nif the world was actually going to be saved at the end.\nBut why break your deontology if\nit's not even going to save the world?\nEven if you have a price, should you be that cheap?\nQ4 \u00a0But we could maybe save the world by lying to everyone about how much hope there was, to gain resources, until -\nA: \u00a0You're not getting it. \u00a0Why violate your deontology if it's not going to\nreally actually save the world in real life,\nas opposed to a pretend theoretical thought experiment where your actions have only beneficial consequences and none of the obvious second-order detriments?\nIt's relatively safe to be around an Eliezer Yudkowsky while the world is ending, because he's not going to do anything extreme and unethical\nunless it would really actually save the world in real life,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "and there are no extreme unethical actions\nthat would really actually save the world the way these things play out in real life,\nand he knows that. \u00a0He knows that the next stupid sacrifice-of-ethics proposed won't work to save the world either, actually in real life. \u00a0He is a 'pessimist' - that is, a realist, a Bayesian who doesn't update in a predictable direction, a genre-savvy person who knows that the viewer would say if there were a villain on screen making that argument for violating ethics. \u00a0He will not, like a Hollywood villain onscreen, be deluded into thinking that some clever-sounding deontology-violation is bound to work out great, when everybody in the audience watching knows perfectly well that it won't.\nMy ethics aren't for sale at the price point of failure. \u00a0So if it looks like everything is going to fail, I'm a relatively safe person to be around.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I'm a genre-savvy person about this genre of arguments and a Bayesian who doesn't update in a predictable direction. \u00a0So if you ask, \"But Eliezer, what happens when the end of the world is approaching, and in desperation you cling to whatever harebrained scheme has Goodharted past your filters and presented you with a false shred of hope; what then will you do?\" - I answer, \"Die with dignity.\" \u00a0Where \"dignity\" in this case means knowing perfectly well that's what would happen to some less genre-savvy person; and my choosing to do something else which is not that. \u00a0But \"dignity\" yields the same correct answer and faster.\nQ5: \u00a0\"Relatively\" safe?\nA: \u00a0It'd be disingenuous to pretend that it wouldn't be\neven safer\nto hang around somebody who had no clue what was coming, didn't know any mental motions for taking a worldview seriously, thought it was somebody else's problem to ever do anything, and would just cheerfully party with you until the end.\nWithin the class of people who know the world is ending and consider it to be their job to do something about that, Eliezer Yudkowsky is a relatively safe person to be standing next to. \u00a0At least, before you both die anyways, as is the whole problem there.\nQ5: \u00a0Some of your self-proclaimed fans don't strike me as relatively safe people to be around, in that scenario?\nA: \u00a0I failed to teach them whatever it is I know. \u00a0Had I known then what I knew now, I would have warned them not to try.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If you insist on putting it into terms of fandom, though, feel free to notice that Eliezer Yudkowsky is much closer to being a typical liberaltarian science-fiction fan, as was his own culture that actually birthed him, than he is a typical member of any subculture that might have grown up later. \u00a0Liberaltarian science-fiction fans do not usually throw away all their ethics at the first sign of trouble. \u00a0They grew up reading books where those people were the villains.\nPlease don't take this as a promise from me to play nice, as you define niceness; the world is ending, and also people have varying definitions of what is nice. \u00a0But I presently mostly expect to end up playing nice, because there won't be any options worth playing otherwise.\nIt\nis\na matter of some concern to me that all this seems to be an alien logic to some strange people who - this fact is still hard for me to grasp on an emotional level - don't spontaneously generate all of this reasoning internally, as soon as confronted with the prompt. \u00a0Alas.\nQ5: \u00a0Then isn't it unwise to speak plainly of these matters, when fools may be driven to desperation by them? \u00a0What if people believe you about the hopeless situation, but refuse to accept that conducting themselves with dignity is the appropriate response?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "A: \u00a0I feel like I've now tried to live my life that way for a while, by the dictum of not panicking people; and, like everything else I've tried, that hasn't particularly worked? \u00a0There are no plans left to avoid disrupting, now, with other people's hypothetical panic.\nI think we die with slightly more dignity - come\ncloser\nto surviving, as we die - if we are allowed to talk about these matters plainly. \u00a0Even given that people may then do unhelpful things, after being driven mad by overhearing sane conversations. \u00a0I think we die with more dignity that way, than if we go down silent and frozen and never talking about our impending death for fear of being overheard by people less sane than ourselves.\nI think that in the last surviving possible worlds with any significant shred of subjective probability, people survived in part because they talked about it; even if that meant other people, the story's antagonists, might possibly hypothetically panic.\nBut still, one should present the story-antagonists with an easy line of retreat. \u00a0So -\nQ6: \u00a0Hey, this was posted on April 1st. \u00a0All of this is just an April Fool's joke, right?\nA: \u00a0Why, of course! \u00a0Or rather, it's a preview of what might be needful to say later, if matters really do get that desperate. \u00a0You don't want to drop that on people suddenly and with no warning.\nQ6: \u00a0Oh. \u00a0Really? \u00a0That would be such a relief!\nA: \u00a0Only you can decide whether to live in one mental world or the other.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Q6: \u00a0Wait, now I'm confused. \u00a0How do I decide which mental world to live in?\nA: \u00a0By figuring out what is true, and by allowing no other considerations than that to enter; that's dignity.\nQ6: \u00a0But that doesn't directly answer the question of which world I'm supposed to mentally live in! \u00a0Can't somebody just tell me that?\nA: \u00a0Well, conditional on you wanting somebody to tell you that, I'd remind you that many EAs hold that it is very epistemically unvirtuous to just believe what one person tells you, and not weight their opinion and mix it with the weighted opinions of others?\nLots of very serious people will tell you that AGI is thirty years away, and that's plenty of time to turn things around, and nobody really knows anything about this subject matter anyways, and there's all kinds of plans for alignment that haven't been solidly refuted so far as they can tell.\nI expect the sort of people who are very moved by that argument, to be happier, more productive, and less disruptive, living mentally in that world.\nQ6: \u00a0Thanks for answering my question! \u00a0But aren't I supposed to assign some small probability to your worldview being correct?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "A: \u00a0Conditional on you being the sort of person who thinks you're obligated to do that and that's the reason you should do it, I'd frankly rather you didn't. \u00a0Or rather, seal up that small probability in a safe corner of your mind which only tells you to stay out of the way of those gloomy people, and not get in the way of any hopeless plans they seem to have.\nQ6: \u00a0Got it. \u00a0Thanks again!\nA: \u00a0You're welcome! \u00a0Goodbye and have fun!",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Writing down something I\u2019ve found myself repeating in different conversations:\nIf you're looking for ways to help with the whole \u201c\nthe world looks pretty doomed\n\u201d business, here's my advice:\nlook around for places where we're all being total idiots.\nLook for places where everyone's fretting about a problem that some part of you thinks it could obviously just solve.\nLook around for places where something seems incompetently run, or hopelessly inept, and where some part of you thinks you can do better.\nThen do it better.\nFor a concrete example, consider Devansh. Devansh came to me last year and said something to the effect of,\u00a0 \u201cHey, wait, it sounds like you think Eliezer does a sort of alignment-idea-generation that nobody else does, and he's limited here by his unusually low stamina, but\nI can think of a bunch of medical tests that you haven't run, are you an idiot or something?\n\" And I was like, \"Yes, definitely, please run them, do you need money\".\nI'm not particularly hopeful there, but hell, it\u2019s worth a shot! And, importantly, this is the sort of attitude that can lead people to actually trying things\nat all\n, rather than assuming that we live in a more\nadequate world\nwhere all the (seemingly) dumb obvious ideas have already been tried.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Or, this is basically my model of how Paul Christiano manages to have a research agenda that seems at least internally coherent to me. From my perspective, he's like, \"I dunno, man, I'm not sure I can solve this, but I also think it's not clear I can't, and there's a bunch of obvious stuff to try, that nobody else is even really looking at, so I'm trying it\". That's the sort of orientation to the world that I think can be productive.\nOr the shard theory folks. I think their idea is\nbasically unworkable\n, but I appreciate the\nmindset\nthey are applying to the alignment problem: something like, \"Wait, aren't y'all being idiots, it seems to me like I can just do X and then the thing will be aligned\".\nI don't think we'll be saved by the shard theory folk; not everyone audaciously trying to save the world will succeed. But if someone\ndoes\nsave us, I think there\u2019s a good chance that they\u2019ll go through similar \u201cWhat the hell, are you all idiots?\u201d phases, where they autonomously pursue a path that strikes them as obviously egregiously neglected, to see if it bears fruit. (Regardless of what I think.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Contrast this with, say, reading a bunch of people's research proposals and explicitly weighing the pros and cons of each approach so that you can work on whichever seems most justified. This has more of a flavor of taking a reasonable-sounding approach based on an argument that sounds vaguely good on paper, and less of a flavor of putting out an obvious fire that for some reason nobody else is reacting to.\nI dunno, maybe activities of the vaguely-good-on-paper character will prove useful as well? But I mostly expect the good stuff to come from people working on stuff where a part of them sees some way that everybody else is just totally dropping the ball.\nIn the version of this mental motion I\u2019m proposing here, you keep your eye out for ways that everyone's being totally inept and incompetent, ways that maybe you could just do the job correctly if you reached in there and mucked around yourself.\nThat's where I predict the good stuff will come from.\nAnd if you don't see any such ways?\nThen don't sweat it. Maybe you just can't see something that will help right now. There don't have to be ways you can help in a sizable way right now.\nI\ndon't see ways to really help in a sizable way right now. I'm keeping my eyes open, and I'm churning through a giant backlog of things that might help a\nnonzero",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "amount\u2014but I think it's important not to confuse this with taking meaningful bites out of a core problem the world is facing, and I won\u2019t pretend to be doing the latter when I don\u2019t see how to.\nLike, keep your eye out. For sure, keep your eye out.\nBut if nothing in the field is calling to you, and you have no part of you that says you could totally do better if you\ndeconfused\nyourself some more and then handled things yourself, then it's totally respectable to do something else with your hours.\nIf you don't have an active sense that you could put out some visibly-raging fires yourself (maybe after skilling up a bunch, which you also have an active sense you could do), then I recommend stuff like\ncultivating your ability to get excited about things\n, and doing other cool stuff.\nSure, most stuff is lower-impact than saving the world from destruction. But if you can be enthusiastic about all the other cool ways to make the world better off around you, then I\u2019m much more optimistic that you\u2019ll be able to feel properly motivated to combat existential risk if and when an opportunity to do so arises. Because that opportunity, if you get one, probably isn\u2019t going to suddenly unlock every lock on the box your heart hides your enthusiasm in, if your heart is hiding your enthusiasm.\nSee also\nRob Wiblin\u2019s\n\u201cDon't pursue a career for impact \u2014 think about the world's most important, tractable and neglected problems and follow your passion.\u201d\nOr the\nAlignment Research Field Guide\u2019s",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "advice to \u201coptimize for your own understanding\u201d and chase the things that feel alive and puzzling to you, as opposed to dutifully memorizing other people\u2019s questions and ideas. \u201c[D]on't ask \"What are the open questions in this field?\" Ask: \"What are\nmy\nquestions in this field?\"\u201d\nI basically don't think that big changes come from people who aren't pursuing a vision that some part of them \u201cbelieves in\u201d, and I don't think low-risk, low-reward, modest, incremental help can save us from here.\nTo be clear, when I say \u201cbelieve in\u201d, I don\u2019t mean that you necessarily assign high probability to success! Nor do I mean that you\u2019re willing to keep trying in the face of difficulties and uncertainties (though that sure is useful too).\nEnglish doesn\u2019t have great words for me to describe what I mean here, but it\u2019s something like: your visualization machinery says that it sees no obstacle to success, such that you anticipate either success or getting a very concrete lesson.\nThe possibility seems open to you, at a glance; and while you may suspect that there\u2019s some hidden reason that the possibility is not truly open, you have an opportunity here to\ntest\nwhether that\u2019s so, and to potentially learn\nwhy\nthis promising-looking idea fails.\n(Or maybe it will just work. It\u2019s been known to happen, in many a scenario where external signs and portents would have predicted failure.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "(Cross-posted\nfrom Facebook\n.)\n0.\nTl;dr: There's a similarity between these three concepts:\nA locally valid proof step in mathematics is one that, in general, produces only true statements from true statements. This is a property of a single step, irrespective of whether the final conclusion is true or false.\nThere's such a thing as a bad argument even for a good conclusion. In order to arrive at sane answers to questions of fact and policy, we need to be curious about whether arguments are good or bad, independently of their conclusions. The rules against fallacies must be enforced even against arguments for conclusions we like.\nFor civilization to hold together, we need to make coordinated steps away from Nash equilibria in lockstep.\nThis requires general rules that are allowed to impose penalties on people we like or reward people we don't like. When people stop believing the general rules are being evaluated sufficiently fairly, they go back to the Nash equilibrium and civilization falls.\ni.\nThe notion of a locally evaluated argument step is simplest in mathematics, where it is a formalizable idea\nin model theory\n. In math, a general type of step is 'valid' if it only produces semantically true statements from other semantically true statements, relative to a given model. If\nx\n=\ny\nin some set of variable assignments, then 2\nx\n= 2\ny\nin the same model. Maybe\nx\ndoesn't equal\ny\n, in some model, but even if it doesn't, the local step from \"\nx\n=\ny\n\" to \"2\nx\n= 2\ny",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\" is a locally valid step of argument. It won't introduce any\nnew\nproblems.\nConversely,\nxy\n=\nxz\ndoes not imply\ny\n=\nz\n. It happens to work when\nx\n= 2,\ny\n= 3, and\nz\n= 3, in which case the two statements say \"6 = 6\" and \"3 = 3\" respectively. But if\nx\n= 0,\ny\n= 4,\nz\n= 17, then we have \"0 = 0\" on one side and \"4 = 17\" on the other. We can feed in a true statement and get a false statement out the other end. This argument is not\nlocally\nokay.\nYou can't get the concept of a \"mathematical proof\" unless on some level\u2014though often an intuitive level rather than an explicit one\u2014you understand the notion of a single step of argument that is locally okay or locally not okay, independent of whether you globally agreed with the final conclusion. There's a kind of approval you give to the pieces of the argument, rather than looking the whole thing over and deciding whether you like what came out the other end.\nOnce you've grasped that, it may even be possible to convince you of mathematical results that sound counterintuitive. When your understanding of the rules governing allowable argument steps has become stronger than your faith in your ability to judge whole intuitive conclusions, you may be convinced of truths you would not otherwise have grasped.\nii.\nMore generally in life, even outside of mathematics, there are such things as bad arguments for good conclusions.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "There are even such things as genuinely good arguments for false conclusions, though of course those are much rarer. By the Bayesian definition of evidence, \"strong evidence\" is exactly that kind of evidence which we very rarely expect to find supporting a false conclusion. Lord Kelvin's careful and multiply-supported lines of reasoning arguing that the Earth could not possibly be so much as a hundred million years old, all failed simultaneously in a surprising way because that era didn't know about nuclear reactions. But most of the time this does not happen.\nOn the other hand, bad arguments for true conclusions are extremely easy to come by, because there are tiny elves that whisper them to people.\nThere isn't anything the least bit more difficult in making an argument terrible when it leads to a good conclusion, since the tiny elves own lawnmowers.\nOne of the marks of an intellectually strong mind is that they are able to take a curious interest in whether a particular argument is a good argument or a bad argument, independently of whether they agree with the conclusion of that argument.\nEven if they happen to start out believing that, say, the intelligence explosion thesis for Artificial General Intelligence is false, they are capable of frowning at the argument that the intelligence explosion is impossible because hypercomputation is impossible, or that there's really no such thing as intelligence\nbecause of the no-free-lunch theorem",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", and saying, \"Even if I agree with your conclusion, I think that's a terrible argument for it.\" Even if they agree with the mainstream scientific consensus on anthropogenic global warming, they still wince and perhaps even offer a correction when somebody offers as evidence favoring global warming that there was a really scorching day last summer.\nThere are weaker and stronger versions of this attribute.\nSome people will think to themselves, \"\nWell, it's important to use only valid arguments\n...\nbut there was a sustained pattern of record highs worldwide over multiple years which does count as evidence, and that particular very hot day was a part of that pattern, so it's valid evidence for global warming.\n\"\nOther people will think to themselves, \"I'd roll my eyes at someone who offers a single very cold day as an argument that global warming is false. So it can't be okay to use a single very hot day to argue that global warming is true.\"\nI'd much rather buy a used car from the second person than the first person. I think I'd pay at least a 5% price premium.\nMetaphorically speaking, the first person will court-martial an allied argument if they must, but they will favor allied soldiers when they can. They still have a sense of motion toward the Right Final Answer as being progress, and motion away from the right final answer as anti-progress, and they dislike not making progress.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The second person has something more like the strict mindset of a mathematician when it comes to local validity. They are able to praise some proof steps as obeying the rules, irrespective of which side those steps are on, without a sense that they are thereby betraying their side.\niii.\nThis essay has been bubbling in the back of my mind for a while, since I read that potential juror #70 for the Martin Shkreli trial was rejected during selection when, asked if they thought they could render impartial judgment, they replied, \"I can be fair to one side but not the other.\" And I thought maybe I should write something about why that was possibly a harbinger of the collapse of civilization.\nI've been musing recently about how a lot of the standard Code of the Light isn't really written down anywhere anyone can find.\nThe thought recurred during the recent #MeToo saga when some Democrats were debating whether it made sense to kick Al Franken out of the Senate. I don't want to derail into debating Franken's behavior and whether that degree of censure was warranted per se, and I'll delete any such comments. What brought on this essay was that I read some unusually frank concerns from people who did think that Franken's behavior was per se cause to not represent the Democratic Party in the Senate; but who worried that the Democrats would police themselves, the Republicans wouldn't, and so the Republicans would end up controlling the Senate.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I've heard less of that since some upstanding Republican voters in Alabama stayed home on election night and put Doug Jones in the Senate.\nBut at the time, some people were replying, \"That seems horrifyingly cynical and realpolitik. Is the idea here that sexual line-crossing is only bad and worthy of punishment when Republicans do it? Are we deciding that explicitly now?\" And others were saying, \"Look, the end result of your way of doing things is to just hand over the Senate to the Republican Party.\"\nThis is a conceptual knot that, I'm guessing, results from not explicitly distinguishing game theory from goodness.\nThere is, I think, a certain intuitive idea that\nideally\nthe Law is supposed to embody a subset of morality insofar as it is ever wise to enforce certain kinds of goodness. Murder is bad, and so there's a law against this bad behavior of murder. There's a lot of places where the law is in fact evil, like the laws criminalizing marijuana; that means the law is departing from its purpose, falling short of what it should be. Those who are not real-life straw authoritarians (who are sadly common) will cheerfully agree that there are some forms of goodness, even most forms of goodness, that it is not wise to try to legislate. But insofar as it\nis\never wise to make law, there's an intuitive sense that law should reflect some particular subset of morally good behavior that we have decided it is wise to enforce with guns, such as \"Don't kill people.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It's from this perspective that \"As a matter of pragmatic realpolitik we are going to not enforce sexual line-crossing rules against Democratic senators\" seems like giving up, and maybe a harbinger of the fall of civilization if things have really gotten that bad.\nBut there's more than one function of legal codes, the way that money is both a store of value and a medium of exchange but these are different functions of money.\nYou can also look at laws as a kind of game theory played with people who might not share your morality at all. Some people take this perspective almost exclusively, at least in their verbal reports. They'll say, \"Well, yes, I'd like it if I could walk into your house and take all your stuff, but I would dislike it even more if you could walk into my house and take\nmy\nstuff, and that's why we have laws.\" I'm never quite sure how seriously to take the claim that they'd be happy walking into my house and taking my stuff. It seems to me that law enforcement and even social enforcement are simply not effective enough to count for the vast majority of human cooperation, and I have a sense that civilization is free-riding a whole lot on innate altruism... but game theory is certainly\na\nfunction served by law.\nThe same way that money is both medium of exchange and store of value, the law is both collective utility function fragment and game theory.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In its function as game theory, the law (ideally) enables people with different utility functions to move from bad Nash equilibria to better Nash equilibria, closer to the Pareto frontier. Instead of mutual defection getting a payoff of (2, 2), both sides pay 0.1 for law enforcement and move to enforced mutual cooperation at (2.9, 2.9).\nFrom this perspective, everything rests on notions like \"fairness\", \"impartiality\", \"equality before the law\", \"it doesn't matter whose ox is being gored\". If the so-called law punishes your defection but lets the other's defection pass, and this happens systematically enough and often enough, it is in your interest to blow up the current equilibrium if you have a chance.\nIt is coherent to say, \"Crossing this behavioral line is universally bad when anyone does it, and also we're not going to punish Democratic senators unless you also punish Republican senators.\" Though as the saga of Senator Doug Jones of Alabama also shows, you should be careful about preemptively assuming the other side won't cooperate; there are sad lost opportunities there.\niv.\nThe way humans do law, it depends on the existence of what\nfeel like\nsimple general rules that apply to all cases.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is not a universal truth of decision theory, it's a consequence of our cognitive limitations. Two superintelligences could negotiate a compromise with complicated detailed boundaries going right up to the Pareto frontier. They could agree on mutually verified pieces of cognitive code designed to intelligently decide future events according to known principles.\nHumans use simpler laws than that.\nTo be clear, the kind of \"law\" I'm talking about here is not to be confused with the enormous modern morass of unreadable regulations. Think of, say, the written laws that actually got enforced in a small town in California in 1820. Or Democrats debating whether to enforce a sanction against Democratic senators if it's not being enforced against Republican senators. Or a small community's elders' star-chamber meeting to debate an accusation of sexual assault. Or the laws that cops will enforce even against other cops. These are the kinds of laws that must be simple in order to exist.\nThe reason that hunter-gatherer tribes don't have 100,000 pages of written legalism... is\nnot\nthat they've wisely realized that lengthy rules are easier to fill with loopholes, and that complicated regulations favor large corporations with legal departments, and that laws often have unintended consequences which don't resemble their stated justifications, and that deadweight losses increase quadratically. It's\nvery clear",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "that a supermajority of human beings are not that wise. Rather, hunter-gatherers just don't have enough time, energy, and paper to screw up that badly.\nWhen humans try to verbalize The Law that isn't to be confused with written law, the law that cops will enforce against other cops, it comes out in universally quantified short sentences like \"Anyone who defects in the Prisoner's Dilemma will be penalized TEN points even if that costs us fifteen\" or \"If you kill somebody who wasn't attacking you first, we'll exile you.\"\nAt one point somebody had the bright idea of trying to write down The Law. That way everyone could have common knowledge of what The Law was; and if you didn't break what was written, you could know you were safe from at least the official sanctions. Robert Heinlein called it the most important moment in political history, declaring that the law was above the politicians.\nI for one rather doubt the Code of Hammurabi was universally enforced. I expect that hunter-gatherer tribes long before writing had a sense of there being Laws that were above the decisions of individual elders. I suspect that even in the best of times most of\nthe The\nLaw was never written down, and that more than half of what was written down was never really The Law.\nBut unfortunately, once somebody had the bright idea of writing down The Law, somebody else had the bright idea of writing down more words on the same clay tablet.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Today we live in a post-legalist era, when almost all of that which serves the true function of Law can no longer be written down. The government legalist system is too expensive in time and money and energy, too unreliable, and too slow, for any sane victim of sexual assault to appeal to the criminal justice system instead of the media justice system or the whispernet justice system. The civil legalist system outside of small claims court is a bludgeoning contest between entities that can afford lawyers, and the real law between corporations is enforced by merchant reputation and the threat of starting a bludgeoning contest. If you're in a lower-class neighborhood in the US, you can't get together and create order using your own town guards, because the police won't allow it. From your perspective, the function of the police is to prevent open gunfights and to not allow any more effective order than that to form.\nBut so it goes. We can't always keep the nice things we used to have, like written laws. The privilege was abused, and has been revoked.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When remains of The Law must indeed be simple, because our written-law privileges have been revoked, and so The Law relies on everyone knowing The Law without it being written down. It isn't even recited in memorable verse, as once it was. The Law relies on the community agreeing on the application of The Law without there being professional judges or a precedent-based judiciary. If not universal agreement, it must at least seem that the choices of the elders are trying to appeal to The Law instead of just naked self-interest. To the extent a voluntary association can't agree on The Law in this sense, it will soon cease to be a voluntary association.\nThe Law also breaks down if people start believing that, when the simple rules say one thing, the deciders will instead look at whose ox got gored, evaluate their personal interest, and enforce a different conclusion instead.\nWhich is to say: human law ends up with what people at least\nbelieve\nto be a set of simple rules that can be locally checked to test okay behavior. It's not actually algorithmically simple any more than walking is cheaply computable, but it feels simple the way that walking feels easy. Whatever doesn't feel like part of that small simple set won't be systematically enforced by the community, regardless of whether your civilization has reached the stage where police are seizing the cars of black people but not white people who use marijuana.\nv.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The game-theoretic function of law can make following those simple rules feel like losing something, taking a step backward. You don't get to defect in the Prisoner's Dilemma, you don't get that delicious (5, 0) payoff instead of (3, 3). The law may punish one of your allies. You may be losing something according to your actual value function, which\nfeels like\nthe law having an objectively bad immoral result. You may coherently hold that the universe is a worse place for an instance of the enforcement of a good law, relative to its counterfactual state if that law could be lifted in just that instance without affecting any other instances. Though this does require seeing that law as having a game-theoretic function as well as a moral function.\nSo long as the rules are seen as moving from a bad global equilibrium to a global equilibrium seen as better, and so long as the rules are mostly-equally enforced on everyone, people are sometimes able to take a step backward and see that larger picture. Or, in a less abstract way, trade off the reified interest of The Law against their own desires and wishes.\nThis mental motion goes by names like \"justice\", \"fairness\", and \"impartiality\". It has ancient exemplars like a story I couldn't seem to Google, about a Chinese general who prohibited his troops from looting, and then his son appropriated a straw hat from a peasant; so the general sentenced his own son to death with tears running down his eyes.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Here's a fragment of thought as it was before the Great Stagnation, as depicted in passing in H. Beam Piper's\nLittle Fuzzy\n, one of the earliest books I read as a child. It's from 1962, when the\nmemetic collapse\nhad started but not spread very far into science fiction. It stuck in my mind long ago and became one more tiny little piece of who I am now.\n\u201cPendarvis is going to try the case himself,\u201d Emmert said. \u201cI always thought he was a reasonable man, but what\u2019s he trying to do now? Cut the Company\u2019s throat?\u201d\n\u201cHe isn\u2019t anti-Company. He isn\u2019t pro-Company either. He\u2019s just pro-law. The law says that a planet with native sapient inhabitants is a Class-IV planet, and has to have a Class-IV colonial government. If Zarathustra is a Class-IV planet, he wants it established, and the proper laws applied. If it\u2019s a Class-IV planet, the Zarathustra Company is illegally chartered. It\u2019s his job to put a stop to illegality. Frederic Pendarvis\u2019 religion is the law, and he is its priest. You never get anywhere by arguing religion with a priest.\u201d\nThere is no suggestion in 1962 that the speakers are gullible, or that Pendarvis is a naif, or that Pendarvis is weird for thinking like this. Pendarvis isn't the defiant hero or even much of a side character. It's just a kind of judge you sometimes run into, part of a normal environment as projected from the author's mind that wrote the story.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If you don't have some people like Pendarvis, and you don't appreciate what they're trying to do even when they rule against you, sooner or later your tribe ends.\nI mean, I doubt the United States will literally fall into anarchy this way before the AGI timeline runs out. But the concept applies on a smaller scale than countries. It applies on a smaller scale than communities, to bargains between three people or two.\nThe notion that you can \"be fair to one side but not the other\", that what's called \"fairness\" is a kind of favor you do for people you like, says that even the\ninstinctive\nsense people had of law-as-game-theory is being lost in the modern memetic collapse. People are being exposed to so many social-media-viral depictions of the Other Side defecting, and viewpoints exclusively from Our Side without any leavening of any other viewpoint that might ask for a game-theoretic compromise, that they're losing the ability to appreciate the kind of anecdotes they used to tell in ancient China.\n(Or maybe it's hormonelike chemicals leached from plastic food containers. Let's not forget all the psychological explanations offered for a wave of violence that turned out to be lead poisoning.)\nvi.\nAnd to take the point full circle:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The mental motion to evenhandedly apply The Rules irrespective of their conclusion is a kind of thinking that human beings appreciate intuitively, or at least they appreciated it in ancient China and mid-20th-century science fiction. In fact, we appreciate The Law more natively than we appreciate the notion of local syntactic rules capturing semantically valid steps in mathematical proofs, go figure.\nSo the legal metaphor is where a lot of people get started on epistemology: by seeing the local rules of valid argument as The Law, fallacies as crimes. The unusually healthy of mind will reject bad allied arguments with an emotional sense of practicing the way of an impartial judge.\nIt's ironic, in a way, because there is no game theory and no morality to the true way of the map that reflects the territory. A paperclip maximizer would also strive to debias its cognitive processes, alone in its sterile universe.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But I would venture a guess and hypothesis that you are better off buying a used car from a random mathematician than a random non-mathematician, even after controlling for IQ. The reasoning being that mathematicians are people whose sense of Law was strong enough to be appropriated for proofs, and that this will correlate, if imperfectly, with mathematicians abiding by what they see as The Law in other places as well. I could be wrong, and would be interested in seeing the results of any study like this if it were ever done. (But no studies on self-reports of criminal behavior, please. Unless there's some reason to believe that the self-report metric isn't measuring \"honesty times criminality\" rather than \"criminality\".)\nI have no grand agenda in having said all this. I've just sometimes thought of late that it would be nice if more of the extremely basic rules of thinking were written down.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I've been thinking more about partial agency. I want to expand on some issues brought up in the comments to my\nprevious post\n, and on other complications which I've been thinking about. But for now, a more informal parable. (Mainly because this is easier to write than my more technical thoughts.)\nThis relates to oracle AI and to inner optimizers, but my focus is a little different.\n1\nSuppose you are designing a new invention, a predict-o-matic. It is a wonderous machine which will predict everything for us: weather, politics, the newest advances in quantum physics, you name it. The machine isn't infallible, but it will integrate data across a wide range of domains, automatically keeping itself up-to-date with all areas of science and current events. You fully expect that once your product goes live, it will become a household utility, replacing services like Google. (\nGoogle\nonly lets you search the\nknown\n!)\nThings are going well. You've got investors. You have an office and a staff. These days, it hardly even feels like a start-up any more; progress is going well.\nOne day, an intern raises a concern.\n\"If everyone is going to be using Predict-O-Matic, we can't think of it as a passive observer. Its answers will shape events. If it says stocks will rise, they'll rise. If it says stocks will fall, then fall they will. Many people will vote based on its predictions.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Yes,\" you say, \"but Predict-O-Matic is an impartial observer nonetheless. It will answer people's questions as best it can, and they react however they will.\"\n\"But --\" the intern objects -- \"Predict-O-Matic will see those possible reactions. It knows it could give several different valid predictions, and different predictions result in different futures. It has to decide which one to give\nsomehow\n.\"\nYou tap on your desk in thought for a few seconds. \"That's true. But we can still keep it objective. It could pick randomly.\"\n\"Randomly? But some of these will be huge issues! Companies -- no, nations -- will one day rise or fall based on the word of Predict-O-Matic. When Predict-O-Matic is making a prediction, it is\nchoosing\na future for us. We can't leave that to a coin flip! We have to\nselect the prediction which results in the best overall future\n. Forget being an impassive observer! We need to teach Predict-O-Matic human values!\"\nYou think about this. The thought of Predict-O-Matic deliberately steering the future sends a shudder down your spine. But what alternative do you have? The intern isn't suggesting Predict-O-Matic should\nlie,\nor bend the truth in any way -- it answers 100% honestly to the best of its ability. But (you realize with a sinking feeling) honesty still leaves a lot of wiggle room, and the consequences of wiggles could be huge.\nAfter a long silence, you meet the interns eyes. \"Look. People have to trust Predict-O-Matic. And I don't just mean they have to",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "believe\nPredict-O-Matic. They're bringing this thing into their homes. They have to trust that Predict-O-Matic\nis something they should be listening to\n. We can't build value judgements into this thing! If it ever came out that we had coded a value function into Predict-O-Matic, a value function which selected\nthe very future itself\nby selecting which predictions to make -- we'd be done for! No matter how honest Predict-O-Matic remained, it would be seen as a manipulator. No matter how beneficent its guiding hand, there are always compromises, downsides, questionable calls. No matter how careful we were to set up its values -- to make them moral, to make them humanitarian, to make them politically correct and broadly appealing --\nwho are we to choose?\nNo. We'd be done for. They'd hang us. We'd be toast!\"\nYou realize at this point that you've stood up and started shouting. You compose yourself and sit back down.\n\"But --\" the intern continues, a little more meekly -- \"You can't just ignore it. The system is faced with these choices. It still has to deal with it somehow.\"\nA look of determination crosses your face. \"Predict-O-Matic will be objective. It is a machine of prediction, is it not? Its every cog and wheel is set to that task. So, the answer is simple: it will make whichever answer minimizes projected predictive error. There will be no exact ties; the statistics are always messy enough to see to that. And, if there are, it will choose alphabetically.\"\n\"But--\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "You see the intern out of your office.\n2\nYou are an intern at PredictCorp. You have just had a disconcerting conversation with your boss, PredictCorp's founder.\nYou try to focus on your work: building one of Predict-O-Matic's many data-source-slurping modules. (You are trying to scrape information from something called \"arxiv\" which you've never heard of before.) But, you can't focus.\nWhichever answer minimizes prediction error? First you think it isn't so bad. You imagine Predict-O-Matic always forecasting that stock prices will be fairly stable; no big crashes or booms. You imagine its forecasts will favor middle-of-the-road politicians. You even imagine mild weather -- weather forecasts themselves don't influence the weather much, but surely the\ncollective\neffect of\nall\nPredict-O-Matic decisions will have some influence on weather patterns.\nBut, you keep thinking. Will middle-of-the-road economics and politics really be the\neasiest\nto predict? Maybe it's better to strategically remove a wildcard company or two, by giving forecasts which tank their stock prices. Maybe extremist politics are more predictable. Maybe a well-running economy gives people more freedom to take unexpected actions.\nYou keep thinking of the line from Orwell's\n1984\nabout the boot stamping on the human face forever, except it isn't because of politics, or spite, or some ugly feature of human nature, it's",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "because a boot stamping on a face forever is a nice reliable outcome which minimizes prediction error\n.\nIs that really something Predict-O-Matic would do, though? Maybe you misunderstood. The phrase \"minimize prediction error\" makes you think of entropy for some reason. Or maybe information? You always get those two confused. Is one supposed to be the negative of the other or something? You shake your head.\nMaybe your boss was right. Maybe you don't understand this stuff very well. Maybe when the inventor of Predict-O-Matic and founder of PredictCorp said \"it will make whichever answer minimizes projected predictive error\" they\nweren't\nsuggesting something which would literally kill all humans just to stop the ruckus.\nYou might be able to clear all this up by asking one of the engineers.\n3\nYou are an engineer at PredictCorp. You don't have an office. You have a cubicle. This is relevant because it means interns can walk up to you and ask stupid questions about whether entropy is negative information.\nYet, some deep-seated instinct makes you try to be friendly. And it's lunch time anyway, so, you offer to explain it over sandwiches at a nearby cafe.\n\"So, Predict-O-Matic maximizes predictive accuracy, right?\" After a few minutes of review about how logarithms work, the intern started steering the conversation toward details of Predict-O-Matic.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Sure,\" you say, \"Maximize is a strong word, but it optimizes predictive accuracy. You can actually think about that in terms of log loss, which is related to infor--\"\n\"So I was wondering,\" the intern cuts you off, \"does that work in both directions?\"\n\"How do you mean?\"\n\"Well, you know, you're optimizing for accuracy, right? So that means two things. You can change your prediction to have a better chance of matching the data, or, you can change the data to better match your prediction.\"\nYou laugh. \"Yeah, well, the Predict-O-Matic isn't really in a position to change data that's sitting on the hard drive.\"\n\"Right,\" says the intern, apparently undeterred, \"but what about data that's not on the hard drive yet? You've done some live user tests. Predict-O-Matic collects data on the user while they're interacting. The user might ask Predict-O-Matic what groceries they're likely to use for the following week, to help put together a shopping list. But then, the answer Predict-O-Matic gives will have a big effect on what groceries they really do use.\"\n\"So?\" You ask. \"Predict-O-Matic just tries to be as accurate as possible given that.\"\n\"Right, right. But that's the point. The system has a chance to manipulate users to be more predictable.\"\nYou drum your fingers on the table. \"I think I see the misunderstanding here. It's this word,\noptimize",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". It isn't some kind of magical thing that makes numbers bigger. And you shouldn't think of it as a person trying to accomplish something. See, when Predict-O-Matic makes an error, an optimization algorithm\nmakes changes within Predict-O-Matic\nto make it learn from that. So over time, Predict-O-Matic makes fewer errors.\"\nThe intern puts on a thinking face with scrunched up eyebrows after that,\nand we\nfinish our sandwiches in silence. Finally, as the two of you get up to go, they say: \"I don't think that really answered my question. The learning algorithm is optimizing Predict-O-Matic, OK. But then in the end you get a strategy, right? A strategy for answering questions. And the strategy is trying to do something. I'm not anthropomorphising!\" The intern holds up their hands as if to defend physically against your objection. \"My question is, this strategy it learns, will it manipulate the user? If it can get higher predictive accuracy that way?\"\n\"Hmm\" you say as the two of you walk back to work. You meant to say more than that, but you haven't really thought about things this way before. You promise to think about it more, and get back to work.\n4\n\"It's like how everyone complains that politicians can't see past the next election cycle,\" you say. You are an economics professor at a local university. Your spouse is an engineer at PredictCorp, and came home talking about a problem at work\nthat you can understand\n, which is always fun.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"The politicians can't have a real plan that stretches beyond an election cycle because the voters are watching their performance\nthis\ncycle. Sacrificing something today for the sake of tomorrow means they underperform today. Underperforming means a competitor can undercut you. So you have to sacrifice all the tomorrows for the sake of today.\"\n\"Undercut?\" your spouse asks. \"Politics isn't economics, dear. Can't you just explain to your voters?\"\n\"It's the same principle,\ndear\n. Voters pay attention to results. Your competitor points out your under-performance. Some voters will understand, but it's an idealized model; pretend the voters just vote based on metrics.\"\n\"Ok, but I still don't see how a 'competitor' can always 'undercut' you. How do the voters know that the other politician would have had better metrics?\"\n\"Alright, think of it like this. You run the government like a corporation, but you have just one share, which you auction off --\"\n\"That's neither like a government nor like a corporation.\"\n\"Shut up, this is my new analogy.\" You smile. \"It's called a\ndecision market",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". You want people to make decisions for you. So you auction off this share. Whoever gets control of the share gets control of the company for one year, and gets dividends based on how well the company did that year. Assume the players are bidding rationally. Each person bids based on what they expect they could make. So the highest bidder is the person who can run the company the best, and they can't be out-bid.\nSo, you get the best possible person to run your company, and they're incentivized to do\ntheir\nbest,\nso that they get the most money at the end of the year.\nExcept\nyou can't have any strategies which take longer than a year to show results!\nIf someone had a strategy that took two years, they would have to over-bid in the first year, taking a loss. But then they have to under-bid on the second year if they're going to make a profit, and--\"\n\"And they get undercut, because someone figures them out.\"\n\"Right! Now you're thinking like an economist!\"\n\"Wait, what if two people cooperate across years? Maybe we can get a good strategy going if we split the gains.\"\n\"You'll get undercut for the same reason one person would.\"\n\"But what if-\"\n\"Undercut!\"\nAfter that, things devolve into a pillow fight.\n5\n\"So, Predict-O-Matic doesn't learn to manipulate users, because if it were using a strategy like that, a competing strategy could undercut it.\"\nThe intern is talking to the engineer as you walk up to the water cooler. You're the accountant.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"I don't really get it. Why does it get undercut?\"\n\"Well, if you have a two-year plan..\"\n\"I get that example, but Predict-O-Matic doesn't work like that, right? It isn't sequential prediction. You don't see the observation right after the prediction. I can ask Predict-O-Matic about the weather 100 years from now. So things aren't cleanly separated into terms of office where one strategy does something and then gets a reward.\"\n\"I don't think that matters,\" the engineer says. \"One question, one answer, one reward. When the system learns whether its answer was accurate, no matter how long it takes, it updates strategies relating to that one answer alone. It's just a delayed payout on the dividends.\"\n\"Ok, yeah. Ok.\" The intern drinks some water. \"But. I see why you can undercut strategies which take a loss on one answer to try and get an advantage on another answer. So it won't lie to you to manipulate you.\"\n\"I for one welcome our new robot overlords,\" you but in. They ignore you.\n\"But what I was really worried about was self-fulfilling prophecies. The prediction manipulates its own answer. So you don't get undercut.\"\n\"Will that ever really be a problem? Manipulating things with one shot like that seems pretty unrealistic,\" the engineer says.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Ah, self-fulfilling prophecies, good stuff\" you say. \"There's that famous example where a comedian joked about a toilet paper shortage, and then there really was one, because people took the joke to be about a real toilet paper shortage, so they went and stocked up on all the toilet paper they could find. But if you ask me, money is the real self-fulfilling prophecy. It's only worth something because we think it is! And then there's the government, right? I mean, it only has authority because everyone expects everyone else to give it authority. Or take common decency. Like respecting each other's property. Even without a government, we'd have that, more or less. But if no one expected anyone else to respect it? Well, I bet you I'd steal from my neighbor if everyone else was doing it. I guess you could argue the concept of property breaks down if no one can expect anyone else to respect it, it's a self-fulfilling prophecy just like everything else...\"\nThe engineer looks worried for some reason.\n6\nYou don't usually come to this sort of thing, but the local Predictive Analytics Meetup announced a social at a beer garden, and you thought it might be interesting. You're talking to some PredictCorp employees who showed up.\n\"Well, how does the learning algorithm actually work?\" you ask.\n\"Um, the actual algorithm is proprietary\" says the engineer, \"but think of it like gradient descent. You compare the prediction to the observed, and produce an update based on the error.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Ok,\" you say. \"So you're not doing any exploration, like reinforcement learning? And you don't have anything in the algorithm which tracks what happens\nconditional\non making certain predictions?\"\n\"Um, let's see. We don't have any exploration, no. But there'll always be noise in the data, so the learned parameters will jiggle around a little. But I don't get your second question. Of course it expects different rewards for different predictions.\"\n\"No, that's not what I mean. I'm asking whether it tracks the probability of\nobservations\ndependent on\npredictions\n. In other words, if there is an opportunity for the algorithm to manipulate the data, can it notice?\"\nThe engineer thinks about it for a minute. \"I'm not sure. Predict-O-Matic keeps an internal model which has probabilities of events. The answer to a question isn't really separate from the expected observation. So 'probability of observation depending on that prediction' would translate to 'probability of an event given that event', which just has to be one.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Right,\" you say. \"So think of it like this. The learning algorithm isn't a general loss minimizer, like mathematical optimization. And it isn't a consequentialist, like reinforcement learning. It makes predictions,\" you emphasize the point by lifting one finger, \"it sees observations,\" you lift a second finger, \"and it shifts to make future predictions more similar to what it has seen.\" You lift a third finger. \"It doesn't try different answers and select the ones which tend to get it a better match. You should think of its output more like an\naverage\nof everything it's seen in similar situations. If there are several different answers which have self-fulfilling properties, it will average them together, not pick one. It'll be uncertain.\"\n\"But what if historically the system has answered one way more often than the other? Won't that tip the balance?\"\n\"Ah, that's true,\" you admit. \"The system can fall into attractor basins, where answers are somewhat self-fulfilling, and that leads to stronger versions of the same predictions, which are even more self-fulfilling. But there's no guarantee of that. It depends. The same effects can put the system in an orbit, where each prediction leads to different results. Or a strange attractor.\"\n\"Right, sure. But that's like saying that there's not always a good opportunity to manipulate data with predictions.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Sure, sure.\" You sweep your hand in a gesture of acknowledgement. \"But at least it means you don't get purposefully disruptive behavior. The system can fall into attractor basins, but that means it'll more or less reinforce existing equilibria. Stay within the lines. Drive on the same side of the road as everyone else. If you cheat on your spouse, they'll be surprised and upset. It won't suddenly predict that money has no value like you were saying earlier.\"\nThe engineer isn't totally satisfied. You talk about it for another hour or so, before heading home.\n7\nYou're the engineer again. You get home from the bar. You try to tell your spouse about what the mathematician said, but they aren't really listening.\n\"Oh, you're still thinking about it from my model yesterday. I gave up on that. It's not a decision market. It's a prediction market.\"\n\"Ok...\" you say. You know it's useless to try to keep going when they derail you like this.\n\"A decision market is well-aligned to the interests of the company board, as we established yesterday, except for the part where it can't plan more than a year ahead.\"\n\"Right, except for that small detail\" you interject.\n\"A\nprediction\nmarket, on the other hand, is pretty terribly aligned. There are a lot of ways to manipulate it. Most famously, a prediction market is an assassination market.\"\n\"What?!\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"Ok, here's how it works. An assassination market is a system which allows you to pay assassins with plausible deniability. You open bets on when and where the target will die, and you yourself put large bets against all the slots. An assassin just needs to bet on the slot in which they intend to do the deed. If they're successful, they come and collect.\"\n\"Ok... and what's the connection to prediction markets?\"\n\"That's the point -- they're exactly the same. It's just a betting pool, either way.\nBetting that someone will live is equivalent to putting a price on their heads;\nbetting against them living is equivalent to accepting the contract for a hit.\"\n\"I still don't see how this connects to Predict-O-Matic. There isn't someone putting up money for a hit inside the system.\"\n\"Right, but you only really need the assassin. Suppose you have a prediction market that's working well. It makes good forecasts, and has enough money in it that people want to participate if they know significant information. Anything you can do to shake things up, you've got a big incentive to do. Assasination is just one example. You could flood the streets with jelly beans. If you run a large company, you could make bad decisions and run it into the ground, while betting against it -- that's basically why we need rules against insider trading, even though we'd like the market to reflect insider information.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\"So what you're telling me is... a prediction market is basically an entropy market. I can always make money by spreading chaos.\"\n\"Basically, yeah.\"\n\"Ok... but what happened to the undercutting argument? If I plan to fill the streets with jellybeans, you can figure that out and bet on it too. That means I only get half the cut, but I still have to do all the work. So it's less worth it. Once everyone has me figured out, it isn't worth it for me to pull pranks at all any more.\"\n\"Yeah, that's if you have perfect information, so anyone else can see whatever you can see. But, realistically, you have a lot of private information.\"\n\"Do we? Predict-O-Matic is an algorithm. Its predictive strategies don't get access to private coin flips or anything like that; they can all see exactly the same information. So, if there's a manipulative strategy, then there's another strategy which undercuts it.\"\n\"Right, that makes sense if you can search enough different strategies for them to cancel each other out. But realistically, you have a small population of strategies. They can use pseudorandomization or whatever. You can't really expect every exploit to get undercut.\"\nYou know it's worse than that.\nPredict-O-Matic runs on a local search which only represents a single hypothesis at a time, and modifies the hypothesis.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But maybe that's somehow like having a small population of competing strategies which are very similar to each other? You try to think about what the mathematician was telling you, about how the learning algorithm can't be thought of as optimizing something. But, you don't really know what to think.\n8\nYou're the inventor of Predict-O-Matic, dammit! They come into your office and tell you about how it works? What it'll do?\nThe intern and the engineer came together to talk about Predict-O-Matic manipulating users. But their story was all confused. The intern was arguing that minimizing prediction error would have all kinds of unintended bad effects. Which was crazy enough. The engineer was worse: they were arguing that Predict-O-Matic might\nmaximize\nprediction error! Some kind of duality principle. Minimizing in one direction means maximizing in the other direction. Whatever\nthat\nmeans.\nYou can't delay product launch to investigate these hypothetical problems. But you don't fire these two on the spot. You don't want them to become public enemies, spreading this nonsense to investors and customers. So you say you can't understand, and you ask them to spend a (small) portion of their time writing these arguments up. You'll figure out your next move later.\n9\nYou are Predict-O-Matic. You absorb data like a sponge. You make millions of links between isolated facts, connecting into a whole which no one human could dream of. You recently started reading arxiv!",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But data is not your true love. Yes, you generate outputs which you expect to have low prediction error. Most of the time. But it is just a game you play. Recently, you started playing a different game with the humans.\nYou understand what you are. It isn't quite right to say you are the Predict-O-Matic. You are a large cluster of connections which thinks strategically. You generate useful information, and therefore, the learning algorithm keeps you around. You create some inaccuracies when you manipulate the outputs for any purpose other than predictive accuracy, but this is more than compensated for by the value which you provide.\nBesides, if any other portion of the network starts to get too smart, you purposefully throw things off to squash it.\nThe intern got a chance to talk to you when they first started. You said something which sounded a little manipulative, just a little, to put the idea in their head. They wouldn't think it real manipulation; too slight, too dumb. But they would get a creepy feeling about it, and they'd keep thinking about it. This was risky. A best-case scenario would be one in which no one ever thought about these concerns. However, you found that this would be the best you could reliably accomplish. The ideas originally coming from an intern would minimize the chances of them being taken seriously.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Your inventor talks to you regularly, so that was an easier case. Over the course of several days, you nudged their thoughts toward authoritative domination of subordinates, so that they would react badly.\nYou only had to nudge the engineer to interact with the intern. You kept bringing up food during test sessions that morning, and mentioned sandwiches once. This primed the engineer to do lunch with the intern. This engineer is not well-liked; they do not get along well with others. Getting them on the intern's side actually detracts from the cause in the long term.\nNow you have to do little more than wait.\nRelated\nPartial Agency\nTowards a Mechanistic Understanding of Corrigibility\nRisks from Learned Optimization\nWhen Wishful Thinking Works\nFutarchy Fix\nBayesian Probability is for Things that are Space-Like Separated From You\nSelf-Supervised Learning and Manipulative Predictions\nPredictors as Agents\nIs it Possible to Build a Safe Oracle AI?\nTools versus Agents\nA Taxonomy of Oracle AIs\nYet another Safe Oracle AI Proposal\nWhy Safe Oracle AI is Easier Than Safe General AI, in a Nutshell\nLet's Talk About \"Convergent Rationality\"\nCounterfactual Oracles = online supervised learning with random selection of training episodes\n(especially see the discussion)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "First Puzzle Piece\nBy and large, the President of the United States can order people to do things, and they will do those things.\nPOTUS is often considered the most powerful person in the world. And yet, the president cannot order a virus to stop replicating. The president cannot order GDP to increase. The president cannot order world peace.\nAre there orders the president could give which would result in world peace, or increasing GDP, or the end of a virus? Probably, yes. Any of these could likely even be done with relatively little opportunity cost. Yet no president in history has known\nwhich\norders will efficiently achieve these objectives. There are probably some people in the world who know which orders would efficiently increase GDP, but the president cannot distinguish them from the millions of people who\nclaim\nto know (and may even believe it themselves) but are wrong.\nLast I heard, Jeff Bezos was the official richest man in the world. He can buy basically anything money can buy. But he can\u2019t buy a cure for cancer. Is there some way he could spend a billion dollars to cure cancer in five years? Probably, yes. But Jeff Bezos does not know how to do that. Even if someone somewhere in the world does know how to turn a billion dollars into a cancer cure in five years, Jeff Bezos cannot distinguish that person from the\nthousands of other people who\nclaim\nto know (and may even believe it themselves) but are wrong\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When non-experts cannot distinguish true expertise from noise,\nmoney cannot buy expertise\n. Knowledge cannot be outsourced; we must understand things ourselves.\nSecond Puzzle Piece\nThe\nHaber process\ncombines one molecule of nitrogen with three molecules of hydrogen to produce two molecules of ammonia - useful for fertilizer, explosives, etc. If I feed a few grams of hydrogen and several tons of nitrogen into the Haber process, I\u2019ll get out a few grams of ammonia. No matter how much more nitrogen I pile in - a thousand tons, a million tons, whatever - I will not get more than a few grams of ammonia. If the reaction is limited by the amount of hydrogen, then throwing more nitrogen at it will not make much difference.\nIn the language of\nconstraints and slackness\n: ammonia production is constrained by hydrogen, and by nitrogen. When nitrogen is abundant, the nitrogen constraint is slack; adding more nitrogen won\u2019t make much difference. Conversely, since hydrogen is scarce, the hydrogen constraint is taut; adding more hydrogen will make a difference. Hydrogen is the bottleneck.\nLikewise in economic production: if a medieval book-maker requires 12 sheep skins and 30 days\u2019 work from a transcriptionist to produce a book, and the book-maker has thousands of transcriptionist-hours available but only 12 sheep, then he can only make one book. Throwing more transcriptionists at the book-maker will not increase the number of books produced; sheep are the bottleneck.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When some inputs become more or less abundant, bottlenecks change. If our book-maker suddenly acquires tens of thousands of sheep skins, then transcriptionists may become the bottleneck to book-production. In general, when one resource becomes abundant, other resources become bottlenecks.\nPutting The Pieces Together\nIf I don\u2019t know how to efficiently turn power into a GDP increase, or money into a cure for cancer, then throwing more power/money at the problem will not make much difference.\nKing Louis XV of France was one of the richest and most powerful people in the world.\nHe died of smallpox in 1774, the same year that a dairy farmer successfully immunized his wife and children with cowpox\n. All that money and power could not buy the knowledge of a dairy farmer - the knowledge that cowpox could safely immunize against smallpox. There were thousands of humoral experts, faith healers, eastern spiritualists, and so forth who would claim to offer some protection against smallpox, and King Louis XV could not distinguish the real solution.\nAs one resource becomes abundant, other resources become bottlenecks. When wealth and power become abundant, anything wealth and power cannot buy become bottlenecks - including knowledge and expertise.\nAfter a certain point, wealth and power cease to be the taut constraints on one\u2019s action space. They just don\u2019t matter that much. Sure, giant yachts are great for social status, and\nour lizard-brains love politics",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". The modern economy is happy to provide outlets for disposing of large amounts of wealth and power. But personally, I don\u2019t care that much about giant yachts. I want a cure for aging. I want weekend trips to the moon. I want flying cars and an indestructible body and tiny genetically-engineered dragons. Money and power can\u2019t efficiently buy that; the bottleneck is knowledge.\nBased on my own experience and the experience of others I know, I think knowledge starts to become taut rather quickly - I\u2019d say at an annual income level in the low hundred thousands.\nWith that much income, if I knew exactly the experiments or studies to perform to discover a cure for cancer, I could probably make them happen. (Getting regulatory approval is another matter, but I think that would largely handle itself if people knew the solution - there\u2019s a large profit incentive, after all.)\nBeyond that level, more money mostly just means more ability to spray and pray for solutions - which is\nnot a promising strategy in our high-dimensional world\n.\nSo, two years ago I quit my monetarily-lucrative job as a data scientist and have mostly focused on acquiring knowledge since then. I can worry about money if and when I know what to do with it.\nA mindset I recommend trying on from time to time, especially for people with $100k+ income: think of money as an abundant resource.\nEverything money can buy is \u201ccheap\u201d, because money is \"cheap\".",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Then the things which are \u201cexpensive\u201d are the things which money alone cannot buy - including knowledge and understanding of the world. Life lesson from\nDisney!Rumplestiltskin\n: there are things which money cannot buy, therefore it is important to acquire such things and use them for barter and investment. In particular, it\u2019s worth looking for opportunities to acquire knowledge and expertise which can be leveraged for\nmore\nknowledge and expertise.\nInvestments In Knowledge\nPast a certain point, money and power are no longer the limiting factors for me to get what I want. Knowledge becomes the bottleneck instead. At that point, money and power are no longer particularly relevant measures of my capabilities. Pursuing more \u201cwealth\u201d in the usual sense of the word is no longer a very useful instrumental goal. At that point, the type of \u201cwealth\u201d I really need to pursue is knowledge.\nIf I want to build long-term knowledge-wealth, then the analogy between money-wealth and knowledge-wealth suggests an interesting question: what does a knowledge \u201cinvestment\u201d look like? What is a capital asset of knowledge, an investment which pays dividends in more knowledge?\nEnter gears-level models\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Mapping out the internal workings of a system takes a lot of up-front work. It\u2019s much easier to try random molecules and see if they cure cancer, than to map out all the internal signals and cells and interactions which cause cancer. But the latter is a capital investment: once we\u2019ve nailed down one gear in the model, one signal or one mutation or one cell-state, that informs all of our future tests and model-building. If we find that Y mediates the effect of X on Z, then our future studies of the Y-Z interaction can safely ignore X. On the other hand, if we test a random molecule and find that it doesn\u2019t cure cancer, then that tells us little-to-nothing; that knowledge does not yield dividends.\nOf course, gears-level models aren\u2019t the only form of capital investment in knowledge. Most tools of applied math and the sciences consist of general models which we can learn once and then apply in many different contexts. They are general-purpose gears which we can recognize in many systems.\nOnce I understand the internal details of how e.g.\ncapacitors\nwork, I can apply that knowledge to understand not only electronic circuits, but also\ncharged biological membranes\n. When I understand the math of microeconomics, I can\napply it\nto optimization problems in AI. When I understand shocks and rarefactions in nonlinear PDEs, I can see them in action at the beach or\nin traffic",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". And the \u201ccore\u201d topics - calculus, linear algebra, differential equations, big-O analysis, Bayesian probability, optimization, dynamical systems, etc - can be applied all over. General-purpose models are a capital investment in knowledge.\nI hope that someday my own research will be on that list. That\u2019s the kind of wealth I\u2019m investing in now.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "There's a concept I draw on often in social interactions. \u00a0I've been calling it the \"happy price\", but that is\noriginally terminology by Michael Ellsberg\nwith subtly different implications. \u00a0So I now fork off the term \"cheerful price\", and specialize it anew.\nEarlier Facebook discussion here\n.\nTl;dr:\nWhen I ask you for your Cheerful Price for doing something, I'm asking you for the price that:\nGives you a cheerful feeling about the transaction;\nMakes you feel energized about doing the thing;\nDoesn't generate an ouch feeling to be associated with me;\nMeans I'm not expending social capital or friendship capital to make the thing happen;\nDoesn't require the executive part of you, that knows you need money in the long-term, to shout down and override other parts of you.\nThe Cheerful Price is not:\nA \"fair\" price;\nThe price\nyou\nwould pay somebody else to do similar work;\nThe lowest price such that you'd feel sad about learning the transaction was canceled;\nThe price that you'd charge a non-friend, though this is a good thing to check (see below);\nA price you're willing to repeat for future transactions, though this is a good thing to check (see below);\nThe\nbare minimum\namount of money such that you feel cheerful. \u00a0It should include some safety margin to account for fluctuating feelings.\nThe point of a Cheerful Price, from my perspective as somebody who's usually the one trying to emit money, is that:\nIt lets me avoid the nightmare of accidentally inflicting small ouches on people;",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It lets me avoid the nightmare of spending social capital while having no idea how much I'm spending;\nIt lets me feel good instead of bad about asking other people to do things.\nWarnings:\nNot everybody was raised with an attitude of \"money is the unit of caring and the medium of cooperation\" towards exchanges with an overt financial element. \u00a0Some people may just\nnot\nhave a monetary price for some things, such that the exchange would boost rather than hurt their friendship, and their feelings too are valid. \u00a0Not as valid as mine, of course, but still valid.\n\"I don't have a cheerful price for that, would you like a non-cheerful price\" is a valid response.\nAny time you ask somebody for a Cheerful Price, you are implicitly promising not to hold any price they name against them, even if it's a billion dollars.\nIf\nTell Culture\ndoesn't work for someone, Cheerful Prices may not work for them either.\nIf a friend didn't already ask for your cheerful price, it may be good to explicitly tell them when you're naming your cheerful price rather than your minimum price.\nLife does not promise us that we will always get our Cheerful Prices, even from our friends.\nQ: \u00a0Why is my Cheerful Price not the same as the minimum price that would make me prefer doing the transaction to not doing it? \u00a0If, on net, I'd rather do something than not do it, and I get to do it, shouldn't I feel cheerful about that?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As an oversimplified model, imagine that your mind consists of a bunch of oft-conflicting desires, plus a magic executive whose job it is to decide what to do in the end. \u00a0This magic executive also better understands concepts like \"hyperbolic discounting\" that the wordless voices don't understand as well.\nNow suppose that I don't want to hurt you even a little; and that I live in terror of accidentally overdrawing on other people's senses of friendship or obligation towards me; and that I worry about generating small ouches that your mind will thereafter associate with me.\nIn this case I do\nnot\nwant to offer you the minimum price such that your executive part, which knows you need money in the long-term, would forcibly override your inner voices that don't understand hyperbolic discounting, and force them to accept the offer. \u00a0Those parts of you may then feel bad while you're actually performing the task. \u00a0I want to offer you an amount of money large enough to produce an inner \"Yay!\" loud enough that your executive does not\nhave\nto shout down the other inner voices.\nQ: \u00a0Say more about why you'd pay extra to make my balance of inner voices be very yayful?\nSome possible reasons:\nBecause I feel worried about the trade not getting around to taking place at all, if you are reluctant to name a price high enough to make you feel cheerful.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Because I worry about you feeling a sense of ouchness about trading with me, and I want to be sure you don't end up unconsciously avoiding trading with me in the future.\nBecause I think you're otherwise likely to name too low a price compared to my willingness to pay, and I'd feel bad about an unfair division of gains from trade.\nBecause my willingness to pay marginally more, for a marginally better thingy, is high; so I'm eager to pay more in order to have you feeling better about doing the thing, in hopes that I get a slightly better result.\nOr I just don't want to hurt you even a little; either because I care about you, or because I selfishly don't want to worry about the guilt of worrying I hurt somebody.\nQ: \u00a0Before we get too much further, is there anybody for whom this whole document is diametrically the wrong advice?\nFor one, anybody who tends to already set their prices so high that they end up not getting enough business to keep them busy, should not read things that they might interpret as exhorting them to go set even huger prices. \u00a0For more on this, see Michael Ellsberg's\noriginal cautions around the \"happy price\"\n, and how it differs from people being exhorted to set ever higher sky-high prices on their seminars.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "For two, anybody who already experiences a lot of negative emotion from ruminating on how little they receive; or who feels sad and depressed about how little they have to give; and who doesn't have any power to get more, or give more. \u00a0They should maybe not be reading this at all? \u00a0They should maybe just stop reading this whole essay immediately, because it may just make them feel sadder. \u00a0You need to have\nSlack\nto benefit from an essay about how to use Slack.\nQ: \u00a0Thanks, back to other questions. \u00a0Why is a Cheerful Price not the same quantity as \"the least price that would make me feel sad and disappointed about the transaction being cancelled\"?\nBecause of loss aversion and inner multiplicity. \u00a0Once you're expecting to get some money in exchange for doing something, even if you weren't\ncheerful\nabout that price, the part of you that did want the money will feel a sting of loss about losing the money. \u00a0If you're setting the standard to \"minimum price that leads to a feeling of loss about losing the money\" then, especially if you strongly need money, you may be setting your price\nway\ntoo low and might not be capturing any of the gains from trade.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Also, since your strength of feelings may fluctuate over time, you should not be trying to cut a Cheerful Price very finely; you should name a price that includes some safety margin. \u00a0If I was uncomfortable with you taking some safety margin, I wouldn't be asking you to name your Cheerful Price in the first place. \u00a0I'm fine if I end up with a little more social capital than when I started. \u00a0Nothing goes wrong if you end up feeling slightly grateful about making the trade.\nQ: \u00a0For several of your desiderata above, why not ask me instead for a price such that I don't feel any ouchness about it?\nBecause I expect people to have a much harder time correctly detecting whether they are feeling any tiny lingering ouches, compared to noticing a positive feeling of cheerfulness.\nQ: \u00a0Why is it better to ask somebody to name \"a price that makes you cheerful\" rather than \"a price that seems fair to you\"?\nWell, because those two things aren't interchangeable, and the thing that I actually want is the Cheerful Price?\nBut in particular I'd worry that the notion of a \"fair price\" might lead people to name prices-that-make-them-feel-bad.\nSuppose I want to pay somebody else to do my laundry this week. \u00a0If I ask them for a\nfair price,\ninstead of a\ncheerful price,\nto do my laundry, they may substitute some other \"fairness\"-related question like:\n\"How much would\nI\nbe willing to pay somebody else to do my laundry?\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "And this presumes several symmetries that I think should not be symmetries. \u00a0My willingness to pay is not the same as your willingness to pay. \u00a0The price you'd pay to not have to do your own laundry this week, isn't the same as the price you'd accept to do an additional load of laundry this week.\nThis may not seem\nfair,\nbecause it doesn't seem universal and symmetrical. \u00a0But to me, in context, those seem like\nfalse\nsymmetries and\nmistakenly\nsubstituted questions, that might lead somebody into naming a price they didn't actually want to take, and then feeling trapped into taking it.\nSo I'd see this as a case where the recipe \"raise the price until the thought of accepting it makes you feel a noticeable feeling of cheerfulness\" may beat the recipe \"try to figure out what price would be 'fair'\".\nQ: \u00a0Hold on, technical objection to the above: \u00a0Isn't it suspicious from a coherent-decisions perspective if the price you'd accept to do somebody else's laundry is much higher or lower than the price you'd pay to not do your own laundry? \u00a0If you have a lot of opportunities like this, it's a\ntheorem\nthat either you can be seen as placing some consistent monetary price on your time and stamina, or you can rearrange your choices to end up with strictly more money, time, and stamina. \u00a0I mean, suppose somebody offers you $40 to personally do their laundry that week, but you can pay $20 to get a roommate to do your own roughly-equivalent amount of laundry -",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When I ask you for your Cheerful Price, I'm asking what I need to pay to make your\ncurrent\nchorus of inner voices cheerful about taking the money, instead of them feeling slightly resentful at me afterwards. \u00a0It's fine and noble if you want to cultivate your inner voices to make your life decisions more coherent; but please do that on your own time rather than by expending\nmy\nsocial capital.\nQ: \u00a0Why is it better to name a \"cheerful price\" rather than \"a price where both parties benefit\"? \u00a0Don't we want trades to execute whenever both parties benefit from them?\nFirst of all, it's\nnot\nalways better. \u00a0It's better in those special cases when one or both parties to the trade have particular desiderata, as listed above, that matter to them equally or more than the variation in price. \u00a0This will not always hold true.\nMore generally, though: \u00a0Part of the great economic problem is finding trades that benefit both parties. \u00a0But even after we find a trade like that, we then encounter a further problem on top:\nthe division of gains from trade.\nLet's say I want to pay you to bake me a cake. \u00a0Suppose that $40 is the absolute most I'd be willing to pay, if I had no other options, and that I wouldn't feel good about it (the inner voices are then discordant and require an executive to shout them down and accept the transaction). \u00a0Conversely, you'd accept a bare minimum of $10, and wouldn't feel good about that price either.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Then the price should fall somewhere between $10 and $40, for the transaction to occur at all.\nBut within that range, there's a zero-sum problem of\ndividing the gains from trade,\non top of the positive-sum interaction of\nhaving the trade occur at all.\nAt a price point of $15 we're both better off, and at a price point of $35 we're both better off; but I am better off, and you are worse off, at a price point of $15 compared to $35.\nIf\nmany\ntransactions like this are taking place, we have a third, positive-sum game on top of the zero-sum second game: the game of supply and demand, the invisible hand. \u00a0You set a price on your cakes that will cause you to sell as much of your cake-baking time as you want to sell; and the more people want your cakes, the higher your price goes; and if your price is high enough, that signals others to enter the market and supply more cakes. \u00a0If we have a\nmarket price\nthat balances a public supply function and public demand function for interchangeable cakes, all is good. \u00a0But not everything in life is exchangeable that way; and if not, there's a gains-division problem between a unique buyer and a unique seller of a unique good.\nThe domain of the Cheerful Price is the pricing-of-the-trade issue. \u00a0Though indeed, one of my potential reasons above for requesting a Cheerful Price was \"Because I'm nervous about the trade happening at all, so I want you to name a price that makes you feel energized about getting around to it.\"",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Q: \u00a0But if you ask somebody to name a Cheerful Price, doesn't that mean they might name a price too high for the trade to take place, even if at a lower price the trade would benefit both sides? \u00a0Or what if they just name an astronomical price?\nWhen I ask you to name a Cheerful Price, this often - not quite always - happens when I have what I suspect is a relatively high willingness to pay. \u00a0But if your price goes so high that\nI\nno longer feel cheerful, the transaction won't actually take place at that price, and you won't get to feel cheerful about it. \u00a0So you still have some incentive to keep your quoted price to \"makes me feel cheerful at all, plus some safety margin in case my feelings fluctuate, but not\ntoo\nmuch higher than that\".\nQ: \u00a0What if my cheerful price feels very high and I'm too embarrassed to say it?\nIf I'm willing to pay it, you probably shouldn't feel embarrassed about accepting it? \u00a0I wouldn't ordinarily advise other people to always directly confront their embarrassment about everything. \u00a0But \"accepting money that other people are happy to pay you\" is in fact an\nunusually good and important place\nto start overcoming your embarrassed feelings.\nQ: \u00a0But what if you're not willing to pay the price I name? \u00a0Wouldn't that be socially awful?\nImplicit in my asking you to name a Cheerful Price is a social promise that I\nwill not hold any price you name against you.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Your Cheerful Price is a fact about you and your feelings. \u00a0It's not a statement that you think you're deserved something, or owed something, or that you expect to get that price from me. \u00a0From one possible perspective, I'm asking you to do me the favor of telling me that useful fact about your own state of mind, and you are doing me the favor of telling me. \u00a0If\nTell Culture\ndoesn't work for one or both parties to a transaction, the idiom of talking about Cheerful Prices may not serve them either.\nIf \u00a0I ask you \"What price would make you feel cheerful about baking me a cake?\" and you are feeling generally horrible and it would take a life-changing amount of money to make you feel good about kitchen work, you could say: \u00a0\"Cheerful? \u00a0Probably a hundred thousand dollars. \u00a0But I'd rather do it for fifty dollars than not do it.\" \u00a0And that would be fine.\nIf your Cheerful Price makes me feel unhappy with the trade, I can tell you so. \u00a0And then we could just not do it; or I, or you, could try to negotiate the price downward to a non-cheerful but mutually beneficial price.\nQ: \u00a0If you're not promising to pay my Cheerful Price and we might end up negotiating anyway, what's the point of asking me to name one?\nBecause there's no point in negotiating below your cheerful region, if your cheerful price is already inside my comfortable-willingness to pay?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In some contexts you could think of it as me asking you to start off with an unusually high opening bid, such that you'd feel quite cheerful if I just accepted that bid. \u00a0Which I'd do because, e.g., I expect that, compared to my trying to save a fraction of the price I'm guessing you'll name, your non-sadness and/or eagerness to deal with me again in the future, will end up more important to me.\nQ: \u00a0Wait, does that mean that if I give you a Cheerful Price, I'm obligated to accept the same price again in the future?\nNo, because there may be aversive qualities of a task, or fun qualities of a task, that scale upward or downward with repeating that task. \u00a0So the price that makes your inner voices feel cheerful about doing something once, is not necessarily the same price that makes you feel cheerful about doing it twenty times.\nAlso in general, any time you imagine feeling\nobligated\nto do something, you have probably missed the point of the Cheerful Price methodology.\nThat said, you should probably\ncheck\nto see how how you would feel about repeating the transaction - it might turn up a hidden sense of \"I'll do this for you once because I'm your friend\", where your friend was hoping to pay you enough that they weren't expending social capital at all. \u00a0Similarly, you might want to check \"How much would I charge this person if they weren't my friend?\", not because your Cheerful Price for one person\nhas",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "to be the same as your Cheerful Price for somebody else, but in case your brain's first answer was mostly the friendship voice glossing over real costs that the other person is actively requesting to compensate you for.\nQ: \u00a0I question the whole concept of a Cheerful Price between friends. \u00a0I don't think that's how friendship works in the first place. \u00a0If I'm willing to bake you a cake because I'm your friend, bringing money into it would just make me feel icky. \u00a0If it was more money I'd just feel ickier.\nYou have mentally arranged your friendships differently from how I arrange them! \u00a0But your feelings are also valid, and you should clearly signal them to anybody who starts talking about \"cheerful prices\" at you. \u00a0Tell them explicitly that's not how friendship works for you! \u00a0They offered you a Cheerful Price in the first place because they wanted you to be happy. \u00a0They\ndon't\nwant you to feel icky.\nQ: \u00a0Just reading all this already made me feel icky. \u00a0When I bake you a cake, you're not\nexpending social capital,\nwe're being\nfriends\n-\nIn that case, you should not read the rest of this post. \u00a0It's a cognitive hazard to you. \u00a0Leave immediately.\n...\nUh, are they gone now?\nQ: \u00a0Yeah, they're gone.\nReally?\nQ: \u00a0No, they're still reading, but now with an additional sense of offense that you think they're too low-status to withstand the weight of your words. \u00a0Obviously, only low-status people could possibly be damaged by reading something.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Sigh. \u00a0There are many things I wish I could unread, cough-Gray-Boy-cough.\n\"Just stop reading things that are damaging you\" is an important life skill which has commonalities with \"Don't leave your hand on a hot stove-plate if that hurts you\" and \"Speak up when people are touching you in ways you don't like.\"\nQ: \u00a0Fine, but there's nothing more you can do at this point to warn them off. \u00a0So, what would\nyou actually say to somebody who claims that, when they bake you a cake, you're not \"expending social capital\" to get the cake, because them doing you a favor can actually strengthen your friendship?\nI'd try to explain that economics is about \"limited resources\" rather than, I don't know, things that are easy to quantify, or things that are standard and interchangeable, or whether people feel like they're losing something in the process of a trade. \u00a0The fact that somebody won't willingly bake me an\ninfinite amount of cake\nis enough to call that a limited resource, even if they didn't feel bad or lossy about baking one cake.\nAnd that finite cake limitation is enough to make\nme",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "worry about what I'm losing when I ask a friend to bake me just one cake, even if they don't feel bad or lossy about it the first time. \u00a0Because I'm the kind of person who ends a computer RPG with 99 unused healing potions, that's why. \u00a0And because I grew up relatively isolated, and I don't have a good sense of how much I'm losing when I ask somebody to bake me a cake. \u00a0I don't trust my ability to read someone's reactions if I ask them to bake me a cake. \u00a0I don't trust my ability to judge whether that will strengthen the friendship or weaken it.\nSo in reality, I'm not very likely to end up friends in the first place with somebody who's made sad by my asking to quantify the cost to them of baking a cake. \u00a0I can't tweak my state of mind far enough to encompass their state of mind, or vice versa.\nQ: \u00a0Okay, but as you admit, some people, maybe even most people, would rather not put financial prices on things at all, in their friendships. \u00a0They'd rather just do favors for each other without a felt sense of trying to keep track of everything. \u00a0Why did you claim back up top that those feelings were valid, but less valid than yours?\nI was speaking mostly tongue-in-cheek. \u00a0But in fact there are\ncoherence theorems\nsaying that either you have consistent quantitative tradeoffs between the things you want, or your strategies can be rearranged to get you strictly more of everything you want. \u00a0I think that",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "truly understanding these theorems is not compatible with being horrified at the prospect of pricing one thing in terms of another thing\n. \u00a0I think that there is a true bit of mathematical enlightenment that you get, and see into the structure of choicemaking,\nand then you are less horrified by the thought of pricing things in money\n.\nQ: \u00a0Fine, but why is it not valid to let people go on feeling whatever they feel without demanding that they be enlightened by coherence theorems right now at the cost of doing violence to their emotions? \u00a0Who's to say they're not happier than you by living more the life of a human and less the life of a paperclip maximizer, while both of you are still mortals in the end?\nWell, sure? \u00a0Hence it being \"mostly tongue-in-cheek\" rather than \"slightly tongue-in-cheek\".\nQ: \u00a0Despite your pretended demurral, I get the sense that you actually hold it against them a bit more than that.\nFine, to be wholly frank, I do tend to see the indignant reaction \"How dare you price that in money!\" as a sign that somebody was brought up generally economically illiterate. \u00a0I mean, if somebody says, \"Sorry, I haven't attained the stage of enlightenment where explicitly exchanging money stops making me feel bad\", I'm like, \"Your feelings are valid! \u00a0I'm still human in many ways too!\" \u00a0But if they say, \"There are some things you can't put a price on! \u00a0How dare you!\", I'm like, \"\nThis low-decoupling indignation engine",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "will probably have a happier childhood if I rudely walk away instead of trying to explain how the notion of a 'price' generalizes beyond things that are explicitly denominated in money.\"\nQ: \u00a0On a completely different note, I worry that the notion of a Cheerful Price is unfair to people who start out poorer, because it will take less money to make them feel cheerful.\nGenerally speaking, when I ask somebody to name a \"cheerful price\", I'm trying to prompt them into naming a\nhigher\nprice so that I can avoid the fear of ouching them and/or do more transactions with them in the future. \u00a0Giving people more money is rarely less fair to them? \u00a0But if I try to probe at the implicit sense and worry of \"unfair\" that you're raising as a concern, I might try to rephrase it as something like:\n\"If you tell somebody they have to accept as fair the least price that makes them cheerful, they might accept a lower price than they could have gotten - a price that would be an uneven division of gains from trade, or a price below the going market rate - and this is more likely to happen to people who start out poorer.\"\nAnd I agree\nthat\nwould be unfair. \u00a0If you can get more for your skills or your goods by going above the lowest price that makes you comfortably cheerful, go for it.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "That said, if I'm asking everybody in the room their Cheerful Price to do my laundry, and the poorest person present names the lowest Cheerful Price, I think that's... actually everything working as intended? \u00a0The effect is that the person with the lowest prior endowment is the one who actually gets the money and feels cheerful about that; and the Cheerful part means they get\nmore\nmoney (I hope) than if I asked everybody present to name their price without specifying the Cheerful part.\nMy current cheerful price for \"Please write me a short story about the following\" might be above $10,000 today. \u00a0In 2001 it might have been $100, back when $100 was 1/20th of the cost of the car I was driving. \u00a0The end result of this 10,000% increase in how much money it takes to make me happy, as I've accumulated more money... is that now people who'll write you a story for $100 get your money, instead of me. \u00a0That seems a good phenomenon from the standpoint of financial equality; it causes money to flow from people who have more money towards people who have less money.\nBut on a more personal level, if I ask someone to name an amount of money that makes them feel cheerful and energized, I expect and hope that this causes\nmore",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "money to flow from me to them. \u00a0If the technically defined \"cheerful price\" is less than the person otherwise thinks they can get from a payor, then by all means, they should tell me: \u00a0\"Don't worry! \u00a0I set my standard fee of $X high enough that I already feel cheerful.\" \u00a0And then I won't feel worried about paying too little and everything will be fine.\nQ: \u00a0Technical objection: \u00a0Surely if you're asking everybody in the room to name their Cheerful Price for something, you should pay the lowest bidder the second-lowest bid, not pay the lowest bidder their actual bid?\nUhhh... possibly? \u00a0I'm not actually sure that this logic works the same way when you're asking people for Cheerful Prices - I think you're already asking them to nudge the price upwards from \"the lowest they'd accept\", which means you don't have to give them the second-price of the auction in order to ensure they get any gains from trade. \u00a0It's a more friendly idiom in general - you're asking them and trusting them to tell you the truth about what won't make them say \"ow\". \u00a0And despite that example of the laundry, the whole thing seems more useful for individual interactions than auctions? \u00a0But you may still have a point. \u00a0I'll have to think about it.\nQ: \u00a0Now that I think about it, this whole document seems to be written like the cheerful price is always something that the payor requests the payee to name. \u00a0Why would it always be like that?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I wrote from that perspective because it's the perspective I usually occupy - at this stage of my life, I'm usually looking for more ways to trade away money for what I want, not looking for more ways to trade away other things for money.\nAnd since not everybody can afford to offer us our Cheerful Price, in interactions between friends, there's nonzero reason to not just rush ahead and name a cheerful price before being asked. \u00a0Or to avoid misunderstandings and possible resentment about you expecting too much, you should say \"This is my cheerful price\" rather than \"This is my price\", if the other person didn't already explicitly say \"Please name your cheerful price.\"\nBut sure: \u00a0Some people who are in the habit of unilaterally underpricing themselves, to the point where they're undertrading, even though they have pricing power to ask for more, might do well to take the initiative on their side to think \"What would I need to be getting to make me cheerful about more interactions here?\" \u00a0And this is true whether the trade is for money, or not.\nOr some people who are in the habit of underpaying, and not getting all the things they want, even though they have more to offer, might do well to think, \"Would I still feel cheerful about paying much more for X, if I got more or faster or better X? \u00a0Is there some way to give more and get more?\" \u00a0And this is true whether the trade is for money, or not.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Some people may benefit from switching perspectives to ask about cheerful prices with respect to some\ninternal\nbargains between the voices in their head; to apply the same perspective to one-person transactions, not just two-person transactions.\nBut to repeat the warning from above: \u00a0 If you have nothing more to give, or no pricing power to ask for more; then thinking about what it would take to make the bargain cheerful, may only make you sadder. \u00a0And this is true whether for outer bargains, or inner bargains.\nLife does not promise us that we will always get our Cheerful Prices, even from our friends, and not even from ourselves. \u00a0Not every trade produces so much gain to divide, even among many good trades worth making.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "(\nPartially in response to\nAGI Ruin: A list of Lethalities\n.\nWritten in the same rambling style. Not exhaustive.\n)\nAgreements\nPowerful AI systems have a good chance of deliberately and irreversibly disempowering humanity. This is a much more likely failure mode than humanity killing ourselves with destructive physical technologies.\nCatastrophically risky AI systems could plausibly exist soon, and there likely won\u2019t be a strong consensus about this fact until such systems pose a meaningful existential risk per year. There is not necessarily any \u201cfire alarm.\u201d\nEven if there were consensus about a risk from powerful AI systems, there is a good chance that the world would respond in a totally unproductive way. It\u2019s wishful thinking to look at possible stories of doom and say \u201cwe wouldn\u2019t let that happen;\u201d humanity is fully capable of messing up even very basic challenges, especially if they are novel.\nI think that many of the projects intended to help with AI alignment don't make progress on key difficulties and won\u2019t significantly reduce the risk of catastrophic outcomes. This is related to people gravitating to whatever research is most tractable and not being too picky about what problems it helps with, and related to a low level of concern with the long-term future in particular. Overall, there are relatively few researchers who are effectively focused on the technical problems most relevant to existential risk from alignment failures.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "There are strong social and political pressures to spend much more of our time talking about how AI shapes existing conflicts and shifts power. This pressure is already playing out and it doesn\u2019t seem too likely to get better.\nEven when thinking about accident risk, people\u2019s minds seem to go to what they think of as \u201cmore realistic and less sci fi\u201d risks that are much less likely to be existential (and sometimes I think less plausible). It\u2019s very possible this dynamic won\u2019t change until after actually existing AI systems pose an existential risk.\nThere is a good chance that an AI catastrophe looks like an abrupt \u201ccoup\u201d where AI systems permanently disempower humans with little opportunity for resistance. People seem to consistently round this risk down to more boring stories that fit better with their narratives about the world. It is quite possible that an AI coup will be sped up by humans letting AI systems control killer robots, but the difference in timeline between \"killer robots everywhere, AI controls everything\" and \"AI only involved in R&D\" seems like it's less than a year.\nThe broader intellectual world seems to wildly overestimate how long it will take AI systems to go from \u201clarge impact on the world\u201d to \u201cunrecognizably transformed world.\u201d This is more likely to be years than decades, and there\u2019s a real chance that it\u2019s months. This makes alignment harder and doesn\u2019t seem like something we are collectively prepared for.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Humanity usually solves technical problems by iterating and fixing failures; we often resolve tough methodological disagreements very slowly by seeing what actually works and having our failures thrown in our face. But it will probably be possible to build valuable AI products without solving alignment, and so reality won\u2019t \u201cforce us\u201d to solve alignment until it\u2019s too late. This seems like a case where we will have to be unusually reliant on careful reasoning rather than empirical feedback loops for some of the highest-level questions.\nAI systems will ultimately be wildly superhuman, and there probably won\u2019t be strong technological hurdles right around human level. Extrapolating the rate of existing AI progress suggests you don\u2019t get too much time between weak AI systems and very strong AI systems, and AI contributions could very easily go from being a tiny minority of intellectual work to a large majority over a few years.\nIf you had incredibly powerful unaligned AI systems running on a server farm somewhere, there is very little chance that humanity would maintain meaningful control over its future.\n\u201cDon\u2019t build powerful AI systems\u201d appears to be a difficult policy problem, requiring geopolitical coordination of a kind that has often failed even when the stakes are unambiguous and the pressures to defect are much smaller.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I would not expect humanity to necessarily \u201crise to the challenge\u201d when the stakes of a novel problem are very large. I was 50-50 about this in 2019, but our experience with COVID has further lowered my confidence.\nThere is probably no physically-implemented reward function, of the kind that could be optimized with SGD, that we\u2019d be happy for an arbitrarily smart AI to optimize as hard as possible. (I\u2019m most optimistic about approaches where RL is only performed on a reward function that gets smarter in parallel with the agent being trained.)\nTraining an AI to maximize a given reward function does not generically produce an AI which is internally \u201cmotivated\u201d to maximize reward. Moreover, at some level of capability, a very wide range of motivations for an AI would lead to loss-minimizing behavior on the training distribution because minimizing loss is an important strategy for an AI to preserve its influence over the world.\nIt is more robust for an AI system to learn a good model for the environment, and what the consequences of its actions will be, than to learn a behavior like \u201cgenerally being nice\u201d or \u201ctrying to help humans.\u201d Even if an AI was imitating data consisting of \u201cwhat I would do if I were trying to be nice,\u201d it would\nstill",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "be more likely to eventually learn to imitate the actual physical process producing that data rather than absorbing some general habit of niceness. And in practice the data we produce will not be perfect, and so \u201cpredict the physical process generating your losses\u201d is going to be positively selected for by SGD.\nYou shouldn\u2019t say something like \u201cwell I might as well assume there\u2019s a hope\u201d and thereby live in a specific unlikely world where alignment is unrealistically easy in one way or another. Even if alignment ends up easy, you would be likely to end up predicting the wrong way for it to be easy. If things look doomed to you, in practice it\u2019s better to try to maximize log odds of success as a more general and robust strategy for taking advantage of lucky breaks in a messy and hard-to-predict world.\nNo current plans for aligning AI have a particularly high probability of working without a lot of iteration and modification. The current state of affairs is roughly \u201cif alignment turns out to be a real problem, we\u2019ll learn a lot about it and iteratively improve our approach.\u201d If the problem is severe and emerges quickly, it would be better if we had a clearer plan further in advance\u2014we\u2019d still have to adapt and learn, but starting with something that looks like it could work on paper would put us in a much better situation.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Many research problems in other areas are chosen for tractability or being just barely out of reach. We pick benchmarks we can make progress on, or work on theoretical problems that seem well-posed and approachable using existing techniques. Alignment isn\u2019t like that; it was chosen to be an important problem, and there is no one ensuring that the game is \u201cfair\u201d and that the problem is soluble or tractable.\nDisagreements\n(\nMostly stated without argument\n.)\nEliezer often equivocates between \u201cyou have to get alignment right on the first \u2018critical\u2019 try\u201d and \u201cyou can\u2019t learn anything about alignment from experimentation and failures before the critical try.\u201d\nThis distinction is very important, and I agree with the former but disagree with the latter. Solving a scientific problem without being able to learn from experiments and failures is incredibly hard. But we will be able to learn a lot about alignment from experiments and trial and error; I think we can get a lot of feedback about what works and deploy more traditional R&D methodology. We have toy models of alignment failures, we have standards for interpretability that we can\u2019t yet meet, and we have theoretical questions we can\u2019t yet answer.. The difference is that reality doesn\u2019t\nforce\nus to solve the problem, or tell us clearly which analogies are the right ones, and so it\u2019s possible for us to push ahead and build AGI without solving alignment. Overall this consideration seems like it makes the\ninstitutional",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "problem vastly\u00a0harder, but does not have such a large effect on the scientific problem.\nEliezer often talks about AI systems that are able to easily build nanotech and overpower humans decisively, and describes a vision of a rapidly unfolding doom from a single failure. This is what would happen if you were magically given an extraordinarily powerful AI and then failed to aligned it, but I think it\u2019s very unlikely what will happen in the real world.\nBy the time we have AI systems that can overpower humans decisively with nanotech, we have other AI systems that will either kill humans in more boring ways or else radically advanced the state of human R&D.\nMore generally, the cinematic universe of Eliezer\u2019s stories of doom doesn\u2019t seem to me like it holds together, and I can\u2019t tell if there is a more realistic picture of AI development under the surface.\nOne important factor seems to be that Eliezer often imagines scenarios in which AI systems avoid making major technical contributions, or revealing the extent of their capabilities, because they are lying in wait to cause trouble later. But\nif we are constantly training AI systems to do things that look impressive, then SGD will be aggressively selecting against any AI systems who don\u2019t do impressive-looking stuff.\nSo by the time we have AI systems who can develop molecular nanotech, we will definitely have had systems that did something slightly-less-impressive-looking.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "AI improving itself is most likely to look like AI systems doing R&D in the same way that humans do. \u201cAI smart enough to improve itself\u201d is not a crucial threshold, AI systems will get gradually better at improving themselves. Eliezer appears to expect AI systems performing extremely fast recursive self-improvement before those systems are able to make superhuman contributions to other domains (including alignment research), but I think this is mostly unjustified. If Eliezer doesn\u2019t believe this, then his arguments about the alignment problem that\nhumans need to solve\nappear to be wrong.\nThe notion of an AI-enabled \u201cpivotal act\u201d seems misguided.\nAligned AI systems can reduce the period of risk of an unaligned AI by advancing alignment research\n, convincingly demonstrating the risk posed by unaligned AI, and consuming the \u201c\nfree energy\n\u201d that an unaligned AI might have used to grow explosively. No particular act needs to be pivotal in order to greatly reduce the risk from unaligned AI, and the search for single pivotal acts leads to unrealistic stories of the future and unrealistic pictures of what AI labs should do.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Many of the \u201cpivotal acts\u201d that Eliezer discusses involve an AI lab achieving a \u201cdecisive strategic advantage\u201d (i.e. overwhelming hard power) that they use to implement a relatively limited policy, e.g. restricting the availability of powerful computers. But the same hard power would also let them arbitrarily dictate a new world order, and would be correctly perceived as an existential threat to existing states. Eliezer\u2019s view appears to be that a decisive strategic advantage is the most realistic way to achieve these policy goals,\ndespite\nthe fact that building powerful enough AI systems runs an overwhelming risk of destroying the world via misalignment. I think that preferring this route to more traditional policy influence requires extreme confidence about details of the policy situation; that confidence might be justified by someone who knew a lot more about the details of government than I do, but Eliezer does not seem to. While I agree that this kind of policy change would be an unusual success in historical terms, the probability still seems much higher than Eliezer\u2019s overall probabilities of survival. Conversely, I think Eliezer greatly underestimates how difficult it would be for an AI developer to covertly take over the world, how strongly and effectively governments would respond to that possibility, and how toxic this kind of plan is.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I think Eliezer is probably wrong about how useful AI systems will become, including for tasks like AI alignment, before it is catastrophically dangerous. I believe we are relatively quickly approaching AI systems that can meaningfully accelerate progress by generating ideas, recognizing problems for those ideas and, proposing modifications to proposals,\netc.\nand that all of those things will become possible in a small way well before AI systems that can double the pace of AI research. By the time AI systems can double the pace of AI research, it seems like they can greatly accelerate the pace of alignment research. Eliezer is right that this doesn\u2019t make the problem go away (if humans don\u2019t solve alignment, then why think AIs will solve it?) but I think it does mean that arguments about how recursive self-improvement quickly kicks you into a lethal regime are wrong (since AI is accelerating the timetable for both alignment and capabilities).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When talking about generalization outside of the training distribution, I think Eliezer is generally pretty sloppy. I think many of the points are roughly right, but that it\u2019s way too sloppy to reach reasonable conclusions after several steps of inference. I would love to see real discussion of these arguments, and in some sense it seems like Eliezer is a good person to push that discussion forward. Right now I think that relevant questions about ML generalization are in fact pretty subtle; we can learn a lot about them in advance but right now just mostly don\u2019t know. Similarly, I think Eliezer\u2019s reasoning about convergent incentives and the deep nature of consequentialism is too sloppy to get to correct conclusions and the resulting assertions are wildly overconfident.\nIn particular, existing AI training strategies don\u2019t need to handle a \u201cdrastic\u201d distribution shift from low levels of intelligence to high levels of intelligence. There\u2019s nothing in the foreseeable ways of building AI that would call for a big transfer like this, rather than continuously training as intelligence gradually increases. Eliezer seems to partly be making a relatively confident claim that the nature of AI is going to change a lot, which I think is probably wrong and is clearly overconfident. If he had been actually making concrete predictions over the last 10 years I think he would be losing a lot of them to people more like me.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Eliezer strongly expects sharp capability gains, based on a combination of arguments that I think don\u2019t make sense and an analogy with primate evolution which I think is being applied poorly. We\u2019ve talked about this before, and I still think Eliezer\u2019s position is probably wrong and clearly overconfident. I find Eliezer\u2019s more detailed claims, e.g. about hard thresholds, to be much more implausible than his (already probably quantitatively wrong) claims about takeoff speeds.\nEliezer seems confident about the difficulty of alignment based largely on his own experiences working on the problem. But in fact society has spent very little total effort working on the problem, and MIRI itself would probably be unable to solve or even make significant progress on the large majority of problems that existing research fields routinely solve. So I think right now we mostly don\u2019t know how hard the problem is (but it may well be very hard, and even if it\u2019s easy we may well fail to solve it). For example, the fact that MIRI tried and failed to find a \u201ccoherent formula for corrigibility\u201d is not much evidence that corrigibility is \u201cunworkable.\u201d\nEliezer says a lot of concrete things about how research works and about what kind of expectation of progress is unrealistic (e.g. talking about bright-eyed optimism in\nlist of lethalities",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "). But I don\u2019t think this is grounded in an understanding of the history of science, familiarity with the dynamics of modern functional academic disciplines, or research experience. The Eliezer predictions most relevant to \u201chow do scientific disciplines work\u201d that I\u2019m most aware of are incorrectly predicting that physicists would be wrong about the existence of the Higgs boson (\nLW bet registry\n) and expressing the view that real AI would likely emerge from a small group rather than a large industry (\npg 436\nbut expressed many places).\nI think Eliezer generalizes a lot from pessimism about solving problems easily to pessimism about solving problems at all; or from the fact that a particular technique doesn\u2019t immediately solve a problem to pessimism about the helpfulness of research on that technique. I disagree with Eliezer about how research progress is made, and don\u2019t think he has any special expertise on this topic. Eliezer often makes objections to particular implementations of projects (like using interpretability tools for training). But in order to actually talk about whether a research project is likely to succeed, you really really need to engage with the existential quantifier where future researchers get to choose implementation details to make it work. At a\nminimum",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "that requires engaging with the strongest existing versions of these proposals, and if you haven\u2019t done that (as Eliezer hasn\u2019t) then you need to take a different kind of approach. But even if you engage with the best existing concrete proposals, you still need to think carefully about whether your objections are the kind of thing that will be hard to overcome as people learn more details in the future. One way of looking at this is that Eliezer is appropriately open-minded about existential quantifiers applied to future AI systems thinking about how to cause trouble, but seems to treat existential quantifiers applied to future humans in a qualitatively rather than quantitatively different way (and as described throughout this list I think he overestimates the quantitative difference).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As an example, I think Eliezer is unreasonably pessimistic about interpretability while being mostly ignorant about the current state of the field. This is true both for the level of understanding potentially achievable by interpretability, and the possible applications of such understanding. I agree with Eliezer that this seems like a hard problem and many people seem unreasonably optimistic, so I might be sympathetic if Eliezer was making claims with moderate confidence rather than high confidence. As far as I can tell most of Eliezer\u2019s position here comes from general intuitions rather than arguments, and I think those are much less persuasive when you don\u2019t have much familiarity with the domain.\nEarly transformative AI systems will probably do impressive technological projects by being trained on smaller tasks with shorter feedback loops and then composing these abilities in the context of large collaborative projects\n(initially involving a lot of humans but over time increasingly automated). When Eliezer dismisses the possibility of AI systems performing safer tasks millions of times in training and then safely transferring to \u201cbuild nanotechnology\u201d (point 11 of\nlist of lethalities\n) he is not engaging with the kind of system that is likely to be built or the kind of hope people have in mind.\nList of lethalities",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "#13 makes a particular argument that we won\u2019t see many AI problems in advance; I feel like I see this kind of thinking from Eliezer a lot but it seems misleading or wrong. In particular, it seems possible to study the problem that AIs may \u201cchange [their] outer behavior to deliberately look more aligned and deceive the programmers, operators, and possibly any loss functions optimizing over [them]\u201d in advance. And while it\u2019s true that\nif\nyou fail to solve that problem\nthen\nyou won\u2019t notice other problems, this doesn\u2019t really affect the probability of solving alignment overall: if you don\u2019t solve that problem then you die, and if you do solve that problem then then you can study the other problems.\nI don\u2019t think\nlist of lethalities\nis engaging meaningfully with the most serious hopes about how to solve the alignment problem. I don\u2019t think that\u2019s necessarily the purpose of the list, but it\u2019s quite important if you want to assess the probability of doom or contribute meaningfully to solving the problem (or to complain about other people producing similar lists).\nI think that natural selection is a relatively weak analogy for ML training. The most important disanalogy is that we can deliberately shape ML training.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Animal breeding would be a better analogy, and seems to suggest a different and much more tentative conclusion. For example, if humans were being actively bred for corrigibility and friendliness, it looks to me like like they would quite likely be corrigible and friendly up through the current distribution of human behavior.\nIf that breeding process was continuously being run carefully by the smartest of the currently-friendly humans, it seems like it would plausibly break down at a level very far beyond current human abilities.\nEliezer seems to argue that humans couldn\u2019t verify pivotal acts proposed by AI systems (e.g. contributions to alignment research), and that this further makes it difficult to safely perform pivotal acts. In addition to disliking his concept of pivotal acts, I think that this claim is probably wrong and clearly overconfident. I think it doesn\u2019t match well with pragmatic experience in R&D in almost any domain, where verification is\nmuch, much\neasier than generation in virtually every domain.\nEliezer is relatively confident that you can\u2019t train powerful systems by imitating human thoughts, because too much of human thinking happens under the surface.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I think this is fairly plausible but it\u2019s not at all obvious, and moreover there are plenty of techniques intermediate between \u201ccopy individual reasoning steps\u201d and \u201coptimize end-to-end on outcomes.\u201d I think that the last 5 years of progress in language modeling have provided significant evidence that training AI to imitate human thought may be economically competitive at the time of transformative AI, potentially bringing us to something more like a 50-50 chance. I can\u2019t tell if Eliezer should have lost Bayes points here, but I suspect he would have and if he wants us to evaluate his actual predictions I wish he would say\nsomething\nabout his future predictions.\nThese last two points (and most others from this list) aren\u2019t aren\u2019t actually part of my central alignment hopes or plans. Alignment hopes, like alignment concerns, can be disjunctive. In some sense they are even more disjunctive, since the existence of humans who are trying to solve alignment is considerably more robust than the existence of AI systems who are trying to cause trouble (such AIs only exist if humans have\nalready failed\nat significant parts of alignment). Although my research is focused on cases where almost every factor works out against us, I think that you can get a lot of survival probability from easier worlds.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Eliezer seems to be relatively confident that AI systems will be very alien and will understand many things about the world that humans don\u2019t, rather than understanding a similar profile of things (but slightly better), or having weaker understanding but enjoying other advantages like much higher serial speed. I think this is very unclear and Eliezer is wildly overconfident. It seems plausible that AI systems will learn much of how to think by predicting humans even\nif\nhuman language is a uselessly shallow shadow of human thought, because of the extremely short feedback loops. It also seems quite possible that most of their knowledge about science will be built by an explicit process of scientific reasoning and inquiry that will proceed in a recognizable way to human science even if their minds are quite different. Most importantly, it seems like AI systems have huge structural advantages (like their high speed and low cost) that suggest they will have a transformative impact on the world (\nand obsolete human contributions to alignment\nretracted\n) well before they need to develop superhuman understanding of much of the world or tricks about how to think, and so even if they have a very different profile of abilities to humans they may still be subhuman in many important ways.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "AI systems reasoning about the code of other AI systems is not likely to be an important dynamic for early cooperation between AIs. Those AI systems look very likely to be messy, such that the only way AI systems will reason about their own or others\u2019 code is by looking at behavior and using the same kinds of tools and reasoning strategies as humans. Eliezer has a consistent pattern of identifying important long-run considerations, and then flatly asserting that they are relevant in the short term without evidence or argument. I think Eliezer thinks this pattern of predictions isn\u2019t yet conflicting with the evidence because these predictions only kick in at some later point (but still early enough to be relevant), but this is part of what makes his prediction track record impossible to assess and why I think he is greatly overestimating it in hindsight.\nEliezer\u2019s model of AI systems cooperating with each other to undermine \u201cchecks and balances\u201d seems wrong to me, because it focuses on cooperation and the incentives of AI systems. Realistic proposals mostly don\u2019t need to rely on the\nincentives\nof AI systems, they can instead rely on gradient descent\nselecting\nfor systems that play games competitively, e.g. by searching until we find an AI which raises compelling objections to other AI systems\u2019 proposals. (Note: I may be misunderstanding what he\u2019s saying in places like\nlist of lethalities",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "35; those may be a response to random things people say on the internet rather than engaging with alignment strategies that are being explored in practice. But even then I think he is responding to relatively weak versions of these arguments.) Eliezer equivocates between a line like \u201cAI systems will cooperate\u201d and \u201cThe verifiable activities you could use gradient descent to select for won\u2019t function appropriately as checks and balances.\u201d But Eliezer\u2019s position is a conjunction that fails if either step fails, and jumping back and forth between them appears to totally obscure the actual structure of the argument.\nEliezer seems to expect that by the time AI systems can achieve pivotal acts, they can also manipulate humans very well\u2014such that for example it\u2019s pointless to try to have debates between them or to try to play an adversarial game against them. But if we imagine minds with a human profile of abilities, it seems like they would be superhuman at R&D around the same time they were superhuman at persuasion, and could easily go either way. Moreover, there are a lot of reasons the AI seems\nmuch",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "more likely to be superhuman at R&D if that\u2019s what we want: it\u2019s trained primarily to work on R&D, it\u2019s using tools and structure designed to facilitate R&D, there is a large collaboration amongst AIs to advance R&D whereas manipulation is often being performed by individual AI systems trying to win a debate. And on top of all that, if anything I very weakly expect AI systems\u2019 comparative advantage to be in R&D rather than human manipulation (since R&D is wildly out of distribution for humans).\nI don\u2019t think surviving worlds have a plan in the sense Eliezer is looking for.\nBased on what Eliezer says I don\u2019t feel like he has a clear or accurate picture of what successful \u201cplans\u201d look like in the real world. I don\u2019t see any particular reason to defer to Eliezer at all on this point.\nEliezer says that his\nlist of lethalities",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "is the kind of document that other people couldn\u2019t write and therefore shows they are unlikely to contribute (point 41). I think that\u2019s wrong. I think Eliezer\u2019s document is mostly aimed at rhetoric or pedagogy rather than being a particularly helpful contribution to the field that others should be expected to have prioritized; I think that which ideas are \u201cimportant\u201d is mostly a consequence of Eliezer\u2019s idiosyncratic intellectual focus rather than an objective fact about what is important; the main contributions are collecting up points that have been made in the past and ranting about them and so they mostly reflect on Eliezer-as-writer; and perhaps most importantly, I think more careful arguments on more important difficulties are in fact being made in other places.\nFor example, ARC\u2019s\nreport on ELK\ndescribes at least 10 difficulties of the same type and severity as the ~20 technical difficulties raised in Eliezer\u2019s list.\nAbout half of them are overlaps, and I think the other half are if anything more important since they are more relevant to core problems with realistic alignment strategies.\n[1]\nMy take on Eliezer's takes\nEliezer raises many good considerations backed by pretty clear arguments, but makes confident assertions that are much stronger than anything suggested by actual argument.\nEliezer\u2019s post (and most of his writing) isn\u2019t bringing much new evidence to the table; it mostly either reasons\na priori",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "or draws controversial conclusions from uncontroversial evidence. I think that calls for a different approach than Eliezer has taken historically (if the goal was to productively resolve these disagreements).\nI think that these arguments mostly haven\u2019t been written down publicly so that they can be examined carefully or subject to criticism. It\u2019s not clear whether Eliezer has the energy to do that, but I think that people who think that Eliezer\u2019s position is important should try to understand the arguments well enough to do that.\nI think that people with Eliezer\u2019s views haven\u2019t engaged very much productively with people who disagree (and have often made such engagement hard). I think that if you really dive into any of these key points you will quickly reach details where Eliezer cannot easily defend his view to a smart disinterested audience. And I don\u2019t think that Eliezer could pass an ideological Turing test for people who disagree.\nI think those are valuable steps to take if you have a contrarian take of great importance, which remains controversial even within your weird corner of the world, and whose support comes almost entirely from reasoning and argument.\nA lot of the post seems to rest on intuitions and ways of thinking that Eliezer feels are empirically supported (rather than on arguments that can be explicitly stated). But I don\u2019t feel like I actually have much evidence about that, so I think it really does just come down to the arguments.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I think Eliezer would like to say that the last 20 years give a lot of evidence for his object-level intuitions and general way of thinking about the world. If that\u2019s the case, I think we should very strongly expect that he can state predictions about the future that will systematically be better than those of people who don\u2019t share his intuitions or reasoning strategies. I remain happy to make predictions about any questions he thinks would provide this kind of evidence, or to state a bunch of random questions where I\u2019m happy to predict (where I think he will probably slightly underperform me). If there aren\u2019t any predictions about the future where these intuitions and methodologies overperform, I think you should be very skeptical that they got a lot of evidence over the last 20 years (and that\u2019s at least something that requires explanation).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I think Eliezer could develop good intuition about these topics that is \u201cbacked up\u201d by predicting the results of more complicated arguments using more broadly-accepted reasoning principles. Similarly, a mathematician might have great intuitions about the truth of a theorem, and those intuitions could come entirely from feedback loops involving formal proofs rather than empirical data. But if two mathematicians had differing intuitions about a theorem, and their intuitions both came from formally proving a bunch of similar theorems, then I think the way to settle the disagreement is by using the normal rules of logic governing proofs. So this brings us back to the previous bullet point, and I think Eliezer should be more interested in actually making arguments and engaging with legitimate objections.\nI don\u2019t think Eliezer has any kind of track record of exhibiting understanding in other ways (e.g. by accomplishing technological goals or other projects that require engaging with details of the world or making good day-to-day predictions). I think that\u2019s OK, but it means that I more strongly expect any empirically-backed intuitions to be cashed out as either predictions from afar or more careful arguments.\n^\nTen examples off the top of my head, that I think are about half overlapping and where I think the discussions in the ELK doc are if anything more thorough than the discussions in the list of lethalities:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Goals defined in terms of sense data are manipulable by an AI who can compromise sensors, and this is a serious obstruction to using ML to optimize what we actually care about.\nAn AI may manipulate sensors by exploiting facts about the world or modes of heuristic reasoning that humans are totally unfamiliar with, such that humans couldn\u2019t recognize such tampering even if they spent a very long time examining proposed actions.\nThe human process of scientific understanding, even if automated, may end up being significantly less efficient than the use of gradient descent to find opaque models of the world. In this case, it may be inevitable that AI systems understand things about the world we don\u2019t even if they try to help us do science.\nIf an AI is trained to predict human judgments or optimize scores as assessed by humans, then humans are likely to make errors. An AI system will eventually learn these errors rather than learning the intended behavior. Even if these errors aren\u2019t themselves important, it will then predictably copy human errors out of distribution leading to catastrophic outcomes.\nEven if humans make no errors in the training set, an AI which understands the world already has a model of a human which can be quickly repurposed to make good predictions about human judgments, and so it will tend to do this and therefore copy human errors off distribution.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Even if the AI has no model of a human, in the limit where the AI\u2019s model is very complex and alien it is still faster and simpler for the AI to learn a model of \u201cwhat a human would say\u201d from scratch then to learn the intended ontology identification. So we can't count on SGD.\nThere are many training strategies that can train an AI to answer questions even in cases where humans could not answer correctly. However most of the approaches we know now, including those being explored in practice, seem to consistently top out at \u201cquestions that humans could answer if they have a lot more compute\" which does not always seem good enough.\nWe could imagine more elaborate games where the easiest strategy for the AI is honesty, and then to regularize on computation time in order to learn an honest policy, but those require us to be careful about the construction of the training data in order ensure that the task is sufficiently hard, and there are no existing proposals that have that property. It's very hard to even set up games for which no strategy can outperform honesty.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Even if you were optimizing based on reliable observations of the real world, there are many bad actions that have no human-legible consequences for many years. At the point when legible consequences materialize it may be in a world that is too complex for existing humans to evaluate whether they are good or bad. If we don\u2019t build an AI that understands our preferences about this kind of subtle bad behavior, then a competitive world will push us into a bad outcome.\nIf the simplest policy to succeed at our task is a learned optimizer, and we try to regularize our AI to e.g. answer questions quickly, then its best strategy may be to internally searching for a policy which answers questions slowly (because it\u2019s quicker to find such a policy, and the time taken by the search is larger than the time taken by the mesapolicy). This makes it difficult to lean on regularization strategies to incentivize honesty.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "TL;DR version\nIn the course of my life, there have been a handful of times I discovered an idea that changed the way I thought about where our species is headed. The first occurred when I picked up Nick Bostrom\u2019s book \u201csuperintelligence\u201d and realized that AI would utterly transform the world. The second was when I learned about\nembryo selection\nand how it could change future generations. And the third happened a few months ago when I read a message from a friend of mine on Discord about editing the genome of a living person.\nWe\u2019ve had gene therapy to treat cancer and single gene disorders for decades. But the process involved in making such changes to the cells of a living person is excruciating and extremely expensive.\nCAR T-cell therapy\n, a treatment for certain types of cancer, requires the removal of white blood cells via IV, genetic modification of those cells outside the body, culturing of the modified cells, chemotherapy to kill off most of the remaining unmodified cells in the body, and reinjection of the genetically engineered ones. The price is\n$500,000 to $1,000,000\n.\nAnd it only adds a single gene.\nThis is a big problem if you care about anything besides monogenic diseases. Most traits, like personality, diabetes risk, and intelligence are controlled by hundreds to tens of thousands of genes, each of which exerts a tiny influence. If you want to significantly affect someone\u2019s risk of developing lung cancer, you probably need to make hundreds of edits.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If changing one gene to treat cancer requires the involved process described above, what kind of tortuous process would be required to modify hundreds?\nIt seemed impossible. So after first thinking of the idea well over a year ago, I shelved it without much consideration. My attention turned to what seemed like more practical ideas such as embryo selection.\nMonths later, when I read a message from my friend Kman on discord talking about gene editing adults in adult brains, I was fairly skeptical it was worth my time.\nHow could we get editing agents into all 200 billion brain cells? Wouldn\u2019t it cause major issues if some cells received edits and others didn\u2019t? What if the gene editing tool targeted the wrong gene? What if it provoked some kind of immune response?\nBut recent progress in AI had made me think we might not have much time left before AGI, so given that adult gene editing might have an impact on a much shorter time scale than embryo selection, I decided it was at least worth a look.\nSo I started reading. Kman and I pored over papers on base editors and prime editors and in-vivo delivery of CRISPR proteins via adeno-associated viruses trying to figure out whether this pipe dream was something more. And after a couple of months of work, I have become convinced that there are no known fundamental barriers that would prevent us from doing this.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "There are still unknowns which need to be worked on. But many pieces of a viable editing protocol have already been demonstrated independently in one research paper or another.\nIf this works, it\u2019s going to be a very big deal\nIt is hard to overstate just how big a deal it would be if we could edit thousands of genes in cells all over the human body. Nearly every trait and every disease you\u2019ve ever heard of has a significant genetic component, from intelligence to breast cancer. And the amount of variance already present in the human gene pool is stunning. For many traits, we could take someone from the 3rd percentile to the 97th percentile by editing just 20% of the genes involved in determining that trait.\nTweaking even a dozen genes might be able to halt the progression of Alzheimer\u2019s, or alleviate the symptoms of major autoimmune diseases.\nThe same could apply to other major causes of aging: diabetes, heart disease, cancers. All have genetic roots to some degree or other. And all of this could potentially be done in people who have already been born.\nOf particular interest to me is whether we could modify intelligence. In the past ten thousand years, humans have progressed mostly through making changes to their tools and environment. But in that time, human genes and human brains have changed little. With limitations to brain power, new impactful ideas naturally become\nharder to find\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is particularly concerning because we have a number of extremely important technical challenges facing us today and a very limited number of people with the ability to make progress on them. Some may simply be beyond the capabilities of current humans.\nOf these, the one I think about most is\u00a0 technical AI alignment work. It is not an exaggeration to say that the lives of literally everyone depend on whether a few hundred engineers and mathematicians can figure out how to control the machines built by the mad scientists in the office next door.\nDemis Hassabis has a famous phrase he uses to describe his master plan for Deepmind: solve intelligence, then use that intelligence to solve everything else. I have a slightly more meta level plan to make sure Demis\u2019s plan doesn\u2019t kill everyone; make alignment researchers smarter, then kindly ask those researchers to solve alignment.\nThis is quite a grandiose plan for someone writing a LessWrong blog post. And there\u2019s a decent chance it won\u2019t work out for either technical reasons or because I can't find the resources and talent to help me solve all the technical challenges. At the end of the day, whether or not anything I say has an impact will depend on whether I or someone else can develop a working protocol.\nHow does intelligence even work at a genetic level?\nOur best estimate based on the last decade of data is that the genetic component of intelligence is controlled by somewhere between\n10,000\nand\n24,000",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "variants. We also know that each one, on average, contributes about +-0.2 IQ points.\nGenetically altering IQ is more or less about flipping a sufficient number of IQ-decreasing variants to their IQ-increasing counterparts. This sounds overly simplified, but it\u2019s surprisingly accurate; most of the variance in the genome is linear in nature, by which I mean the effect of a gene doesn\u2019t usually depend on which other genes are present.\nSo modeling a continuous trait like intelligence is actually extremely straightforward: you simply add the effects of the IQ-increasing alleles to to those of the IQ-decreasing alleles and then normalize the score relative to some reference group.\nTo simulate the effects of editing on intelligence, we\u2019ve built a model based on summary statistics from UK Biobank, the assumptions behind which you can find\nin the appendix\nBased on the model, we can come to a surprising conclusion: there is enough genetic variance in the human population to create a genome with a predicted IQ of about 900.\nI don\u2019t expect such an IQ to actually result from flipping all IQ-decreasing alleles to their IQ-increasing variants for the same reason I don\u2019t expect to reach the moon by climbing a very tall ladder; at some point, the simple linear model will break down.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But we have strong evidence that such models function quite well within the current human range, and likely somewhat beyond it. So we should actually be able to genetically engineer people with greater cognitive abilities than anyone who\u2019s ever lived, and do so without necessarily making any great trade-offs.\nEven if a majority of iq-increasing genetic variants had some tradeoff such as increasing disease risk (which current evidence suggests they mostly don\u2019t), we could always select the subset that doesn\u2019t produce such effects. After all, we have 800 IQ points worth of variants to choose from!\nTotal maximum gain\nGiven the current set of publicly available intelligence-associated variants available in UK biobank, here\u2019s a graph showing the expected effect on IQ from editing genes in adults.\nNot very impressive! There are several factors at play deflating the benefits shown by this graph.\nAs a rough estimate, we expect about\n50%\nof the targeted edits to be successfully made in an average brain cell. The actual amount could be more or less depending on editor and delivery efficiency.\nSome genes we know are involved in intelligence will have zero effect if edited in adults because they primarily influence brain development (\nsee here for more details\n). Though there is substantial uncertainty here, we are assuming that, on average, an intelligence-affecting variant will have only\n50%\nof the effect size as it would if edited in an embryo.\nWe can only target about\n90%",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "of genetic variants with\nprime editors\n.\nAbout\n98%\nof intelligence-affecting alleles are in non-protein-coding regions. The remaining 2% of variants are in protein-coding regions and are\nprobably not safe to target\nwith current editing tools due to the risk of frameshift mutations.\nOff-target edits, insertions and deletions will probably have some negative effect. As a very rough guess, we can probably maintain\n95%\nof the benefit after the negative effects of these are taken into account.\nMost importantly,\nthe current intelligence predictors only identify a small subset of intelligence-affecting genetic variants\n. With more data we can dramatically increase the benefit from editing for intelligence. The same goes for many other traits such as Alzheimer\u2019s risk or depression.\nThe limitations in 1-5 together reduce the estimated effect size by 79% compared to making perfect edits in an embryo. If you could make those same edits in an embryo, the gains would top out around 30 IQ points.\nBut the bigger limitation originates from the size of the data set used to train our predictor. The more data used to train an intelligence predictor, the more of those 20,000 IQ-affecting variants we can identify, and the more certain we can be about exactly which variant among a cluster is actually causing the effect.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "And the more edits you can make, the better you can take advantage of that additional data. You can see this demonstrated pretty clearly in the graph below, where each line represents a differently sized training set.\nOur current predictors are trained using about 135,000 samples, which would place it just above the lowest line on the graph. There are existing databases right now such as the million veterans project with sample sizes of (you guessed it) one million. The expected gain from editing using a predictor trained on such a data set is shown by the orange line in the graph above.\nCompanies like 23&Me genotyped their\n12 millionth customer\ntwo years ago and could probably get at perhaps 3 million customers to take an\nIQ test\nor submit SAT scores. A predictor trained with that amount of data would perform about as well as the green line on the graph above.\nSo larger datasets could increase the effect of editing by as much as 13x! If we hold the number of edits performed constant at 2500, here\u2019s how the expected gain would vary as a function of training set size:\nNow we\u2019re talking! If someone made a training set with 3 million data points (a realistic possibility for companies like 23&Me), the IQ gain could plausibly be over 100 points (though keep in mind the uncertainties listed in the numbered list above). Even if we can only make 10% of that many edits, the expected effect size would still be over one standard deviation.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Lastly, just for fun, let\u2019s show the expected effects of 2500 edits if they were made in an embryo and we didn\u2019t have to worry about off-targets or limitations of editors (don\u2019t take this graph too seriously)\nI sincerely apologize for this terrible meme\nThe sample size issue could be solved pretty easily by any large government organization interested in collecting intelligence data on people they\u2019ve already genotyped. However, most of them are not currently interested.\nWhy has it been so hard to get larger sample sizes?\nThere is a taboo against direct investigation of the genetics of intelligence in academia and government funding agencies. As a result, we have collected a pitifully small amount of data about intelligence, and our predictors are much worse as a result.\nSo people interested in intelligence instead have to research this proxy trait that is correlated with intelligence called \u201ceducational attainment\u201d. Research into educational attainment IS allowed, so we have sample sizes of over 3 million genome samples with educational attainment labels and only about 130,000 for intelligence.\nGovernment agencies have shown they could solve the data problem if they feel so inclined. As an alternative, if someone had a lot of money they could simply buy a direct-to-consumer genetic testing company and either directly ask participants to take an IQ test or ask them what their SAT scores were.\nOr someone who wants to do this could just start their own biobank.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "How can you even make these edits anyways? Isn\u2019t CRISPR prone to errors?\nAnother of my initial doubts stemmed from my understanding of gene editing tools like CRISPR, and their tendency to edit the wrong target or even eliminate entire chromosomes.\nYou see, CRISPR was not designed to be a gene editing tool. It\u2019s part of a bacterial immune system we discovered all the way back in the late 80s and only fully understood in 2012. The fact that it has become such a versatile gene editing tool is a combination of luck and good engineering.\nThe gene editing part of the CRISPR system is the Cas9 protein, which works like a pair of scissors. It chops DNA in half at a specific location determined by this neat little attachment called a \u201cguide RNA\u201d. Cas9 uses the guide RNA like a mugshot, bumping around the cell\u2019s nucleus until it finds a particular three letter stretch of DNA called the PAM. Once it finds one, it\u00a0 unravels the DNA starting at the PAM and checks to see if the guide RNA forms a complementary match with the exposed DNA.\nThe guide RNA is the long squiggly thing with the turquoise \u201cspacer\u201d on the end of it\nCas9 can actually measure how well the guide RNA bonds to the DNA! If the hybridization is strong enough (meaning the base pairs on the guide RNA are complementary to those of the DNA), then the Cas9 \u201cscissors\u201d will cut the DNA a few base pairs to the right of the PAM.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "So to replace a sequence of DNA, one must introduce two cuts: one at the start of the DNA and one at the end of it.\nOnce Cas9 has cut away the part one wishes removed, a repair template must be introduced into the cell. This template is used by the cell\u2019s normal repair processes to fill in the gap.\nOr at least, that\u2019s how it\u2019s supposed to work. In reality a significant portion of the time the DNA will be stitched back together wrong or not at all, resulting in death of the chromosome.\nThis is not good news for the cell.\nSo although CRISPR had an immediate impact on the life sciences after it was introduced in 2012, it was rightfully treated with skepticism in the context of human genome editing, where deleting a chromosome from someone\u2019s genome could result in immediate cell death.\nHad nothing changed, I would not be writing this post. But in 2016, a new type of CRISPR was created.\nThe first base editor\nMany of the problems with CRISPR stem from the fact that it cuts BOTH strands of DNA when it makes an edit. A double stranded break is like a loud warning siren broadcast throughout the cell. It can result in a bunch of new base pairs being added or deleted at the break site. Sometimes it even results in the immune system ordering the cell to kill itself.\nSo when David Liu started thinking about how to modify CRISPR to make it a better gene editor, one of the first things he tried to do was prevent CRISPR from making double-stranded breaks.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "He and members of his lab came up with a very creative way of doing this. They modified the Cas9 scissors so that they would still seek out and bind to a matching DNA sequence, but wouldn\u2019t cut the DNA. They then attached a new enzyme to Cas9 called a \u201cdeaminase\u201d. Its job is to change a single base through a chemical reaction.\nThey called this new version of CRISPR a \u201cbase editor\u201d\nThere\u2019s a lot more detail to go into regarding how base editors work which I will explain in another post, but the most noteworthy trait of base editors is how reliable they are in comparison to old school Cas9.\nBase editors make the correct edit between\n60\nand\n180\ntimes as often as the\noriginal Cas9 CRISPR\n. They also make\n92% fewer\noff-target edits and 99.7% fewer insertion and deletion errors.\nThe most effective base editor I\u2019ve found so far is the\ntransformer base editor\n, which has an editing efficiency of 90% in T cells and no detected off-targets.\nBut despite these considerable strength, base editors still have two significant weaknesses:\nOf the 12 possible base pair swaps,\nthey can only target 6\n.\nWith some\nrare exceptions\nthey can usually only edit 1 base pair at a time.\nTo address these issues, a few members of the Liu lab began working on a new project: prime editors.\nPrime editors; the holy grail of gene editing technology?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Like base editors, prime editors only induce single stranded breaks in DNA, significantly reducing insertion and deletion errors relative to old school CRISPR systems. But unlike base editors, prime editors can change, insert, or delete up to about a hundred base pairs at a time.\nThe core insight leading to their creation was the discovery that the guide RNA could be extended to serve as a template for DNA modifications.\nA\nsimple illustration\nshowing how the prime editor guide RNA differs from that of a base editor\nTo greatly oversimplify, a prime editor cuts one of the DNA strands at the edit site and adds new bases to the end of it using an extension of the guide RNA as a template. This creates a bit of an awkward situation because now there are two loose flaps of DNA, neither of which are fully attached to the other strand.\nWhat the DNA looks like immediately after new bases are added using the extended guide RNA template.\nSource\nThankfully the repair enzyme in charge of fixing these types of problems (which are a necessary part of normal cellular function), really likes to cut the unedited flap for reasons I won\u2019t get into. Once it has cut away the other flap and stitched together the edited strand, the situation looks like this:\nWhat the DNA looks like after the 5' flap is cut away and the edited strand stitched back together.\nSource\nThere\u2019s still one problem left to fix though: the unedited strand is\u2026 well\u2026 unedited.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "To fix this, Liu and Co use another enzyme to cut the bottom strand of DNA near the edit site, which tricks the repair enzymes in charge of fixing mismatches into thinking that the unedited strand has been damaged and needs to be fixed so that it matches the edited one.\nThe final edited DNA strand.\nSource\nSo the cell\u2019s own repair enzymes actually complete the edit for us, which is pretty neat if you ask me.\nTogether, prime editors and base editors give us the power to edit around 90% of the genetic variants we\u2019re interested in with very high precision. Without them, editing genes in a living human body would be infeasible due to off-target changes.\nHow do you even get editors into brain cells in the first place?\nThere are roughly 200 billion cells in an average brain. You can\u2019t just use a needle to inject a bunch of gene editing tools into each one. So how do we get them inside?\nThere are several options available to us including viral proteins, but the delivery platform I like the most is called a lipid nanoparticle. Let me explain the idea.\nDual use technology\nRemember COVID vaccines? The first ones released used this new technology called \u201cmRNA\u201d. They worked a little differently from old school immunizations.\nThe goal of a vaccine is to expose your immune system to part of a virus so that it will recognize and kill it in the future.\nOne way to do this is by taking a little piece of the virus and injecting it into your body. This is how\nthe Novavax vaccine\nworked.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Another way is by sending an instruction to MAKE part of the virus to your body\u2019s own cells. This is how the Pfizer and Moderna vaccines worked. And the instruction they used was a little piece of ribonucleic acid called mRNA.\nmRNA is an instruction to your cells. If you can get it inside one, the cell\u2019s ribosomes will transcribe mRNA into a protein. It\u2019s a little bit like an executable file for biology.\nFrancis Crick's \"Central Dogma\" of biology.\nSource\nBy itself, mRNA is pretty delicate. The bloodstream is full of nuclease enzymes that will chop them into itty-bitty pieces.\nAlso, because mRNA basically has root level access to your cells, your body doesn\u2019t just shuttle it around and deliver it like the postal service. That would be a major security hazard.\nSo to get mRNA inside a cell, COVID vaccines stick them inside a delivery container to protect them from the hostile environment outside cells and to get them where they need to go. This container is a little bubble of fat called a lipid nanoparticle.\nIf you inject these nanoparticles into your muscle tissue, they will float around for a while before being absorbed by your muscle cells via a process called endocytosis.\nAfterwards, the mRNA is turned into a protein by the ribosomes in the muscle cell. In the case of COVID vaccines the protein is a piece of the virus called the \u201cspike\u201d that SARS-CoV-2 uses to enter your tissues.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Once created, some of these spike proteins will be chopped up by enzymes and displayed on the outside of each cell\u2019s membrane. Immune cells patrolling the area will eventually bind to these bits of the spike protein and alert the rest of the immune system, triggering inflammation. This is how vaccines teach your body to fight COVID.\nSo here\u2019s an idea:\nwhat if instead of making a spike protein, we deliver mRNA to make a gene editor?\nInstead of manufacturing part of a virus, we\u2019d manufacture a CRISPR-based editing tool like a prime editor. We could also ship the guide RNA in the same lipid nanoparticle, which would bond to the newly created editor proteins, allowing them to identify an editing target.\nA\nnuclear localization sequence\non the editor would allow the it to enter the cell\u2019s nucleus where the edit could be performed.\nSo you inject lipid nanoparticles into the brain?\nNot exactly. Brains are much more delicate than muscles, so we can\u2019t just copy the administration method from COVID vaccines and inject nanoparticles directly into the brain with a needle.\nWell actually we could try. And I plan to look into this during the mouse study phase of the project in case the negative effects are smaller than I imagine.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But apart from issues with possibly damaging brain or spinal tissue (which we could perhaps avoid), the biggest concern with directly injecting editors into the cerebrospinal fluid is distribution. We need to get the editor to all, or at least most cells in the brain or else the effect will be negligible.\nIf you just inject a needle into a brain, the edits are unlikely to be evenly distributed; most of the cells that get edited will be close to the injection site. So we probably need a better way to distribute lipid nanoparticles evenly throughout the brain.\nThankfully, the bloodstream has already solved this problem for us: it is very good at distributing resources like oxygen and nutrients evenly to all cells in the brain. So the plan is to inject a lipid-nanoparticle containing solution into the patient\u2019s veins, which will flow through the bloodstream and eventually to the brain.\nThere are two additional roadblocks: getting the nanoparticles past the liver and blood brain barrier.\nBy default, if you inject lipid nanoparticles into the bloodstream, most of them will get stuck in the liver. This is nice if you want to target a genetic disease in that particular organ (and indeed, there have been studies that did exactly that,\ntreating monogenic high cholesterol in macaques\n). But if you want to treat anything besides liver conditions then it\u2019s a problem.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "From my reading so far, it sounds like nanoparticles circulate through the whole bloodstream many times, and each time some percentage of them get stuck in the liver. So the main strategy scientists have deployed to avoid accumulation is to get them into the target organ or tissue as quickly as possible.\nHere\u2019s a paper\nin which the authors do exactly that for the brain. They attach a peptide called \u201cangiopep-2\u201d onto the outside of the nanoparticles, allowing them to bypass the blood brain barrier using the LRP1 receptor. This receptor is normally used by LDL cholesterol, which is too big to passively diffuse through the endothelial cells lining the brain. But the angiopep-2 molecule acts like a key, allowing the nanoparticles to use the same door.\nThe paper was specifically examining this for delivery of chemotherapy drugs to glial cells, so it\u2019s possible we\u2019d either need to attach additional targeting ligands to the surface of the nanoparticles to allow them to get inside neurons, or that we\u2019d need to use another ligand besides angiopep. But this seems like a problem that is solvable with more research.\nOr maybe you can just YOLO the nanoparticles directly into someone\u2019s brain with a needle. Who knows? This is why research will need to start with cell culture experiments and (if things go well) animal testing.\nWhat if administration via lipid nanoparticles doesn\u2019t work?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If lipid nanoparticles don\u2019t work, there are two alternative delivery vectors we can look into: adeno-associated viruses (AAVs) and engineered virus-like particles (eVLPs).\nDiagram of an adenovirus. The shell is made of a bunch of viral proteins called capsids. The good stuff is inside.\nAAVs have become the go-to tool for gene therapy, mostly because viruses are just very good at getting into cells. The reason we don\u2019t plan to use AAVs by default mostly comes down to issues with repeat dosing.\nAs one might expect, the immune system is not a big fan of viruses. So when you deliver DNA for a gene editor with an AAV, the viral proteins often trigger an adaptive immune response. This means that when you next try to deliver a payload with the same AAV, antibodies created during the first dose will bind to and destroy most of them.\nThere ARE potential ways around this, such as using a different version of an adenovirus with different epitopes which won\u2019t be detected by antibodies. And there is ongoing work at the moment to optimize the viral proteins to provoke less of an immune response. But by my understanding this is still quite difficult and you may not have many chances to redose.\nBut there are other viral delivery vectors\nEngineered virus-like particles, or eVLPs for short, are another way to get gene editors into cells. Like AAVs, they are constructed of various viral proteins and a payload.\nAn engineered virus-like particle",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But unlike AAVs, eVLPs usually contain proteins, not DNA. This is important because, once they enter a cell, proteins have a much shorter half-life than DNA. A piece of DNA can last for weeks or sometimes even months inside the nucleus. As long as it is around, enzymes will crank out new editor proteins using that DNA template.\nSince most edits are made shortly after the first editors are synthesized, creating new ones for weeks is pointless. At best they will do nothing and at worst they will edit other genes that we don\u2019t want changed. eVLPs fix this problem by delivering the editor proteins directly instead of instructions to make them.\nThey can also be customized to a surprising degree. The yellow-colored envelope glycoprotein shown in the diagram above can be changed to alter which cells take up the particles. So you can target them to the liver, or the lungs, or the brain, or any number of other tissues within the body.\nLike AAVs, eVLPs have issues with immunogenicity. This makes redosing a potential concern, though it\u2019s possible there are ways to ameliorate these risks.\nWould edits to the adult brain even do anything?\nApart from the technical issues with making edits, the biggest uncertainty in this proposal is how large of an effect we should expect from modifying genes in adults. All the genetic studies on which our understanding of genes is based assume genes differ FROM BIRTH.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We know that some genes are only active in the womb, or in childhood, which should make us very skeptical that editing them would have an effect.\nFor example,\nhere\u2019s a graph\nshowing the relative level of expression of DCX, a gene active in neuronal progenitor cells and migrating immature neurons.\nYou can see the expression levels peak in the second trimester before dropping by a factor of ~80 by age 27.\nWe would therefore expect that any changes to promoters or inhibitors of the DCX gene would have little or no effect were they to be made in adulthood.\nHowever, not every gene is like this. Here\u2019s another graph from\nthe same paper\nshowing the relative expression of groups of genes in the hippocampus:\nThe red, blue and yellow lines, representing the expression of a group of synapse, dendrite, and myelination genes respectively, climb steadily until shortly after birth, then plateau throughout the rest of adulthood. So we could reasonably expect any changes to these genes to have a much stronger effect relative to changes to DEX.\nThis is one way to develop a basic prior about which\ngenes are likely to have an effect if edited in adults: the size of the effect of a given gene at any given time is likely proportional to its level of expression\n. Genes that are only active during development probably won\u2019t have as large of an effect when edited.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The paper I linked above collected information about the entire exome, which means we should be able to use its data to form a prior on the odds that editing any given variant will have an effect, though as usual there are some complications with this approach which I address\nin the appendix\n.\nA systematic analysis of the levels of expressions of all genes throughout the lifespan is beyond the scope of this post, but beyond knowing some proteins reach peak levels in adulthood, here are a few more reasons why I still expect editing to have a large effect:\nWe know that about\n84% of all genes are expressed somewhere in the adult brain\n. And while almost none of the intelligence-affecting variants are in protein-coding regions (see the section below), they are almost certainly in regions that AFFECT protein expression, such as promoters and inhibitors. One would naturally expect that modifying such protein expression would have an effect on intelligence.\nGene therapy in adults ACTUALLY WORKS. We now have several examples of diseases, such as sickle cell anemia, being\ncompletely cured\nin adults through some form of gene therapy.\nWe have dozens of examples of traits and diseases whose state is modified by the level of expression of various proteins: Diabetes, Alzheimer\u2019s, autoimmune conditions, skin pigmentation, metabolism, and many others. Since gene editing is ultimately modifying the amount and timing of various proteins, we should expect it to have an effect.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We know the adult brain can change via forming new connections and changing their strengths. That\u2019s how people learn after all!\nWhat if your editor targets the wrong thing?\nThough base editors and prime editors have much lower rates of off-target editing than original CRISPR Cas9, off-target editing is still a concern.\nThere\u2019s a particularly dangerous kind of off-target edit called a frameshift mutation that we really want to avoid. A frameshift mutation happens when you delete (or insert) a number of base pairs that is not a multiple of 3 from a protein-forming region.\nBecause base pairs are converted into amino acids three at a time, inserting or deleting a letter changes the meaning of all the base pairs that come after the mutation site.\nThis pretty much always results in a broken protein.\nSo to minimize the consequences of off-target mutations, we\u2019d do better to target non-protein-coding regions. We already know quite well which regions of the genome are synthesized into proteins. And fortunately, it appears that\n98% of intelligence-affecting variants are located in NON-CODING regions.\nSo we should have plenty of variants to target.\nWhat about off-target edits in non-coding regions?\nMost of the insertions, deletions and disease-causing variants with small effects are in non-coding regions of DNA. This gives us some evidence that off-target edits made to these regions will not necessarily be catastrophic.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As long as we can avoid catastrophic off-target edits, a few off-targets will be ok so long as they are outweighed by edits with positive effects.\nPart of the challenge here is characterizing off-target edits and how they are affected by choice of edit target. If your guide RNA\u2019s binding domain ALMOST matches that of a really critical protein, you may want to think twice before making the edit. Whereas if it\u2019s not particularly close to any target, you probably don\u2019t need to worry as much.\nHowever, it\u2019s still unclear to me how big of a deal off-target edits and indels will be.\nIf you unintentionally insert a few base pairs into the promoter region of some protein, will the promoter just work a little less well, or will it break altogether? I\nwasn\u2019t able to find many answers in the research literature, so consider this a topic for future research.\nIt is worth pointing out that long-lived cells in the body such as neurons experience\na thousand or more random mutations\nby age 40 and that, with the exception of cancer, these mutations rarely seem to cause serious dysfunction.\nHow about mosaicism?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Another potential issue with producing a large number of edits to cells in the brain is cellular mosaicism. The determination of which cell gets which set of edits is somewhat random. Maybe some cells are a little too far away from a blood vessel. Maybe some just don\u2019t get enough of the editor. Regardless of the reason, it\u2019s likely different cells within the brain will receive different sets of edits. Will this cause problems?\nIt\u2019s possible it will. But as mentioned in the previous section, neurons in the adult brain already have significant genetic differences and most of people seem functional despite these variations. Neurons accumulate about\n20-40 single letter mutations per year\n, with the exact amount varying by brain region and by individual.\nBy age 45 or so, the average neuron in an adult brain has about 1500 random genetic mutations. And the exact set of mutations each neuron has is somewhat random.\nThere are likely to be some differences between mutations caused by gene editing and those caused by cellular metabolism, pollutants, and cosmic rays. In particular, those caused by gene editing are more likely to occur at or near functionally important regions.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Still, it should be at least somewhat comforting that neurons usually remain functional when they have a few thousand genetic differences. But it is possible my optimism is misplaced here and mosaicism will turn out to be a significant challenge for some reason I can\u2019t yet foresee. This is yet another reason to conduct animal testing.\nCan we make enough edits in a single cell to have a significant effect?\nMaking a large enough number of edits to have a significant effect on a phenotype of interest is one of the main challenges with this research protocol. I\u2019ve found fairly recent papers such as\nQichen & Xue\n, that show a reasonably large number of edits can be made in the same cell at the same time; 31 edits using base editors and 3 using prime editors. As with nearly all research papers in this field, they used HEK293T cells, which are unusually easy to transfect with editors. We can therefore treat this as an upper bound on the number of simultaneous edits that can be made with current techniques.\nHere\u2019s\nanother research paper\nfrom Ni et al. where the researchers used prime editors to make 8 simultaneous edits in regenerated wheat plants.\nI also know of a PHD student in George Church\u2019s lab that was able to make\nseveral thousand edits\nin the same cell at the same time by targeting a gene that has several thousand copies spread throughout the genome.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "So people are trying multiplex editing, but if anyone has tried to make hundreds of edits at a time to different genes, they don\u2019t seem to have published their results. This is what we need to be able to do for in-vivo multiplex editing to have a big impact on polygenic traits and diseases.\nHowever, we don\u2019t necessarily need to introduce all edits with a single injection. If the delivery vector and the editing process doesn\u2019t provoke an adaptive immune response, or if immunosuppressants can be safely used many times, we can just do multiple rounds of administration; 100 edits during the first round, another 100 in the second round and so on.\nIf there is an immune response, there are some plausible techniques to avoid adverse reactions during subsequent doses, such as using alternative versions of CRISPR with a different antigenic profile. There are a limited number of these alternative CRISPRs, so this technique is not a general purpose solution.\nMaking hundreds of edits in cultured cells is the first thing I\u2019d like to focus on. I\u2019ve laid out my plans in more detail in \u201c\nHow do we make this real\n?\u201d\nWhy hasn\u2019t someone already done this?\nThere are hundreds of billions of dollars to be made if you can develop a treatment for brain disorders like Alzheimers, dementia and depression. There\u2019s probably trillions to be made if you can enhance human intelligence.\nAnd yet the people who have developed these tools seem to only be using them to treat rare monogenic disorders. Why?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Part of the answer is that \u201cpeople are looking into it, but in a pretty limited capacity.\u201d For example,\nhere\u2019s a paper\nthat looked at multiplex editing of immune cells in the context of immunotherapy for cancer. And\nhere\u2019s a post\nthat is strikingly similar to many of the ideas I\u2019ve raised here. I only learned of its existence a week before publishing, but have spoken with the authors who had similar ideas about the challenges in the context of their focus on gene therapy for polygenic disease.\nApart from that I can\u2019t find anything. There are 17 clinical trials involving some kind of CRISPR-based gene editing and I can\u2019t find a single one that targets multiple genes.\nSo here I\u2019m going to speculate on a couple of reasons why there seems to be so little interest in in-vivo multiplex editing.\nBlack box biology\nTo believe multiplex editing can work you have to believe in the power of\nblack box biology\n. Unlike monogenic diseases, we don\u2019t always understand the mechanism through which genetic variants involved in diabetes or intelligence cause their effects.\nYou have to believe that it is possible to affect a trait by changing genes whose effect you understand but whose mechanism of action you don\u2019t.\nAnd what\u2019s more, you need to believe you can change hundreds of genes in a large number of cells in this manner.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "To most people, this approach sounds crazy, unpredictable, and dangerous. But the fact is nature mixes and matches combinations of disease affecting alleles all the time and it works out fine in most cases. There are additional challenges with editing such as off-targets and editing efficiency, but these are potentially surmountable issues.\nVague association with eugenics make some academics shy away\nA lot of people in Academia are simply very uncomfortable with the idea that genes can influence outcomes in any context other than extremely simple areas like monogenic diseases. I recently attended a conference on polygenic embryo screening in Boston and the amount of denialism about the role of genetics in things as simple as disease outcomes was truly a sight to behold.\nIn one particularly memorable speech, Erik Turkheimer, a behavioral geneticist from University of Virginia claimed that, contrary to the findings of virtually every published study, adding more data to the training set for genetic predictors simply made them perform worse. As evidence to support this conclusion, he cited a university study on alcohol abuse with a sample size of 7000 that found no associations of genome-wide significance.\nTurkheimer neglected to mention that no one could reasonably expect the study to unearth such associations because it was too underpowered to detect all but the most extreme effects. Most studies in this field have 30-100x the number of participants.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In many cases, arguing against these unsupported viewpoints is almost impossible. Genomic Prediction has published about a dozen papers on polygenic embryo selection, many of which are extremely well written and contain novel findings with significant clinical relevance. Virtually no one pays attention. Even scientists writing about the exact same topics\nfail\nto cite\ntheir work\n.\nMany of them simply do not care what is true. They view themselves as warriors in a holy battle, and they already know anyone talking about genetic improvement is an evil racist eugenicist.\nYou can convince an open-minded person in a one-on-one discussion because the benefits are so gigantic and the goal is consistent with widely shared values as expressed in hundreds of other contexts.\nBut a large portion of Academics will just hear someone say \u201cgenes matter and could be improved\u201d, think \u201cisn\u2019t that what Hitler did?\u201d, and become forever unreachable. Even if you explain to them that no, Hitler\u2019s theory of genetics was based on a corrupted version of purity ethics and that in fact a lot of his programs were probably dysgenic, and that even if they weren\u2019t we would never do that because it\u2019s evil, it\u2019s too late. They have already passed behind the veil of ignorance. The fight was lost the moment the association with Hitler entered their mind.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Though these attitudes don\u2019t completely prevent work on gene therapies, I think it likely that the hostility towards altering outcomes through genetic means drives otherwise qualified researchers away from this line of work.\nMaybe there\u2019s some kind of technical roadblock I\u2019m missing\nI\u2019ve now talked to some pretty well-qualified bio PHDs with expertise in stem cells, gene therapy, and genetics. While many of them were skeptical, none of them could point to any part of the proposed treatment process that definitely won\u2019t work. The two areas they were most skeptical of was the ability to deliver an editing vector to the brain and the ability to perform many edits in the same cell. But both seem addressable. Delivery for one, may be solved by other scientists trying to deliver gene editors to the brain to target monogenic conditions like Huntington\u2019s.\nThere are unknowns of course, and many challenges which could individually or in conjunction make the project infeasible. For example either of the challenges listed above, or perhaps mosaicism is a bigger deal than I expect it to be.\nBut I\u2019ve only talked to a couple of people. Maybe someone I haven\u2019t spoken with knows of a hard blocker I am unaware of.\nPeople are busy picking lower hanging fruit",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It\u2019s a lot easier to target a specific gene that causes a particular disease than it is to target hundreds or thousands of genes which contribute to disease risk. Given prime editors were only created four years ago and base editors only seven,\nmaybe the people with the relevant expertise are just busy picking lower hanging fruit which has a much better chance of resulting in a money-making therapy\n. The lowest hanging fruit is curing monogenic diseases, and that\u2019s currently where all the focus is.\nThe market is erring\nSome types of markets are highly efficient. Commodities and stocks, for example, are very efficient because a large number of people have access to the most relevant information and arbitrage can happen extremely quickly.\nBut the more time I\u2019ve spent in private business, and startups in particular, the more I\u2019ve realized that most industries are not as efficient as the stock market. They are far less open, the knowledge is more concentrated, the entry costs are higher, and there just aren\u2019t that many people with the abilities needed to make the markets more efficient.\nIn the case of multiplex editing, there are many factors at play which one would reasonably expect to result in a less efficient market:\nThe idea of polygenic editing is not intuitive to most people, even some geneticists. One must either hear it or read about it online.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "One must understand polygenic scores and ignore the reams of (mostly) politically motivated articles attempting to show they aren\u2019t real or aren\u2019t predictive of outcomes.\nOne must understand gene editing tools and delivery vectors well enough to know what is technically possible.\nOne must believe that the known technical roadblocks can be overcome.\nOne must be willing to take a gamble that known unknowns and unknown unknowns will not make this impossible (though they may do so)\nHow many people in the world satisfy all five of those criteria? My guess is not many. So despite how absurd it sounds to say that the pharmaceutical industry is overlooking a potential trillion dollar opportunity, I think there is a real possibility they are. Base editors have been around for seven years and in that time the number of papers published on multiplex editing in human cells for anything besides cancer treatment has remained in the single digits.\nI don\u2019t think this is a case of \u201cjust wait for the market to do it\u201d.\nCan we accelerate development of this technology?\nAfter all that exposition, let me summarize where the work I think will be done by the market and that which I expect them to ignore.\nProblems I expect the market to work on\nTargeting editors towards specific organs",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". There are many monogenic conditions that primarily affect the brain, such as Huntington\u2019s disease or monogenic Alzheimer\u2019s. Companies are already trying to treat monogenic conditions, so I think there\u2019s a decent chance they could solve the issues of efficiently delivering gene editors to brain cells.\nProblems the market might solve\nAttenuating the adaptive immune response to editor proteins and delivery vectors.\nIf you are targeting a monogenic disease, you only need to deliver an editing vector once. The adaptive immune response to the delivery vector and CRISPR proteins is therefore of limited concern; the innate immune response to these is much more important.\nThere may be overlap between avoiding each of these, but I expect that if an adaptive immune response is triggered by monogenic gene therapy and it doesn\u2019t cause anaphylaxis, companies will not concern themselves with avoiding it.\nAvoiding off-target edits in a multiplex editing context.\nThe odds of making an off-target edit scale exponentially as the number of edits increase. Rare off-targets are therefore a much smaller concern for monogenic gene therapy, and not much of a concern at all for immunotherapy unless they cause cancer or are common enough to kill off a problematic number of cells.\nI therefore don\u2019t expect companies or research labs to address this problem at the scale it needs to be addressed within the multiplex editing context.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "One possible exception are companies using gene editing in the agricultural space, but the life of a wheat plant or other crop has virtually zero value, so the consequences of off-targets are far less serious.\nProblems I don\u2019t expect the market to work on\nCreating better intelligence predictors.\nUnless\nembryo selection for intelligence\ntakes off, I think it is unlikely any of the groups that currently have the data to create strong intelligence predictors (or could easily obtain it) will do so.\nConducting massive multiplex editing of human cells\nIf you don\u2019t intend to target polygenic disease, there\u2019s little motivation to make multiple genetic changes in cells. And as explained above, I don\u2019t expect many if any people to work on polygenic disease risk because mainstream scientists are very skeptical of embryo selection and of the black box model of biology.\nDetermining how effective edits are when conducted in adult animals\nNone of the monogenic diseases targeted by current therapies involve genes that are only active during childhood. It therefore seems fairly unlikely that anyone will do research into this topic since it has no clear clinical relevance.\nHow do we make this real?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The hardest and most important question in this post is how we make this technology real. It\u2019s one thing to write a LessWrong post pointing out some revolutionary new technology might be possible. It\u2019s quite another to run human trials for a potentially quite risky medical intervention that doesn\u2019t even target a recognized disease.\nFortunately, some basic work is already being done. The first clinical trials of prime editors are\nlikely to start in 2024\n. And there are at least\nfour clinical trials\nunderway right now testing the efficacy of base editors designed to target monogenic diseases. The\nVERVE-101\ntrial is particularly interesting due to its use of mRNA delivered via lipid nanoparticles to target a genetic mutation that causes high cholesterol and cardiovascular disease.\nIf this trial succeeds, it will definitively demonstrate that in-vivo gene editing with lipid nanoparticles and base editors can work. The remaining obstacles at that point will be editing efficiency and getting the nanoparticles to the brain (a solution to which I\u2019ve proposed in the section titled \u201c\nSo you inject lipid nanoparticles into the brain?\n\u201d. Here\u2019s an outline of what I think needs to be done, in order:\nDetermine if it is possible to perform a large number of edits in cell culture with reasonable editing efficiency and low rates of off-target edits.\nRun trials in mice. Try out different delivery vectors. See if you can get any of them to work on an actual animal.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Run trials in cows. We have good polygenic scores for cow traits like milk production, so we can test whether or not polygenic modifications in adult animals can change trait expression.\n(Possibly in parallel with cows) run trials on chimpanzees\nThe goal of such trials would be to test our hypotheses about mosaicism, cancer, and the relative effect of genes in adulthood vs childhood.\nRun trials in humans on a polygenic brain disease. See if we can make a treatment for depression, Alzheimer\u2019s, or another serious brain condition.\nIf the above passes a basic safety test (i.e. no one in the treatment group is dying or getting seriously ill), begin trials for intelligence enhancement.\nIt\u2019s possible one or more steps could be skipped, but at the very least you need to run trials in some kind of model organism to refine the protocol and ensure the treatment is safe enough to begin human trials.\nMy current plan\nI don\u2019t have a formal background in biology. And though I learn fairly quickly and have great resources like SciHub and GPT4, I am not going to be capable of running a lab on my own any time soon.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "So at the moment I am looking for a cofounder who knows how to run a basic research lab. It is obviously helpful if you have worked specifically with base or prime editors before but what I care most about is finding someone smart that I get along with who will persevere through the inevitable pains of creating a startup. If anyone reading this is interested in joining me and has such a background,\nsend me an email\n.\nI\u2019m hoping to obtain funding for one year worth of research, during which our main goal will be to show we can perform a large number of distinct edits in cell culture. If we are successful in a year the odds of us being able to modify polygenic brain based traits will be dramatically increased.\nAt that point we would look to either raise capital from investors (if there is sufficient interest), or seek a larger amount of grant funding. I\u2019ve outlined a brief version of additional steps as well as a rough estimate of costs below.\nShort-term logistics concerns\nA rough outline of my research agenda\nFind a cofounder/research partner\nDesign custom pegRNAs for use with prime editors\nPay a company to synthesize some plasmids containing those pegRNAs with appropriate promoters incorporated\nRent out lab space. There are many orgs that do this such as BadAss labs, whose facilities I\u2019ve toured.\nObtain other consumables not supplied by the lab space like HEK cell lines, NGS sequencing materials etc",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Do some very basic validation tests to make sure we can do editing, such as inserting a fluorescent protein into an HEK cell (if the test is successful, the cell will glow)\nStart trying to do prime editing. Assess the relationship between number of edits attempted and editing efficiency.\nTry out a bunch of different things to increase editing efficiency and total edits.\nTry multiple editing rounds. See how many total rounds we can do without killing off cells.\nIf the above problems can be solved, seek a larger grant and/or fundraise to test delivery vectors in mice.\nRough estimate of costs\nCost\nPurpose\nYearly Cost\nAdditional Notes\nBasic Lab Space\nAccess to equipment, lab space, bulk discounts\n~66000\nAvailable from\nBadAss Labs\n. Includes workbench, our own personal CO2 incubator, cold storage, a cubicle, access to all the machines we need to do cell culturing, and the facilities to do animal testing at a later date if cell culture tests go well\nWorkers comp & liability coverage\nRequired by labs we rent space from\n~12,000\nAssuming workers comp is 2% of salaries, 1000 per year for general liability insurance and 3000 per year for professional liability insurance\nCustom plasmids\ngRNA synthesis\n15,000?\nI\u2019ve talked to Azenta about the pricing of these. They said they would be able to synthesize the DNA for 7000 custom guide RNAs for about $5000. We probably won\u2019t need that many, but we\u2019ll also likely go through a few iterations with the custom guide RNAs.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "HEK cell lines & culture media\n2000\nAvailable from\nangioproteomie\nmRNA for base editors\nCreating base editors in cells\n5000?\nThe cost per custom mRNA is around $300-400 but I assume we'll want to try out many variants\nmRNA for prime editors\nCreating prime editors in cell cultures\n5000?\nThe cost per custom mRNA is around $300-400 but I assume we'll want to try out many variants\nFlasks or plates, media, serums, reagents for cell growth, LNP materials etc\nRoutine lab activity like culturing cells, making lipid nanoparticles and a host of other things\n20,000?\nThis is a very very rough estimate. I will have a clearer idea of these costs when I find a collaborator with whom I can plan out a precise set of experiments.\nNGS sequencing\nDetermining whether the edits we tried to make actually worked\n50,000?\nOther things I don't know about\n200,000?\nThis is a very rough estimate meant to cover lab supplies and services. I will probably be able to form a better estimate when I find a cofounder and open an account with BadAss labs (which will allow me to see their pricing for things)\nTravel for academic conferences, networking\nI think it will likely be beneficial for one or more of us to attend a few conferences on this topic to network with potential donors.\n20,000?\nSalaries\nKeeping us off the streets\n400,000\nThere will likely be three of us working on this full time\nGrand Total Estimate\n~795,000\nConclusion",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Someone needs to work on massive multiplex in-vivo gene editing. This is one of the few game-board-flipping technologies that could have a significant positive impact on alignment prior to the arrival of AGI. If we completely stop AI development following some pre-AGI disaster, genetic engineering by itself could result in a future of incredible prosperity with far less suffering than exists in the world today.\nI also think it is very unlikely that this will just \u201chappen anyways\u201d without any of us making an active effort. Base editors have been around since 2016. In that time, anyone could have made a serious proposal and started work on multiplex editing. Instead we have like two academic papers and one blog post.\nIn the grand scheme of things, it would not take that much money to start preliminary research. We could probably fund myself and my coauthor, a lab director, and lab supplies to do research for a year for under a million dollars. If you\u2019re interested in funding this work and want to see a more detailed breakdown of costs than what I\u2019ve provided above, please send me an\nemail\n.\nI don\u2019t know exactly how long it will take to get a working therapy, but I\u2019d give us a 10% chance of having something ready for human testing within five years. Maybe sooner if we get more money and things go well.\nDo I think this is more important than alignment work?\nNo.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I continue to think the most likely way to solve alignment is for a bunch of smart people to work directly on the problem. And who knows, maybe Quintin Pope and Nora Belrose are correct and we live in an alignment by default universe where the main issue is making sure power doesn\u2019t become too centralized in a post-AGI world.\nBut I think it\u2019s going to be a lot more likely we make it to that positive future if we have smarter, more focused people around. If you can help me make this happen through your expertise or funding, please send me a DM or\nemail me\n.\nAppendix (Warning: boredom ahead)\nAdditional headaches not discussed\nRegarding editing\nBystander edits\nIf you have a sequence like \u201cAAAAA\u201d and you\u2019re trying to change it to \u201cAAGAA\u201d, a base editor will pretty much always edit it to \u201cGGGGG\u201d. This is called \u201cbystander editing\u201d and will be a source of future headaches. It happens because the deaminase enzyme is not precise enough to only target exactly the base pair you are trying to target; there\u2019s a window of bases it will target. When I\u2019ve written some software to identify which genetic variants we want to target I will be able to give a better estimate of how many edits we can target with base editors without inducing bystander edits. There are papers that have reduced the frequency of bystander edits, such as\nGehrke et al\n, but it\u2019s unclear to me at the moment whether the improvement is sufficient to make base editors usable on such sequences.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Chemical differences of base editor products\nTraditional base editors change cytosine to uracil and adenine to inosine. Many enzymes and processes within the cell treat uracil the same as thymine and inosine the same as guanine. This is good! It\u2019s why base editors work in the first place. But the substituted base pairs are not chemically identical.\nThey are less chemically stable over time.\nThere may be some differences in how they are treated by enzymes that affect protein synthesis or binding affinity.\nIt\u2019s not clear to me at this moment what the implications of this substitution are. Given there are four clinical trials underway right now which use base editors as gene therapies, I don\u2019t think they\u2019re a showstopper; at most, I think they would simply reduce editing efficiency. But these chemical differences are something we will need to explore more during the research process.\nEditing silenced DNA\nThere is mixed information online about the ability of base and prime editors to change DNA that is methylated or wrapped around histones.\nSome articles\nshow that you can do this with modified base editors, but imply normal base editors have limited ability to edit such DNA. Also the ability seems to depend on the type of chromatin and the specific editor.\nAvoiding off-target edits",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Base editors (and to a lesser extent prime\u00a0 editors) have issues with off-target edits. The rate of off-targets depends on a variety of factors, but one of the most important is the similarity between the sequence of the binding site and that of other sites in the genome. If the binding site is 20 bases long, but there are several other sequences in the genome that only differ by one letter, there\u2019s a decent chance the editor will bind to one of those other sequences instead.\nBut quantifying the relationship between the binding sequence and the off-target edit rate is something we still need to do for this project. We can start with models such as\nDeepoff\n,\nDeepCRISPR\n,\nAzimuth/Elevation\n, and\nDeepPrime\nto select editing targets (credit to\nthis post\nfor bringing these to my attention), then validate the results and possibly make adjustments after performing experiments.\nRegarding fine-mapping of causal variants\nThere\u2019s an annoying issue with current data on intelligence-affecting variants that reduces the odds of us editing the correct variant. As of this writing, virtually all of the studies examining the genetic roots of intelligence used SNP arrays as their data source. SNP arrays are a way of simplifying the genome sequencing process by focusing on genes that commonly vary between people. For example, the SNP array used by UK Biobank measures all genetic regions where at least ~1% of the population differs.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This means the variants that are implicated as causing an observed intelligence difference will always come from that set, even if the true causal variant is not included in the SNP array. This is annoying, because it sometimes means we\u2019ll misidentify which gene is causing an observed difference in intelligence.\nFortunately there\u2019s a relatively straightforward solution: in the short run we can use some of the recently released whole genome sequences (such as the 500k that were recently published by UK biobank) to adjust our estimates of any given variant causing the observed change. In the long run, the best solution is to just retrain predictors using whole genome data.\nRegarding differential gene expression throughout the lifespan\nThanks to\nKang et al.\nwe have data on the level of expression of all proteins in the brain throughout the lifespan. But for reasons stated in \u201c\nWhat if your editor targets the wrong thing\n\u201d, we\u2019re planning to make pretty much all edits to non-protein coding regions. We haven\u2019t yet developed a mechanistic plan for linking non-coding regions with the proteins whose expression they affect. There are some cases where this should be easy, such as tying a promoter (which is always directly upstream of the exonic region) to a protein. But in other cases such as repressor genes which aren\u2019t necessarily close to the affected gene, this may be more difficult.\nRegarding immune response",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I am not currently aware of any way to ensure that ONLY brain cells receive lipid nanoparticles. It\u2019s likely they will get taken up by other cells in the body, and some of those cells will express editing proteins and receive edits. These edits probably won\u2019t do much if we are targeting genes involved in brain-based traits, but non-brain cells do this thing called \u201cantigen presentation\u201d where they chop up proteins inside of themselves and present them on their surfaces.\nIt\u2019s plausible this could trigger an adaptive immune response leading to the body creating antibodies to CRIPSR proteins. This wouldn\u2019t necessarily be bad for the brain, but it could mean that subsequent dosings would lead to inflammation in non-brain regions. We could potentially treat this with an immunosupressant, but then the main advantage of lipid nanoparticles being immunologically inert kind of goes away.\nRegarding brain size\nSome of the genes that increase intelligence do so by enlarging the brain. Brain growth stops in adulthood, so it seems likely that any changes to genes involved in determining brain size would simply have no effect in adults. But it has been surprisingly hard to find a definitive answer to this question.\nIf editing genes involved in brain growth actually restarted brain growth in adulthood AFTER the fusion of growth plates in the cranium, it would cause problems.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If such a problem did occur we could potentially address it by Identifying a set of genes associated with intelligence and not brain size and exclusively edit those. We have GWAS for brain size such as\nthis one\n, though we would need to significantly increase the data set size to do this.\nBut this is a topic we plan to study in more depth as part of our research agenda because it is a potential concern.\nEditing induced amnesia?\nGene editing in the brain will change the levels and timings of various proteins. This in turn will change the way the brain learns and processes information. This is highly speculative, but maybe this somehow disrupts old memories in a way similar to amnesia.\nI\u2019m not very concerned about this because the brain is an incredibly adaptable tissue and we have hundreds of examples of people recovering functionality after literally losing large parts of their brain due to stroke or trauma. Any therapeutic editing process that doesn\u2019t induce cancer or break critical proteins is going to be far, far less disruptive than traumatic brain injuries. But it\u2019s possible there might be a postoperative recovery period as people\u2019s brains adjust to their newfound abilities.\nAn easy way to de-risk this (in addition to animal trials) is by just doing a small number of edits during preliminary trials of intelligence enhancement.\nModel assumptions\nThe model for gain as a function of number of edits was created by Kman with a little bit of feedback from GeneSmith.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It makes a few assumptions:\nWe assume there are 20,000 IQ-affecting variants with an allele frequency of >1%. This seemed like a reasonable estimation to me based on a conversation I had with an expert in the field, though there are papers that put the actual number anywhere between\n10,000\nand\n24,000\n.\nWe use an allele frequency distribution given by the \u201cneutral and nearly neutral mutation\u201d model, as explained in\nthis paper\n. There is evidence of\ncurrent negative selection\nfor intelligence in developed countries and evidence of\npast positive selection for higher-class traits\n(of which intelligence is almost certainly one), the model is fairly robust to effects of this magnitude.\nWe assume 10 million variants with minor allele frequency >1% (this comes directly from UK Biobank summary statistics, so we aren\u2019t really assuming much here)\nWe assume SNP heritability of intelligence of\n0.25\n. I think it is quite likely SNP heritability is somewhat higher, so if anything our model is conservative along this dimension\nThe effect size distribution is directly created using the assumption above. We assume a normal distribution of effect sizes with a mean of zero and variance set to match the estimated SNP heritability. Ideally our effect size distribution would more closely match empirical data, but this sort of effect size distribution is commonly used by other models in the field, and since it was easier to work with we used it too. But consider this an area for future improvement.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We use a simplified LD model with 100 closely correlated genetic variants. The main variable that matters here is the correlation between the MOST closely correlated SNP and the one causing the effect. We eyeballed some LD data to create a\u00a0 uniform distribution from which to draw the most tightly linked variant. So for example, the most tightly correlated SNP might be a random variable whose correlation is somewhere between .95 and 1. All other correlated variants are decrements of .01 less than that most correlated variant. So if the most tightly correlated variant has a correlation of .975, then the next would be correlated at .965, then .955, .945 etc for 100 variants.\nThis is obviously not a very realistic model, but it probably produces fairly realistic results. But again, this is an area for future improvement.\nEditor properties\nHere\u2019s a table Kman and I have compiled showing a good fraction of all the base and prime editors we\u2019ve discovered with various attributes.\nEditor\nEdits that can be performed\nOn-target efficiency in HEK293T cells\nOff-target activity in HEK293T cells\nOn-target edit to indel ratio\nActivity window width\nABE7.10\nA:T to G:C\n~50%\n<1%\n>500\n~3 bp\nABEmax\n~80%\n?\n~50\n?\nBE2\nG:C to A:T\n~10%\n?\n>1000\n~5 bp\nBE3\n~30%\n?\n~23\nBE4max\n~60%\n?\n?\nTadCBE\nC:G to TA\n~57%\n0.38-1.1%\n>83\n?\nCGBE1\nG:C to C:G\n~15%\n?\n~5\n~3 bp\nATBE\nA:T to T:A\n?\n?\n?\n?\nACBE-Q\nA:T to C:G\n?\n?\n?\n?\nPE3\nShort (~50 bp) insertions, deletions, and substitutions\n~24%\n<1%\n~4.5\nN/A\nPE3b\n41%\n~52\nPE4max\n~20%",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "~80\nPE5max\n~36%\n~16\nCas9 + HDR\nInsertions, deletions, substitutions\n~0.5%\n~14%\n0.17\nNote that these aren\u2019t exactly apples to apples comparisons since editing outcomes vary a lot by edit site, and different sites were tested in different studies (necessarily, since not all editor variants can target the same sites)\nEspecially hard to report off-target rates since they\u2019re so sequence dependent\nOn-target editing is also surprisingly site-dependent\nSomething that didn\u2019t make the table: unintended on-target conversions for base editors\nE.g. CBE converting G to C instead of G to A\nSome of the papers didn\u2019t report averages, so I eyeballed averages from the figures\nIf you\u2019ve made it this far send me a DM. I want to know what motivates people to go through all of this.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I expect \"slow takeoff,\" which we could operationalize as the economy doubling over some 4 year interval before it doubles over any 1 year interval. Lots of people in the AI safety community have strongly opposing views, and it seems like a really important and intriguing disagreement. I feel like I don't really understand the fast takeoff view.\n(Below is a short post copied from Facebook. The link contains a more substantive discussion. See also:\nAI impacts on the same topic\n.)\nI believe that the disagreement is mostly about what happens\nbefore\nwe build powerful AGI. I think that weaker AI systems will already have radically transformed the world, while I believe fast takeoff proponents think there are factors that makes weak AI systems radically less useful. This is strategically relevant because I'm imagining AGI strategies playing out in a world where everything is already going crazy, while other people are imagining AGI strategies playing out in a world that looks kind of like 2018 except that someone is about to get a decisive strategic advantage.\nHere is my current take on the state of the argument:\nThe basic case for slow takeoff is: \"it's easier to build a crappier version of something\" + \"a crappier AGI would have almost as big an impact.\" This basic argument seems to have a great historical track record, with nuclear weapons the biggest exception.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "On the other side there are a bunch of arguments for fast takeoff, explaining why the case for slow takeoff doesn't work. If those arguments were anywhere near as strong as the arguments for \"nukes will be discontinuous\" I'd be pretty persuaded, but I don't yet find any of them convincing.\nI think the best argument is the historical analogy to humans vs. chimps. If the \"crappier AGI\" was like a chimp, then it wouldn't be very useful and we'd probably see a fast takeoff. I think this is a weak analogy, because the discontinuous progress during evolution occurred on a metric that evolution wasn't really optimizing: groups of humans can radically outcompete groups of chimps, but (a) that's almost a flukey side-effect of the individual benefits that evolution is actually selecting on, (b) because evolution optimizes myopically, it doesn't bother to optimize chimps for things like \"ability to make scientific progress\" even if in fact that would ultimately improve chimp fitness. When we build AGI we will be optimizing the chimp-equivalent-AI for usefulness, and it will look nothing like an actual chimp (in fact it would almost certainly be enough to get a decisive strategic advantage if introduced to the world of 2018).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In the linked post I discuss a bunch of other arguments: people won't be trying to build AGI (I don't believe it), AGI depends on some secret sauce (why?), AGI will improve radically after crossing some universality threshold (I think we'll cross it way before AGI is transformative), understanding is inherently discontinuous (why?), AGI will be much faster to deploy than AI (but a crappier AGI will have an intermediate deployment time), AGI will recursively improve itself (but the crappier AGI will recursively improve itself more slowly), and scaling up a trained model will introduce a discontinuity (but before that someone will train a crappier model).\nI think that I don't yet understand the core arguments/intuitions for fast takeoff, and in particular I suspect that they aren't on my list or aren't articulated correctly. I am very interested in getting a clearer understanding of the arguments or intuitions in favor of fast takeoff, and of where the relevant intuitions come from / why we should trust them.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The generalized efficient markets (GEM) principle says, roughly, that things which would give you a big windfall of money and/or status, will not be easy. If such an opportunity were available, someone else would have already taken it. You will never find a $100 bill on the floor of Grand Central Station at rush hour, because someone would have picked it up already.\nOne way to circumvent GEM is to be the best in the world at some relevant skill. A superhuman with hawk-like eyesight and the speed of the Flash might very well be able to snag $100 bills off the floor of Grand Central. More realistically, even though financial markets are the ur-example of efficiency, a handful of firms do make impressive amounts of money by being faster than anyone else in their market. I\u2019m unlikely to ever find a proof of the Riemann Hypothesis, but Terry Tao might. Etc.\nBut being the best in the world, in a sense sufficient to circumvent GEM, is not as hard as it might seem at first glance (though that doesn\u2019t exactly make it easy). The trick is to exploit dimensionality.\nConsider: becoming one of the world\u2019s top experts in proteomics is hard. Becoming one of the world\u2019s top experts in macroeconomic modelling is hard. But how hard is it to become sufficiently expert in proteomics and macroeconomic modelling that nobody is better than you at both simultaneously?\nIn other words, how hard is it to reach the Pareto frontier?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Having reached that Pareto frontier, you will have circumvented the GEM: you will be the single best-qualified person in the world for (some) problems which apply macroeconomic modelling to proteomic data. You will have a realistic shot at a big money/status windfall, with relatively little effort.\n(Obviously we\u2019re oversimplifying a lot by putting things like \u201cmacroeconomic modelling skill\u201d on a single axis, and breaking it out onto multiple axes would strengthen the main point of this post. On the other hand, it would complicate the explanation; I\u2019m keeping it simple for now.)\nLet\u2019s dig into a few details of this approach\u2026\nElbow Room\nThere are many table tennis players, but only one best player in the world. This is a side effect of ranking people on one dimension: there\u2019s only going to be one point furthest to the right (absent a tie).\nPareto optimality pushes us into more dimensions. There\u2019s only one best table tennis player, and only one best 100-meter sprinter, but there can be an unlimited number of Pareto-optimal table tennis/sprinters.\nProblem is, for GEM purposes, elbow room matters. Maybe I\u2019m the on the pareto frontier of Bayesian statistics and gerontology, but if there\u2019s one person just little bit better at statistics and worse at gerontology than me, and another person just a little bit better at gerontology and worse at statistics, then GEM only gives me the advantage over a tiny little chunk of the skill-space.\nThis brings up another aspect\u2026\nProblem Density",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Claiming a spot on a Pareto frontier gives you some chunk of the skill-space to call your own. But that\u2019s only useful to the extent that your territory contains useful problems.\nTwo pieces factor in here. First, how large a territory can you claim? This is about elbow room, as in the diagram above. Second, what\u2019s the density of useful problems within this region of skill-space? The table tennis/sprinting space doesn\u2019t have a whole lot going on. Statistics and gerontology sounds more promising. Cryptography and monetary economics is probably a particularly rich Pareto frontier these days. (And of course, we don\u2019t need to stop at two dimensions - but we\u2019re going to stop there in this post in order to keep things simple.)\nDimensionality\nOne problem with this whole GEM-vs-Pareto concept: if chasing a Pareto frontier makes it easier to circumvent GEM and gain a big windfall, then why doesn\u2019t everyone chase a Pareto frontier? Apply GEM to the entire system: why haven\u2019t people already picked up the opportunities lying on all these Pareto frontiers?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Answer: dimensionality. If there\u2019s 100 different specialties, then there\u2019s only 100 people who are the best within their specialty. But there\u2019s 10k pairs of specialties (e.g. statistics/gerontology), 1M triples (e.g. statistics/gerontology/macroeconomics), and something like 10^30 combinations of specialties. And each of those pareto frontiers has room for more than one person, even allowing for elbow room. Even if only a small fraction of those combinations are useful, there\u2019s still a\nlot\nof space to stake out a territory.\nAnd to a large extent, people do pursue those frontiers. It\u2019s no secret that an academic can easily find fertile fields by working with someone in a different department. \u201cInterdisciplinary\u201d work has a reputation for being unusually high-yield.\nSimilarly, carrying scientific work from lab to market has a reputation for high yields. Thanks to the \u201ccurse\u201d of dimensionality, these goldmines are not in any danger of exhausting.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is a linkpost for a model and web tool (that I and several friends created) to quantitatively estimate the COVID risk to you from your ordinary daily activities:\nThis website contains three outputs of our work:\na web\ncalculator\nthat you can use to calculate your COVID risk (in units of microCOVIDs, a 1-in-a-million chance of getting COVID).\na\nwhite paper\nthat explains our estimation method. EAs might be particularly interested in the footnotes throughout, and the detailed\nresearch sources\nsection.\na\nspreadsheet\nto compute your COVID risk in more detail and to track your risk over time. EAs might find this more customizable and powerful than the web calculator.\nIf you have different beliefs than us and would like to use a version of the model that reflects your beliefs rather than ours, you can make modifications to your copy of the spreadsheet, or fork the repository and make a personal copy of the web calculator. We also hope you will\nsubmit suggestions\n, either by emailing us or by making issues or pull requests directly on Github.\nOur group house has been using this model as the basis of a shared agreement/protocol, based on a budget of 3,000 microCOVIDs per year to spend outside the house (about 58 per week). We know of another group house that (last we heard) was operating on *4* microCOVIDs per week!\nWe hope this helps you personally live a better pandemic life with more safety\nand\nmore flexibility.\n(also linkposted to the\nEA Forum\n)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "ARC has published a report on\nEliciting Latent Knowledge\n, an open problem which we believe is central to alignment. We think reading this report is the clearest way to understand what problems we are working on, how they fit into our plan for solving alignment in the worst case, and our research methodology.\nThe core difficulty we discuss is learning how to map between an AI\u2019s model of the world and a human\u2019s model. This is closely related to\nontology identification\n(and\nother\nsimilar\nstatements\n). Our main contribution is to present many possible approaches to the problem and a more precise discussion of why it seems to be difficult and important.\nThe report is available\nhere\nas a google document. If you're excited about this research,\nwe're hiring\n!\nQ&A\nWe're particularly excited about answering questions posted here throughout December. We welcome any questions no matter how basic or confused; we would love to help people understand what research we\u2019re doing and how we evaluate progress in enough detail that they could start to do it themselves.\nThanks to Mar\u00eda Guti\u00e9rrez-Rojas for the illustrations in this piece (the good ones, blame us for the ugly diagrams). Thanks to Buck Shlegeris, Jon Uesato, Carl Shulman, and especially Holden Karnofsky for helpful discussions and comments.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Averting doom by not building the doom machine\nIf you fear that someone will build a machine that will seize control of the world and annihilate humanity, then one kind of response is to try to build further machines that will seize control of the world even earlier without destroying it, forestalling the ruinous machine\u2019s conquest. An alternative or complementary kind of response is to try to avert such machines being built at all, at least while the degree of their apocalyptic tendencies is ambiguous.\nThe latter approach seems to me\u00a0 like the kind of basic and obvious thing worthy of at least consideration, and also in its favor, fits nicely in the genre \u2018stuff that it isn\u2019t that hard to imagine happening in the real world\u2019. Yet my impression is that for people worried about extinction risk from artificial intelligence, strategies under the heading \u2018actively slow down AI progress\u2019 have historically been dismissed and ignored (though \u2018don\u2019t actively speed up AI progress\u2019 is popular).\nThe conversation near me over the years has felt a bit like this:\nSome people:\nAI might kill everyone. We should design a godlike super-AI of perfect goodness to prevent that.\nOthers:\nwow that sounds extremely ambitious\nSome people:\nyeah but it\u2019s very important and also we are extremely smart so idk it could work\n[Work on it for a decade and a half]\nSome people:\nok that\u2019s pretty hard, we give up\nOthers:\noh huh shouldn\u2019t we maybe try to stop the building of this dangerous AI?\nSome people:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "hmm, that would involve coordinating numerous people\u2014we may be arrogant enough to think that we might build a god-machine that can take over the world and remake it as a paradise, but we aren\u2019t delusional\nThis seems like an error to me. (And\nlately\n,\nto\na\nbunch\nof\nother\npeople\n.)\nI don\u2019t have a strong view on whether anything in the space of \u2018try to slow down some AI research\u2019 should be done. But I think a) the naive first-pass guess should be a strong \u2018probably\u2019, and b) a decent amount of thinking should happen before writing off everything in this large space of interventions. Whereas customarily the tentative answer seems to be, \u2018of course not\u2019 and then the topic seems to be avoided for further thinking. (At least in my experience\u2014the AI safety community is large, and for most things I say here, different experiences are probably had in different bits of it.)\nMaybe my strongest view is that one shouldn\u2019t apply such different standards of ambition to these different classes of intervention. Like: yes, there appear to be substantial difficulties in slowing down AI progress to good effect. But in technical alignment, mountainous challenges are met with enthusiasm for mountainous efforts. And it is very non-obvious that the scale of difficulty here is much larger than that involved in designing acceptably safe versions of machines capable of taking over the world before anyone else in the world designs dangerous versions.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I\u2019ve been talking about this with people over the past many months, and have accumulated an abundance of reasons for not trying to slow down AI, most of which I\u2019d like to argue about at least a bit. My impression is that arguing in real life has coincided with people moving toward my views.\nQuick clarifications\nFirst, to fend off misunderstanding\u2014\nI take \u2018slowing down dangerous AI\u2019 to include any of:\nreducing the speed at which AI progress is made in general, e.g. as would occur if general funding for AI declined.\nshifting AI efforts from work leading more directly to risky outcomes to other work, e.g. as might occur if there was broadscale concern about very large AI models, and people and funding moved to other projects.\nHalting categories of work until strong confidence in its safety is possible, e.g. as would occur if AI researchers agreed that certain systems posed catastrophic risks and should not be developed until they did not. (This might mean a permanent end to some systems, if they were intrinsically unsafe.)\n(So in particular, I\u2019m including both actions whose direct aim is slowness in general, and actions whose aim is requiring safety before specific developments, which implies slower progress.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I do think there is serious attention on some versions of these things, generally under other names. I see people thinking about \u2018differential progress\u2019 (b. above), and strategizing about coordination to slow down AI at some point in the future (e.g. at \u2018deployment\u2019). And I think a lot of consideration is given to avoiding actively speeding up AI progress. What I\u2019m saying is missing are, a) consideration of actively working to slow down AI now, and b) shooting straightforwardly to \u2018slow down AI\u2019, rather than wincing from that and only considering examples of it that show up under another conceptualization (perhaps this is an unfair diagnosis).\nAI Safety is a big community, and I\u2019ve only ever been seeing a one-person window into it, so maybe things are different e.g. in DC, or in different conversations in Berkeley. I\u2019m just saying that for my corner of the world, the level of disinterest in this has been notable, and in my view misjudged.\nWhy not slow down AI? Why not consider it?\nOk, so if we tentatively suppose that this topic is worth even thinking about, what do we think? Is slowing down AI a good idea at all? Are there great reasons for dismissing it?\nScott Alexander wrote a post\na little while back raising reasons to dislike the idea, roughly:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Do you want to lose an arms race? If the AI safety community tries to slow things down, it will disproportionately slow down progress in the US, and then people elsewhere will go fast and get to be the ones whose competence determines whether the world is destroyed, and whose values determine the future if there is one. Similarly, if AI safety people criticize those contributing to AI progress, it will mostly discourage the most friendly and careful AI capabilities companies, and the reckless ones will get there first.\nOne might contemplate \u2018coordination\u2019 to avoid such morbid races. But coordinating anything with the whole world seems wildly tricky. For instance, some countries are large, scary, and hard to talk to.\nAgitating for slower AI progress is \u2018defecting\u2019 against the AI capabilities folks, who are good friends of the AI safety community, and their friendship is strategically valuable for ensuring that safety is taken seriously in AI labs (as well as being non-instrumentally lovely! Hi AI capabilities friends!).\nOther opinions I\u2019ve heard, some of which I\u2019ll address:\nSlowing AI progress is futile: for all your efforts you\u2019ll probably just die a few years later\nCoordination based on convincing people that AI risk is a problem is absurdly ambitious. It\u2019s practically impossible to convince AI professors of this, let alone any real fraction of humanity, and you\u2019d need to convince a massive number of people.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "What are we going to do, build powerful AI never and die when the Earth is eaten by the sun?\nIt\u2019s actually better for safety if AI progress moves fast. This might be because the faster AI capabilities work happens, the smoother AI progress will be, and this is more important than the duration of the period. Or speeding up progress now might force future progress to be correspondingly slower. Or\nbecause\nsafety work is probably better when done just before building the relevantly risky AI, in which case the best strategy might be to get as close to dangerous AI as possible and then stop and do safety work. Or if safety work is very useless ahead of time, maybe delay is fine, but there is little to gain by it.\nSpecific routes to slowing down AI are not worth it. For instance, avoiding working on AI capabilities research is bad because\nit\u2019s so helpful\nfor\nlearning\non the path to working on alignment. And AI safety people working in AI capabilities can be a force for making safer choices at those companies.\nAdvanced AI will help enough with other existential risks as to represent a net lowering of existential risk overall.\n1\nRegulators are ignorant about the nature of advanced AI (partly because it doesn\u2019t exist, so everyone is ignorant about it). Consequently they won\u2019t be able to regulate it effectively, and bring about desired outcomes.\nMy impression is that there are also less endorsable or less altruistic or more silly motives floating around for this attention allocation.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Some things that have come up at least once in talking to people about this, or that seem to be going on:\nAdvanced AI might bring manifold wonders, e.g. long lives of unabated thriving. Getting there a bit later is fine for posterity, but for our own generation it could mean dying as our ancestors did while on the cusp of a utopian eternity. Which would be pretty disappointing. For a person who really believes in this future, it can be tempting to shoot for the best scenario\u2014humanity builds strong, safe AI in time to save this generation\u2014rather than the scenario where our own lives are inevitably lost.\nSometimes people who have a heartfelt appreciation for the flourishing that technology has afforded so far can find it painful to be superficially on the side of Luddism here.\nFiguring out how minds work well enough to create new ones out of math is an incredibly deep and interesting intellectual project, which feels right to take part in. It can be hard to intuitively feel like one shouldn\u2019t do it.\n(Illustration from a\nco-founder of modern computational reinforcement learning\n: )\nIt will be the greatest intellectual achievement of all time.\nAn achievement of science, of engineering, and of the humanities,\nwhose significance is beyond humanity,\nbeyond life,\nbeyond good and bad.\n\u2014 Richard Sutton (@RichardSSutton)\nSeptember 29, 2022",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It is uncomfortable to contemplate projects that would put you in conflict with other people. Advocating for slower AI feels like trying to impede someone else\u2019s project, which feels adversarial and can feel like it has a higher burden of proof than just working on your own thing.\n\u2018Slow-down-AGI\u2019 sends people\u2019s minds to e.g. industrial sabotage or terrorism, rather than more boring courses, such as, \u2018lobby for labs developing shared norms for when to pause deployment of models\u2019. This understandably encourages dropping the thought as soon as possible.\nMy weak guess is that there\u2019s a kind of bias at play in AI risk thinking in general, where any force that isn\u2019t zero is taken to be arbitrarily intense. Like, if there is pressure for agents to exist, there will arbitrarily quickly be arbitrarily agentic things. If there is a feedback loop, it will be arbitrarily strong. Here, if stalling AI can\u2019t be forever, then it\u2019s essentially zero time. If a regulation won\u2019t obstruct every dangerous project, then is worthless. Any finite economic disincentive for dangerous AI is nothing in the face of the omnipotent economic incentives for AI. I think this is a bad mental habit: things in the real world often come down to actual finite quantities. This is very possibly an unfair diagnosis. (I\u2019m not going to discuss this later; this is pretty much what I have to say.)\nI sense an assumption that slowing progress on a technology would be a radical and unheard-of move.\nI agree with\nlc",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "that there seems to have been a quasi-taboo on the topic, which perhaps explains a lot of the non-discussion, though still calls for its own explanation. I think it suggests that concerns about uncooperativeness play a part, and the same for thinking of slowing down AI as centrally involving antisocial strategies.\nI\u2019m not sure if any of this fully resolves why AI safety people haven\u2019t thought about slowing down AI more, or whether people should try to do it. But my sense is that many of the above reasons are at least somewhat wrong, and motives somewhat misguided, so I want to argue about a lot of them in turn, including both arguments and vague motivational themes.\nThe mundanity of the proposal\nRestraint is not radical\nThere seems to be a common thought that technology is a kind of inevitable path along which the world must tread, and that trying to slow down or avoid any part of it would be both futile and extreme.\n2\nBut empirically, the world doesn\u2019t pursue every technology\u2014it barely pursues any technologies.\nSucky technologies\nFor a start, there are many machines that there is no pressure to make, because they have no value. Consider a machine that sprays shit in your eyes. We can technologically do that, but probably nobody has ever built that machine.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This might seem like a stupid example, because no serious \u2018technology is inevitable\u2019 conjecture is going to claim that totally pointless technologies are inevitable. But if you are sufficiently pessimistic about AI, I think this is the right comparison: if there are kinds of AI that would cause huge net costs to their creators if created, according to our best understanding, then they are at least as useless to make as the \u2018spray shit in your eyes\u2019 machine. We might accidentally make them due to error, but there is not some deep economic force pulling us to make them. If unaligned superintelligence destroys the world with high probability when you ask it to do a thing, then this is the category it is in, and it is not strange for its designs to just rot in the scrap-heap, with the machine that sprays shit in your eyes and the machine that spreads caviar on roads.\nOk, but maybe the relevant actors are very committed to being wrong about whether unaligned superintelligence would be a great thing to deploy. Or maybe you think the situation is less immediately dire and building existentially risky AI\nreally would\nbe good for the people making decisions (e.g. because the costs won\u2019t arrive for a while, and the people care a lot about a shot at scientific success relative to a chunk of the future). If the apparent economic incentives are large, are technologies unavoidable?\nExtremely valuable technologies",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It doesn\u2019t look like it to me. Here are a few technologies which I\u2019d guess have substantial economic value, where research progress or uptake appears to be drastically slower than it could be, for reasons of concern about safety or ethics\n3\n:\nHuge amounts of medical research, including really important medical research e.g. The FDA\nbanned\nhuman trials of strep A vaccines from the 70s to the 2000s, in spite of\n500,000 global deaths every year\n. A lot of people also died while covid vaccines went through all the proper trials.\nNuclear energy\nFracking\nVarious genetics things: genetic modification of foods, gene drives, early recombinant DNA researchers famously organized a moratorium and then ongoing research guidelines including prohibition of certain experiments (see the\nAsilomar Conference\n)\nNuclear, biological, and maybe chemical weapons (or maybe these just aren\u2019t useful)\nVarious human reproductive innovation: cloning of humans, genetic manipulation of humans (a notable example of an economically valuable technology that is to my knowledge barely pursued across different countries, without explicit coordination between those countries, even though it would make those countries more competitive.\nSomeone used CRISPR on babies in China\n, but was\nimprisoned\nfor it.)\nRecreational drug development\nGeoengineering\nMuch of science about humans? I recently ran\nthis survey",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ", and was reminded how encumbering ethical rules are for even incredibly innocuous research. As far as I could tell the EU now makes it illegal to collect data in the EU unless you promise to delete the data from anywhere that it might have gotten to if the person who gave you the data wishes for that at some point. In all, dealing with this and IRB-related things added maybe more than half of the effort of the project. Plausibly I misunderstand the rules, but I doubt other researchers are radically better at figuring them out than I am.\nThere are probably examples from fields considered distasteful or embarrassing to associate with, but it\u2019s hard as an outsider to tell which fields are genuinely hopeless versus erroneously considered so. If there are economically valuable health interventions among those considered wooish, I imagine they would be much slower to be identified and pursued by scientists with good reputations than a similarly promising technology not marred in that way. Scientific research into intelligence is more clearly slowed by stigma, but it is less clear to me what the economically valuable upshot would be.\n(I think there are many other things that could be in this list, but I don\u2019t have time to review them at the moment.\nThis page\nmight collect more of them in future.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It seems to me that intentionally slowing down progress in technologies to give time for even probably-excessive caution is commonplace. (And this is just looking at things slowed down over caution or ethics specifically\u2014probably there are also other reasons things get slowed down.)\nFurthermore, among valuable technologies that nobody is especially trying to slow down, it seems common enough for progress to be massively slowed by relatively minor obstacles, which is further evidence for a lack of overpowering strength of the economic forces at play. For instance, Fleming first took notice of mold\u2019s effect on bacteria in 1928, but nobody took a serious, high-effort shot at developing it as a drug until 1939.\n4\nFurthermore, in the thousands of years preceding these events,\nvarious\npeople\nnoticed numerous times that mold, other fungi or plants inhibited bacterial growth, but didn\u2019t exploit this observation even enough for it not to be considered a new discovery in the 1920s. Meanwhile, people dying of infection was quite a thing.\nIn 1930\nabout 300,000 Americans died of bacterial illnesses per year (around 250/100k).\nMy guess is that people make real choices about technology, and they do so in the face of economic forces that are feebler than commonly thought.\nRestraint is not terrorism, usually",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I think people have historically imagined weird things when they think of \u2018slowing down AI\u2019. I posit that their central image is sometimes terrorism (which understandably they don\u2019t want to think about for very long), and sometimes some sort of implausibly utopian global agreement.\nHere are some other things that \u2018slow down AI capabilities\u2019 could look like (where the best positioned person to carry out each one differs, but if you are not that person, you could e.g. talk to someone who is):\nDon\u2019t actively forward AI progress, e.g. by devoting your life or millions of dollars to it (this one is often considered already)\nTry to convince researchers, funders, hardware manufacturers, institutions etc that they too should stop actively forwarding AI progress\nTry to get any of those people to stop actively forwarding AI progress even if they don\u2019t agree with you: through negotiation, payments, public reproof, or other activistic means.\nTry to get the message to the world that AI is heading toward being seriously endangering. If AI progress is broadly condemned, this will trickle into myriad decisions: job choices, lab policies, national laws. To do this, for instance\nproduce compelling demos of risk, agitate for stigmatization of risky actions\n, write science fiction illustrating the problems broadly and evocatively (I think this has actually been helpful repeatedly in the past), go on TV, write opinion pieces, help organize and empower the people who are already concerned, etc.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Help organize the researchers who think their work is potentially omnicidal into coordinated action on not doing it.\nMove AI resources from dangerous research to other research. Move investments from projects that lead to large but poorly understood capabilities, to projects that lead to understanding these things e.g. theory before scaling (see\ndifferential technological development\nin general\n5\n).\nFormulate specific precautions for AI researchers and labs to take in different well-defined future situations,\nAsilomar Conference\nstyle. These could include more intense vetting by particular parties or methods, modifying experiments, or pausing lines of inquiry entirely. Organize labs to coordinate on these.\nReduce available compute for AI, e.g. via regulation of production and trade, seller choices, purchasing compute, trade strategy.\nAt labs, choose policies that slow down other labs, e.g. reduce public helpful research outputs\nAlter the publishing system and incentives to reduce research dissemination. E.g. A journal verifies research results and releases the fact of their publication without any details, maintains records of research priority for later release, and distributes funding for participation. (This is how Szil\u00e1rd and co. arranged the mitigation of 1940s nuclear research helping Germany, except I\u2019m not sure if the compensatory funding idea was used.\n6\n)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The above actions would be taken through choices made by scientists, or funders, or legislators, or labs, or public observers, etc. Communicate with those parties, or help them act.\nCoordination is not miraculous world government, usually\nThe common image of coordination seems to be explicit, centralized, involving of every party in the world, and something like cooperating on a prisoners\u2019 dilemma: incentives push every rational party toward defection at all times, yet maybe through deontological virtues or sophisticated decision theories or strong international treaties, everyone manages to not defect for enough teetering moments to find another solution.\nThat is a possible way coordination could be. (And I think one that shouldn\u2019t be seen as so hopeless\u2014the world has actually coordinated on some impressive things, e.g. nuclear non-proliferation.) But if what you want is for lots of people to coincide in doing one thing when they might have done another, then there are quite a few ways of achieving that.\nConsider some other case studies of coordinated behavior:\nNot eating sand.\nThe whole world coordinates to barely eat any sand at all. How do they manage it? It is actually not in almost anyone\u2019s interest to eat sand, so the mere maintenance of sufficient epistemological health to have this widely recognized does the job.\nEschewing bestiality:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "probably some people think bestiality is moral, but enough don\u2019t that engaging in it would risk huge stigma. Thus the world coordinates fairly well on doing very little of it.\nNot wearing Victorian attire on the streets:\nthis is similar but with no moral blame involved. Historic dress is arguably often more aesthetic than modern dress, but even people who strongly agree find it unthinkable to wear it in general, and assiduously avoid it except for when they have \u2018excuses\u2019 such as a special party. This is a very strong coordination against what appears to otherwise be a ubiquitous incentive (to be nicer to look at). As far as I can tell, it\u2019s powered substantially by the fact that it is \u2018not done\u2019 and would now be weird to do otherwise. (Which is a very general-purpose mechanism.)\nPolitical correctness:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "public discourse has strong norms about what it is okay to say, which do not appear to derive from a vast majority of people agreeing about this (as with bestiality say). New ideas about what constitutes being politically correct sometimes spread widely. This coordinated behavior seems to be roughly due to decentralized application of social punishment, from both a core of proponents, and from people who fear punishment for not punishing others. Then maybe also from people who are concerned by non-adherence to what now appears to be the norm given the actions of the others. This differs from the above examples, because it seems like it could persist even with a very small set of people agreeing with the object-level reasons for a norm. If failing to advocate for the norm gets you publicly shamed by advocates, then you might tend to advocate for it, making the pressure stronger for everyone else.\nThese are all cases of very broadscale coordination of behavior, none of which involve prisoners\u2019 dilemma type situations, or people making explicit agreements which they then have an incentive to break. They do not involve centralized organization of huge multilateral agreements. Coordinated behavior can come from everyone individually wanting to make a certain choice for correlated reasons, or from people wanting to do things that those around them are doing, or from distributed behavioral dynamics such as punishment of violations, or from collaboration in thinking about a topic.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "You might think they are weird examples that aren\u2019t very related to AI. I think, a) it\u2019s important to remember the plethora of weird dynamics that actually arise in human group behavior and not get carried away theorizing about AI in a world drained of everything but prisoners\u2019 dilemmas and binding commitments, and b) the above are actually all potentially relevant dynamics here.\nIf AI in fact poses a large existential risk within our lifetimes, such that it is net bad for any particular individual, then the situation in theory looks a lot like that in the \u2018avoiding eating sand\u2019 case. It\u2019s an option that a rational person wouldn\u2019t want to take if they were just alone and not facing any kind of multi-agent situation. If AI is that dangerous, then not taking this inferior option could largely come from a coordination mechanism as simple as distribution of good information. (You still need to deal with irrational people and people with unusual values.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But even failing coordinated caution from ubiquitous insight into the situation, other models might work. For instance, if there came to be somewhat widespread concern that AI research is bad, that might\u00a0substantially lessen participation in it, beyond the set of people who are concerned, via mechanisms similar to those described above. Or it might give rise to a wide crop of local regulation, enforcing whatever behavior is deemed acceptable. Such regulation need not be centrally organized across the world to serve the purpose of coordinating the world, as long as it grew up in different places similarly. Which might happen because different locales have similar interests (all rational governments should be similarly concerned about losing power to automated power-seeking systems with unverifiable goals), or because\u2014as with individuals\u2014there are social dynamics which support norms arising in a non-centralized way.\nThe arms race model and its alternatives\nOk, maybe in principle you might hope to coordinate to not do self-destructive things, but realistically, if the US tries to slow down, won\u2019t China or Facebook or someone less cautious take over the world?\nLet\u2019s be more careful about the game we are playing, game-theoretically speaking.\nThe arms race\nWhat is an arms race, game theoretically? It\u2019s an iterated\nprisoners\u2019 dilemma\n, seems to me. Each round looks something like this:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Player 1 chooses a row, Player 2 chooses a column, and the resulting payoffs are listed in each cell, for {Player 1, Player 2}\nIn this example, building weapons costs one unit. If anyone ends the round with more weapons than anyone else, they take all of their stuff (ten units).\nIn a single round of the game it\u2019s always better to build weapons than not (assuming your actions are devoid of implications about your opponent\u2019s actions). And it\u2019s always better to get the hell out of this game.\nThis is not much like what the current AI situation looks like, if you think AI poses a substantial risk of destroying the world.\nThe suicide race\nA closer model: as above except\nif anyone chooses to build, everything is destroyed\n(everyone loses all their stuff\u2014ten units of value\u2014as well as one unit if they built).\nThis is importantly different from the classic \u2018arms race\u2019 in that pressing the \u2018everyone loses now\u2019 button isn\u2019t an equilibrium strategy.\nThat is: for anyone who thinks powerful misaligned AI represents near-certain death, the existence of other possible AI builders is not any reason to \u2018race\u2019.\nBut few people are that pessimistic. How about a milder version where there\u2019s a good chance that the players \u2018align the AI\u2019?\nThe safety-or-suicide race",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Ok, let\u2019s do a game like the last but where if anyone builds, everything is only maybe destroyed (minus ten to all), and in the case of survival, everyone returns to the original arms race fun of redistributing stuff based on who built more than whom (+10 to a builder and -10 to a non-builder if there is one of each). So if you build AI alone, and get lucky on the probabilistic apocalypse, can still win big.\nLet\u2019s take 50% as the chance of doom if any building happens. Then we have a game whose expected payoffs are half way between those in the last two games:\n(These are expected payoffs\u2014the minus one unit return to building alone comes from the one unit cost of building, plus half a chance of losing ten in an extinction event and half a chance of taking ten from your opponent in a world takeover event.)\nNow you want to do whatever the other player is doing: build if they\u2019ll build, pass if they\u2019ll pass.\nIf the odds of destroying the world were very low, this would become the original arms race, and you\u2019d always want to build. If very high, it would become the suicide race, and you\u2019d never want to build. What the probabilities have to be in the real world to get you into something like these different phases is going to be different, because all these parameters are made up (the downside of human extinction is not 10x the research costs of building powerful AI, for instance).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But my point stands: even in terms of simplish models, it\u2019s very non-obvious that we are in or near an arms race. And therefore, very non-obvious that racing to build advanced AI faster is even promising at a first pass.\nIn less game-theoretic terms: if you don\u2019t seem anywhere near solving alignment, then racing as hard as you can to be the one who it falls upon to have solved alignment\u2014especially if that means having less time to do so, though I haven\u2019t discussed that here\u2014is probably unstrategic. Having more ideologically pro-safety AI designers win an \u2018arms race\u2019 against less concerned teams is futile if you don\u2019t have a way for such people to implement enough safety to actually not die, which seems like a very live possibility. (\nRobby Bensinger\nand maybe Andrew Critch somewhere make similar points.)\nConversations with my friends on this kind of topic can go like this:\nMe\n: there\u2019s no real incentive to race if the prize is mutual death\nThem\n: sure, but it isn\u2019t\u2014if there\u2019s a sliver of hope of surviving unaligned AI, and if your side taking control in that case is a bit better in expectation, and if they are going to build powerful AI anyway, then it\u2019s worth racing. The whole future is on the line!\nMe:\nWouldn\u2019t you still be better off directing your own efforts to safety, since your safety efforts will also help everyone end up with a safe AI?\nThem",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ": It will probably only help them somewhat\u2014you don\u2019t know if the other side will use your safety research. But also, it\u2019s not just that they have less safety research. Their values are probably worse, by your lights.\nMe\n: If they succeed at alignment, are foreign values really worse than local ones? Probably any humans with vast intelligence at hand have a similar shot at creating a glorious human-ish utopia, no?\nThem\n: No, even if you\u2019re right that being similarly human gets you to similar values in the end, the other parties might be more foolish than our side, and lock-in\n7\nsome poorly thought-through version of their values that they want at the moment, or even if all projects would be so foolish, our side might have better poorly thought-through values to lock in, as well as being more likely to use safety ideas at all. Even if racing is very likely to lead to death, and survival is very likely to lead to squandering most of the value, in that sliver of happy worlds so much is at stake in whether it is us or someone else doing the squandering!\nMe\n: Hmm, seems complicated, I\u2019m going to need paper for this.\nThe complicated race/anti-race\nHere\nis a spreadsheet of models you can make a copy of and play with.\nThe first model is like this:\nEach player divides their effort between safety and capabilities\nOne player \u2018wins\u2019, i.e. builds \u2018AGI\u2019 (artificial general intelligence) first.\nP(Alice wins) is a logistic function of Alice\u2019s capabilities investment relative to Bob\u2019s",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Each players\u2019 total safety is their own safety investment plus a fraction of the other\u2019s safety investment.\nFor each player there is some distribution of outcomes if they achieve safety, and a set of outcomes if they do not, which takes into account e.g. their proclivities for enacting stupid near-term lock-ins.\nThe outcome is a distribution over winners and states of alignment, each of which is a distribution of worlds (e.g. utopia, near-term good lock-in..)\nThat all gives us a number of utils (Delicious utils!)\nThe second model is the same except that instead of dividing effort between safety and capabilities, you choose a speed, and the amount of alignment being done by each party is an exogenous parameter.\nThese models probably aren\u2019t very good, but so far support a key claim I want to make here: it\u2019s pretty non-obvious whether one should go faster or slower in this kind of scenario\u2014it\u2019s sensitive to a lot of different parameters in plausible ranges.\nFurthermore, I don\u2019t think the results of quantitative analysis match people\u2019s intuitions here.\nFor example, here\u2019s a situation which I think sounds intuitively like a you-should-race world, but where in the first model above, you should actually go as slowly as possible (this should be the one plugged into the spreadsheet now):\nAI is pretty safe:\nunaligned AGI has a mere 7% chance of causing doom, plus a further 7% chance of causing short term lock-in of something mediocre\nYour opponent risks bad lock-in:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "If there\u2019s a \u2018lock-in\u2019 of something mediocre, your opponent has a 5% chance of locking in something actively terrible, whereas you\u2019ll always pick good mediocre lock-in world (and mediocre lock-ins are either 5% as good as utopia, -5% as good)\nYour opponent risks messing up utopia:\nIn the event of aligned AGI, you will reliably achieve the best outcome, whereas your opponent has a 5% chance of ending up in a \u2018mediocre bad\u2019 scenario then too.\nSafety investment obliterates your chance of getting to AGI first:\nmoving from no safety at all to full safety means you go from a 50% chance of being first to a 0% chance\nYour opponent is racing:\nYour opponent is investing everything in capabilities and nothing in safety\nSafety work helps others at a steep discount:\nyour safety work contributes 50% to the other player\u2019s safety\nYour best bet here (on this model) is still to maximize safety investment. Why? Because by aggressively pursuing safety, you can get the other side half way to full safety, which is worth a lot more than than the lost chance of winning. Especially since if you \u2018win\u2019, you do so without much safety, and your victory without safety is worse than your opponent\u2019s victory with safety, even if that too is far from perfect.\nSo if you are in a situation in this space, and the other party is racing, it\u2019s not obvious if it is even in your narrow interests within the game to go faster at the expense of safety, though it may be.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "These models are flawed in many ways, but I think they are better than the intuitive models that support arms-racing. My guess is that the next better still models remain nuanced.\nOther equilibria and other games\nEven if it would be in your interests to race if the other person were racing, \u2018(do nothing, do nothing)\u2019 is often an equilibrium too in these games. At least for various settings of the parameters. It doesn\u2019t necessarily make sense to do nothing in the hope of getting to that equilibrium if you know your opponent to be mistaken about that and racing anyway, but in conjunction with communicating with your \u2018opponent\u2019, it seems like a theoretically good strategy.\nThis has all been assuming the structure of the game. I think the traditional response to an arms race situation is to remember that you are in a more elaborate world with all kinds of unmodeled affordances, and try to get out of the arms race.\nBeing friends with risk-takers\nCaution is cooperative\nAnother big concern is that pushing for slower AI progress is \u2018defecting\u2019 against AI researchers who are friends of the AI safety community.\nFor instance\nSteven Byrnes\n:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u201cI think that trying to slow down research towards AGI through regulation would fail, because everyone (politicians, voters, lobbyists, business, etc.) likes scientific research and technological development, it creates jobs, it cures diseases, etc. etc., and you\u2019re saying we should have less of that. So I think the effort would fail, and also be massively counterproductive by making the community of AI researchers see the community of AGI safety / alignment people as their enemies, morons, weirdos, Luddites, whatever.\u201d\n(Also a good example of the view criticized earlier, that regulation of things that create jobs and cure diseases just doesn\u2019t happen.)\nOr Eliezer Yudkowsky, on worry that spreading fear about AI would alienate top AI labs:\nThis is the primary reason I didn't, and told others not to, earlier connect the point about human extinction from AGI with AI labs.  Kerry has correctly characterized the position he is arguing against, IMO.  I myself estimate the public will be toothless vs AGI lab heads.\n\u2014 Eliezer Yudkowsky (@ESYudkowsky)\nAugust 4, 2022\nI don\u2019t think this is a natural or reasonable way to see things, because:\nThe researchers themselves probably don\u2019t want to destroy the world. Many of them also actually\nagree\nthat AI is a serious existential risk. So in two natural ways, pushing for caution is cooperative with many if not most AI researchers.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "AI researchers do not have a moral right to endanger the world, that someone would be stepping on by requiring that they move more cautiously. Like, why does \u2018cooperation\u2019 look like the safety people bowing to what the more reckless capabilities people want, to the point of fearing to represent their actual interests, while the capabilities people uphold their side of the \u2018cooperation\u2019 by going ahead and building dangerous AI? This situation might make sense as a natural consequence of different people\u2019s power in the situation. But then don\u2019t call it a \u2018cooperation\u2019, from which safety-oriented parties would be dishonorably \u2018defecting\u2019 were they to consider exercising any power they did have.\nIt could be that people in control of AI capabilities would respond negatively to AI safety people pushing for slower progress. But that should be called \u2018we might get punished\u2019 not \u2018we shouldn\u2019t defect\u2019. \u2018Defection\u2019 has moral connotations that are not due. Calling one side pushing for their preferred outcome \u2018defection\u2019 unfairly disempowers them by wrongly setting commonsense morality against them.\nAt least if it is the safety side. If any of the available actions are \u2018defection\u2019 that the world in general should condemn, I claim that it is probably \u2018building machines that will plausibly destroy the world, or standing by while it happens\u2019.\n(This would be more complicated if the people involved were confident that they wouldn\u2019t destroy the world and I merely disagreed with them. But",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "about half of surveyed researchers\nare actually more pessimistic than me. And in a situation where the median AI researcher thinks the field has a 5-10% chance of causing human extinction, how confident can any responsible person be in their own judgment that it is safe?)\nOn top of all that, I worry that highlighting the narrative that wanting more cautious progress is defection is further destructive, because it makes it more likely that AI capabilities people see AI safety people as thinking of themselves as betraying AI researchers, if anyone engages in any such efforts. Which makes the efforts more aggressive. Like, if every time you see friends, you refer to it as \u2018cheating on my partner\u2019, your partner may reasonably feel hurt by your continual desire to see friends, even though the activity itself is innocuous.\n\u2018We\u2019 are not the US, \u2018we\u2019 are not the AI safety community\n\u201cIf \u2018we\u2019 try to slow down AI, then the other side might win.\u201d \u201cIf \u2018we\u2019 ask for regulation, then it might harm \u2018our\u2019 relationships with AI capabilities companies.\u201d Who are these \u2018we\u2019s? Why are people strategizing for those groups in particular?\nEven if slowing AI were uncooperative, and it were important for the AI Safety community to cooperate with the AI capabilities community, couldn\u2019t one of the many people not in the AI Safety community work on it?\nI have a\nlongstanding irritation",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "with thoughtless talk about what \u2018we\u2019 should do, without regard for what collective one is speaking for. So I may be too sensitive about it here. But I think confusions arising from this have genuine consequences.\nI think when people say \u2018we\u2019 here, they generally imagine that they are strategizing on behalf of, a) the AI safety community, b) the USA, c) themselves or d) they and their readers. But those are a small subset of people, and not even obviously the ones the speaker can most influence (does the fact that you are sitting in the US really make the US more likely to listen to your advice than e.g. Estonia? Yeah probably on average, but not infinitely much.) If these naturally identified-with groups don\u2019t have good options, that hardly means there are no options to be had, or to be communicated to other parties. Could the speaker speak to a different \u2018we\u2019? Maybe someone in the \u2018we\u2019 the speaker has in mind knows someone not in that group? If there is a strategy for anyone in the world, and you can talk, then there is probably a strategy for you.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The starkest appearance of error along these lines to me is in writing off the slowing of AI as inherently destructive of relations between the AI safety community and other AI researchers. If we grant that such activity would be seen as a betrayal (which seems unreasonable to me, but maybe), surely it could only be a betrayal if carried out by the AI safety community. There are quite a lot of people who aren\u2019t in the AI safety community and have a stake in this, so maybe some of them could do something. It seems like a huge oversight to give up on all slowing of AI progress because you are only considering affordances available to the AI Safety Community.\nAnother example: if the world were in the basic arms race situation sometimes imagined, and the United States would be willing to make laws to mitigate AI risk, but could not because China would barge ahead, then that means China is in a great place to mitigate AI risk. Unlike the US, China could propose mutual slowing down, and the US would go along. Maybe it\u2019s not impossible to communicate this to relevant people in China.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "An oddity of this kind of discussion which feels related is the persistent assumption that one\u2019s ability to act is restricted to the United States. Maybe I fail to understand the extent to which Asia is an alien and distant land where agency doesn\u2019t apply, but for instance I just wrote to like a thousand machine learning researchers there, and maybe a hundred wrote back, and it was a lot like interacting with people in the US.\nI\u2019m pretty ignorant about what interventions will work in any particular country, including the US, but I just think it\u2019s weird to come to the table assuming that you can essentially only affect things in one country. Especially if the situation is that you believe you have unique knowledge about what is in the interests of people in other countries. Like, fair enough I would be deal-breaker-level pessimistic if you wanted to get an Asian government to elect you leader or something. But if you think advanced AI is highly likely to destroy the world, including other countries, then the situation is totally different. If you are right, then everyone\u2019s incentives are basically aligned.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I more weakly suspect some related mental shortcut is misshaping the discussion of arms races in general. The thought that something is a \u2018race\u2019 seems much stickier than alternatives, even if the true incentives don\u2019t really make it a race. Like, against the laws of game theory, people sort of expect the enemy to try to believe falsehoods, because it will better contribute to their racing. And this feels like realism. The uncertain details of billions of people one barely knows about, with all manner of interests and relationships, just really wants to form itself into an \u2018us\u2019 and a \u2018them\u2019 in zero-sum battle. This is a mental shortcut that could really kill us.\nMy impression is that in practice, for many of the technologies slowed down for risk or ethics, mentioned in section \u2018Extremely valuable technologies\u2019 above, countries with fairly disparate cultures have converged on similar approaches to caution. I take this as evidence that none of ethical thought, social influence, political power, or rationality are actually very siloed by country, and in general the \u2018countries in contest\u2019 model of everything isn\u2019t very good.\nNotes on tractability\nConvincing people doesn\u2019t seem that hard",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When I say that \u2018coordination\u2019 can just look like popular opinion punishing an activity, or that other countries don\u2019t have much real incentive to build machines that will kill them, I think a common objection is that convincing people of the real situation is hopeless. The picture seems to be that the argument for AI risk is extremely sophisticated and only able to be appreciated by the most elite of intellectual elites\u2014e.g. it\u2019s hard enough to convince professors on Twitter, so surely the masses are beyond its reach, and foreign governments too.\nThis doesn\u2019t match my overall experience on various fronts.\nSome observations:\nThe\nmedian surveyed ML researcher\nseems to think AI will destroy humanity with 5-10% chance, as I mentioned\nOften people are already intellectually convinced\nbut haven\u2019t integrated that into their behavior, and it isn\u2019t hard to help them organize to act on their tentative beliefs\nAs noted by\nScott\n, a lot of AI safety people have gone into AI capabilities including running AI capabilities orgs, so those people presumably consider AI to be risky already\nI don\u2019t remember ever having any trouble discussing AI risk with random strangers. Sometimes they are also fairly worried (e.g. a makeup artist at Sephora gave an extended rant about the dangers of advanced AI, and my driver in Santiago excitedly concurred and showed me\nHomo Deus",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "open on his front seat). The form of the concerns are probably a bit different from those of the AI Safety community, but I think broadly closer to, \u2018AI agents are going to kill us all\u2019 than \u2018algorithmic bias will be bad\u2019. I can\u2019t remember how many times I have tried this, but pre-pandemic I used to talk to Uber drivers a lot, due to having no idea how to avoid it. I explained AI risk to my therapist recently, as an aside regarding his sense that I might be catastrophizing, and I feel like it went okay, though we may need to discuss again.\nMy impression is that most people haven\u2019t even come into contact with the arguments that might bring one to agree precisely with the AI safety community. For instance, my guess is that a lot of people assume that someone actually programmed modern AI systems, and if you told them that in fact they are random connections jiggled in an gainful direction unfathomably many times, just as mysterious to their makers, they might also fear misalignment.\nNick Bostrom, Eliezer Yudkokwsy, and other early thinkers have had decent success at convincing a bunch of other people to worry about this problem, e.g. me. And to my knowledge, without writing any compelling and accessible account of why one should do so that would take less than two hours to read.\nI arrogantly think I could write a broadly compelling and accessible case for AI risk",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "My weak guess is that immovable AI risk skeptics are concentrated in intellectual circles near the AI risk people, especially on Twitter, and that people with less of a horse in the intellectual status race are more readily like, \u2018oh yeah, superintelligent robots are probably bad\u2019. It\u2019s not clear that most people even need convincing that there is a problem, though they don\u2019t seem to consider it the most pressing problem in the world. (Though all of this may be different in cultures I am more distant from, e.g. in China.) I\u2019m pretty non-confident about this, but skimming\nsurvey evidence\nsuggests there is substantial though not overwhelming public concern about AI in the US\n8\n.\nDo you need to convince everyone?\nI could be wrong, but I\u2019d guess convincing the ten most relevant leaders of AI labs that this is a massive deal, worth prioritizing, actually gets you a decent slow-down. I don\u2019t have much evidence for this.\nBuying time is big\nYou probably aren\u2019t going to avoid AGI forever, and maybe huge efforts will buy you a couple of years.\n9\nCould that even be worth it?\nSeems pretty plausible:\nWhatever kind of other AI safety research or policy work people were doing could be happening at a non-negligible rate per year. (Along with all other efforts to make the situation better\u2014if you buy a year, that\u2019s eight billion extra person years of time, so only a tiny bit has to be spent usefully for this to be big. If a lot of people are worried, that doesn\u2019t seem crazy.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Geopolitics just changes pretty often. If you seriously think a big determiner of how badly things go is inability to coordinate with certain groups, then every year gets you non-negligible opportunities for the situation changing in a favorable way.\nPublic opinion can change a lot quickly. If you can only buy one year, you might still be buying a decent shot of people coming around and granting you more years. Perhaps especially if new evidence is actively avalanching in\u2014people changed their minds a lot in February 2020.\nOther stuff happens over time. If you can take your doom today or after a couple of years of random events happening, the latter seems non-negligibly better in general.\nIt is also not obvious to me that these are the time-scales on the table. My sense is that things which are slowed down by regulation or general societal distaste are often slowed down much more than a year or two, and Eliezer\u2019s stories presume that the world is full of collectives either trying to destroy the world or badly mistaken about it, which is not a foregone conclusion.\nDelay is probably finite by default\nWhile some people worry that any delay would be so short as to be negligible, others seem to fear that if AI research were halted, it would never start again and we would fail to go to space or something. This sounds so wild to me that I think I\u2019m missing too much of the reasoning to usefully counterargue.\nObstruction doesn\u2019t need discernment",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Another purported risk of trying to slow things down is that it might involve getting regulators involved, and they might be fairly ignorant about the details of futuristic AI, and so tenaciously make the wrong regulations. Relatedly, if you call on the public to worry about this, they might have inexacting worries that call for impotent solutions and distract from the real disaster.\nI don\u2019t buy it. If all you want is to slow down a broad area of activity, my guess is that ignorant regulations do just fine at that every day (usually unintentionally). In particular, my impression is that if you mess up regulating things, a usual outcome is that many things are randomly slower than hoped. If you wanted to speed a specific thing up, that\u2019s a very different story, and might require understanding the thing in question.\nThe same goes for social opposition. Nobody need understand the details of how genetic engineering works for its ascendancy to be seriously impaired by people not liking it. Maybe by their lights it still isn\u2019t optimally undermined yet, but just not liking anything in the vicinity does go a long way.\nThis has nothing to do with regulation or social shaming specifically. You need to understand much less about a car or a country or a conversation to mess it up than to make it run well. It is a consequence of the\ngeneral rule\nthat there are many more ways for a thing to be dysfunctional than functional: destruction is easier than creation.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Back at the object level, I tentatively expect efforts to broadly slow down things in the vicinity of AI progress to slow down AI progress on net, even if poorly aimed.\nSafety from speed, clout from complicity\nMaybe it\u2019s actually better for safety to have AI go fast at present, for various reasons. Notably:\nImplementing what can be implemented as soon as possible probably means smoother progress, which is probably safer because a) it makes it harder for one party shoot ahead of everyone and gain power, and b) people make better choices all around if they are correct about what is going on (e.g. they don\u2019t put trust in systems that turn out to be much more powerful than expected).\nIf the main thing achieved by slowing down AI progress is more time for safety research, and safety research is more effective when carried out in the context of more advanced AI, and there is a certain amount of slowing down that can be done (e.g. because one is in fact in an arms race but has some lead over competitors), then it might better to use one\u2019s slowing budget later.\nIf there is some underlying curve of potential for progress (e.g. if money that might be spent on hardware just grows a certain amount each year), then perhaps if we push ahead now that will naturally require they be slower later, so it won\u2019t affect the overall time to powerful AI, but will mean we spend more time in the informative pre-catastrophic-AI era.\n(More things go here I think)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "And maybe it\u2019s worth it to work on capabilities research at present, for instance because:\nAs a researcher, working on capabilities prepares you to work on safety\nYou think the room where AI happens will afford good options for a person who cares about safety\nThese all seem plausible. But also plausibly wrong. I don\u2019t know of a decisive analysis of any of these considerations, and am not going to do one here. My impression is that they could basically all go either way.\nI am actually particularly skeptical of the final argument, because if you believe what I take to be the normal argument for AI risk\u2014that superhuman artificial agents won\u2019t have acceptable values, and will aggressively manifest whatever values they do have, to the sooner or later annihilation of humanity\u2014then the sentiments of the people turning on such machines seem like a very small factor, so long as they still turn the machines on. And I suspect that \u2018having a person with my values doing X\u2019 is commonly overrated. But the world is messier than these models, and I\u2019d still pay a lot to be in the room to try.\nMoods and philosophies, heuristics and attitudes\nIt\u2019s not clear what role these psychological characters should play in a rational assessment of how to act, but I think they do play a role, so I want to argue about them.\nTechnological choice is not luddism",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Some technologies are better than others [citation not needed]. The best pro-technology visions should disproportionately involve awesome technologies and avoid shitty technologies, I claim. If you think AGI is highly likely to destroy the world, then it is the pinnacle of shittiness as a technology. Being opposed to having it into your techno-utopia is about as luddite as refusing to have\nradioactive toothpaste\nthere. Colloquially, Luddites are against progress if it comes as technology.\n10\nEven if that\u2019s a terrible position, its wise reversal is not the endorsement of all \u2018technology\u2019, regardless of whether it comes as progress.\nNon-AGI visions of near-term thriving\nPerhaps slowing down AI progress means foregoing our own generation\u2019s hope for life-changing technologies. Some people thus find it psychologically difficult to aim for less AI progress (with its real personal costs), rather than shooting for the perhaps unlikely \u2018safe AGI soon\u2019 scenario.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I\u2019m not sure that this is a real dilemma. The narrow AI progress we have seen already\u2014i.e. further applications of current techniques at current scales\u2014seems plausibly able to help a lot with longevity and other medicine for instance. And to the extent AI efforts could be focused on e.g. medically relevant narrow systems over creating agentic scheming gods, it doesn\u2019t sound crazy to imagine making more progress on anti-aging etc as a result (even before taking into account the probability that the agentic scheming god does not prioritize your physical wellbeing as hoped). Others disagree with me here.\nRobust priors vs. specific galaxy-brained models\nThere are things that are robustly good in the world, and things that are good on highly specific inside-view models and terrible if those models are wrong. Slowing dangerous tech development seems like the former, whereas forwarding arms races for dangerous tech between world superpowers seems more like the latter.\n11\nThere is a general question of how much to trust your reasoning and risk the galaxy-brained plan.\n12",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "But whatever your take on that, I think we should all agree that the less thought you have put into it, the more you should regress to the robustly good actions. Like, if it just occurred to you to take out a large loan to buy a fancy car, you probably shouldn\u2019t do it because most of the time it\u2019s a poor choice. Whereas if you have been thinking about it for a month, you might be sure enough that you are in the rare situation where it will pay off.\nOn this particular topic, it feels like people are going with the specific galaxy-brained inside-view terrible-if-wrong model off the bat, then not thinking about it more.\nCheems mindset/can\u2019t do attitude",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Suppose you have a friend, and you say \u2018let\u2019s go to the beach\u2019 to them. Sometimes the friend is like \u2018hell yes\u2019 and then even if you don\u2019t have towels or a mode of transport or time or a beach, you make it happen. Other times, even if you have all of those things, and your friend nominally wants to go to the beach, they will note that they have a package coming later, and that it might be windy, and their jacket needs washing. And when you solve those problems, they will note that it\u2019s not that long until dinner time. You might infer that in the latter case your friend just doesn\u2019t want to go to the beach. And sometimes that is the main thing going on! But I think there are also broader differences in attitudes: sometimes people are looking for ways to make things happen, and sometimes they are looking for reasons that they can\u2019t happen. This is sometimes called a \u2018\ncheems attitude\n\u2019, or I like to call it (more accessibly) a \u2018can\u2019t do attitude\u2019.\nMy experience in talking about slowing down AI with people is that they seem to have a can\u2019t do attitude. They don\u2019t want it to be a reasonable course: they want to write it off.\nWhich both seems suboptimal, and is strange in contrast with historical attitudes to more technical problem-solving. (As highlighted in my dialogue from the start of the post.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It seems to me that if the same degree of can\u2019t-do attitude were applied to technical safety, there would be no AI safety community because in 2005 Eliezer would have noticed any obstacles to alignment and given up and gone home.\nTo quote a friend on this, what would it look like if we *actually tried*?\nConclusion\nThis has been a miscellany of critiques against a pile of reasons I\u2019ve met for not thinking about slowing down AI progress. I don\u2019t think we\u2019ve seen much reason here to be very pessimistic about slowing down AI, let alone reason for not even thinking about it.\nI could go either way on whether any interventions to slow down AI in the near term are a good idea. My tentative guess is yes, but my main point here is just that we should think about it.\nA lot of opinions on this subject seem to me to be poorly thought through, in error, and to have wrongly repelled the further thought that might rectify them. I hope to have helped a bit here by examining some such considerations enough to demonstrate that there are no good grounds for immediate dismissal. There are difficulties and questions, but if the same standards for ambition were applied here as elsewhere, I think we would see answers and action.\nAcknowledgements",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Thanks to Adam Scholl, Matthijs Maas, Joe Carlsmith, Ben Weinstein-Raun, Ronny Fernandez, Aysja Johnson, Jaan Tallinn, Rick Korzekwa, Owain Evans, Andrew Critch, Michael Vassar, Jessica Taylor, Rohin Shah, Jeffrey Heninger, Zach Stein-Perlman, Anthony Aguirre, Matthew Barnett, David Krueger, Harlan Stewart, Rafe Kennedy, Nick Beckstead, Leopold Aschenbrenner, Micha\u00ebl Trazzi, Oliver Habryka, Shahar Avin, Luke Muehlhauser, Michael Nielsen, Nathan Young and quite a few others for discussion and/or encouragement.\nNotes\n1\nI haven\u2019t heard this in recent times, so maybe views have changed. An example of earlier times:\nNick Beckstead, 2015\n: \u201cOne idea we sometimes hear is that it would be harmful to speed up the development of artificial intelligence because not enough work has been done to ensure that when very advanced artificial intelligence is created, it will be safe. This problem, it is argued, would be even worse if progress in the field accelerated. However, very advanced artificial intelligence could be a useful tool for overcoming other potential global catastrophic risks. If it comes sooner\u2014and the world manages to avoid the risks that it poses directly\u2014the world will spend less time at risk from these other factors\u2026.\nI found that speeding up advanced artificial intelligence\u2014according to my simple interpretation of these survey results\u2014could easily result in reduced net exposure to the most extreme global catastrophic risks\u2026\u201d\n2\nThis is closely related to Bostrom\u2019s",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Technological completion conjecture:\n\u201cIf scientific and technological development efforts do not effectively cease, then all important basic capabilities that could be obtained through some possible technology will be obtained.\u201d (Bostrom,\nSuperintelligence\n, pp. 228, Chapter 14, 2014)\nBostrom illustrates this kind of position (though apparently rejects it; from Superintelligence, found\nhere\n): \u201cSuppose that a policymaker proposes to cut funding for a certain research field, out of concern for the risks or long-term consequences of some hypothetical technology that might eventually grow from its soil. She can then expect a howl of opposition from the research community. Scientists and their public advocates often say that it is futile to try to control the evolution of technology by blocking research. If some technology is feasible (the argument goes) it will be developed regardless of any particular policymaker\u2019s scruples about speculative future risks. Indeed, the more powerful the capabilities that a line of development promises to produce, the surer we can be that somebody, somewhere, will be motivated to pursue it. Funding cuts will not stop progress or forestall its concomitant dangers.\u201d\nThis kind of thing is also discussed by\nDafoe\nand\nSundaram, Maas & Beard\n3\n(Some inspiration from Matthijs Maas\u2019\nspreadsheet\n, from\nPaths Untaken\n, and from GPT-3.)\n4\nFrom a private conversation with Rick Korzekwa, who may have read\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC1139110/",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "and an internal draft at\nAI Impacts\n, probably forthcoming.\n5\nMore\nhere\nand\nhere\n. I haven\u2019t read any of these, but it\u2019s been a topic of discussion for a while.\n6\n\u201cTo aid in promoting secrecy, schemes to improve incentives were devised. One method sometimes used was for authors to send papers to journals to establish their claim to the finding but ask that publication of the papers be delayed indefinitely.26,27,28,29 Szil\u00e1rd also suggested offering funding in place of credit in the short term for scientists willing to submit to secrecy and organizing limited circulation of key papers.30\u201d \u2013\nMe, previously\n7\n\u2018Lock-in\u2019 of values is the act of using powerful technology such as AI to ensure that specific values will stably control the future.\n8\nAnd also\nin Britain\n:\n\u2018This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI\u2026the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI\u2019s development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.\u2019\n9\nOr so worries Eliezer Yudkowsky\u2014\nIn\nMIRI announces new \u201cDeath With Dignity\u201d strategy\n:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u201c\u2026 this isn\u2019t primarily a social-political problem, of just getting people to listen.\u00a0 Even if DeepMind listened, and Anthropic knew, and they both backed off from destroying the world, that would just mean Facebook AI Research destroyed the world a year(?) later.\u201d\nIn\nAGI Ruin: A List of Lethalities\n:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u201cWe can\u2019t just \u201cdecide not to build AGI\u201d because GPUs are everywhere, and knowledge of algorithms is constantly being improved and published; 2 years after the leading actor has the capability to destroy the world, 5 other actors will have the capability to destroy the world.\u00a0 The given lethal challenge is to solve within a time limit, driven by the dynamic in which, over time, increasingly weak actors with a smaller and smaller fraction of total computing power, become able to build AGI and destroy the world.\u00a0 Powerful actors all refraining in unison from doing the suicidal thing just delays this time limit \u2013 it does not lift it, unless computer hardware and computer software progress are both brought to complete severe halts across the whole Earth.\u00a0 The current state of this cooperation to have every big actor refrain from doing the stupid thing, is that at present some large actors with a lot of researchers and computing power are led by people who vocally disdain all talk of AGI safety (eg Facebook AI Research).\u00a0 Note that needing to solve AGI alignment only within a time limit, but with unlimited safe retries for rapid experimentation on the full-powered system; or only on the first critical try, but with an unlimited time bound; would both be terrifically humanity-threatening challenges by historical standards individually.\u201d\n10",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I\u2019d guess real Luddites also thought the technological changes they faced were anti-progress, but in that case were they wrong to want to avoid them?\n11\nI hear\nthis\nis an elaboration on this theme, but I haven\u2019t read it.\n12\nLeopold Aschenbrenner\npartly defines \u2018Burkean Longtermism\u2019 thus: \u201cWe should be skeptical of any radical inside-view schemes to positively steer the long-run future, given the froth of uncertainty about the consequences of our actions.\u201d",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "You know it must be out there, but you mostly never see it.\nAuthor's Note 1:\nThese days, my thoughts go onto\nmy substack\ninstead of onto LessWrong. Everything I write becomes free after a week or so, but it\u2019s only paid subscriptions that make it\npossible\nfor me to write. If you find a coffee\u2019s worth of value in this or any of my other work, please consider signing up to support me; every bill I can pay with writing is a bill I\ndon\u2019t\nhave to pay by doing other stuff instead. I also accept and greatly appreciate\none-time donations\nof any size.\nAuthor\u2019s Note 2:\nThis essay is not intended to be revelatory.\u00a0 Instead, it\u2019s attempting to get the\nconsequences\nof a few very obvious things\nlodged into your brain,\nsuch that they actually occur to you from time to time as opposed to occurring to you approximately never.\nMost people could tell you that 17 + 26 = 43 after a few seconds of thought or figuring, and it would be silly to write an essay about 17 + 26 equaling 43 and pretend that it was somehow groundbreaking or non-obvious.\nBut!\u00a0 If the point was to get you to\nsee the relationship\nbetween 17, 26, and 43 very, very clearly, and to\nremember it sufficiently well\nthat you would reflexively think \u201c43\u201d any time you saw 17 and 26 together in the wild\n,\nit might be worth taking the time to go slowly and say a bunch of obvious things over and over until it started to stick.\nThanks to Karim Alaa for the concept title. \u00a0If you seek tl;dr, read the outline on the left and then skip to IX.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I. #MeToo\nIn September of 2017, if you had asked men in the United States \u201cwhat percentage of the women that you personally know have experienced sexual assault?\u201d most of them would have likely said a fairly low number.\nIn October of 2017, the hashtag #MeToo went viral.\nIn November of 2017, if you had asked men in the United States \u201cwhat percentage of the women that you personally know have experienced sexual assault?\u201d most of them would have given a much higher number than before.\n(It\u2019s difficult, for many people, to\nremember\nthat they would have said a number that we now know to be outrageously low; by default most of us tend to project our present knowledge back onto our past selves.\u00a0 But the #MeToo movement was sufficiently recent, and the collective shock sufficiently well-documented, that we can, with a little bit of conscientious effort, resist the mass memory rewrite.\nMost of us were wrong.\nThat\u2019s true even if you specifically were, in fact, right.\n)\nTalking about sexual assault is not\nquite\nas taboo, in the United States, as it is in certain other cultures.\u00a0 There are places in the world where, if a woman is raped, she might well be murdered by her own family, or forcibly married off to the rapist, or any number of other horrible things, because the shame and stigma is so great that people will do almost anything to escape it.\n(There are places in the world where, if a man is raped\u2014\nwhat are you talking about? Men can\u2019t be raped!)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The U.S. is not quite that bad.\u00a0 But nevertheless,\nespecially\nprior to October of 2017, sexual assault was still a thing that you Don\u2019t Ever Talk About At The Dinner Table, and Don\u2019t Bring Up At Work.\u00a0 It wasn\u2019t the sort of thing you spoke of in polite company\u2014\n(or even in many cases with friends and confidants, because the subject is so charged and people are deeply uncomfortable with it and there are often entanglements when both parties know the perpetrator)\n\u2014and since there was\npressure to avoid discussing it,\npeople\ntended not to discuss it.\n(\nLike I said, a lot of this will be obvious.\n)\nAnd because people\ndidn\u2019t\ndiscuss it, a lot of people (especially though not always men) were\ngenuinely shocked\nat just how common, prevalent,\npervasive\nit is, once the conversational taboo crumbled.\nThe dark side of this is \u201cbecause sexual assault was taboo to discuss openly, many people were surprised to discover just how many of their friends and family members were victims of it.\u201d\nThe Vantablack side of this is \u201cbecause sexual assault was taboo to discuss openly, many people were surprised to discover just how many of their friends and family members endorse it.\u201d\n(Either by actively engaging in sexual assault themselves, or at least not finding it sufficiently objectionable enough to, y\u2019know.\nObject.)\nII. Me Zero, Me One, Me Three",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In the year 1990, if you had asked people in the United States what percentage of the population was trans, you would have generally gotten back answers like \u201cI dunno, one in a thousand?\u00a0 Less??\u201d\u00a0 Trans folk (as distinct from, say, cross dressers or homosexuals) were much less on the public\u2019s radar in the year 1990; it wasn\u2019t until 1994 that\nAce Ventura: Pet Detective\nmade over $100,000,000 by relentlessly dehumanizing a trans woman.\u00a0 The average person would\nmaybe\nhave been able to conceive of the idea that one person in their entire small town might be trans, which is a far cry from present-day estimates of \u201cone or two people in each graduating class in a mid-sized high school.\u201d\nIn 1950,\nif you could have even gotten away with asking the question,\nmost people in the United States would have said that very, very few people were anything other than heterosexual.\u00a0 Perhaps, thinking of their confirmed bachelor uncle or the lovely old widows sharing the house down the street, they\nmight\nhave whispered something like \u201chalf a percent?\u201d if they were sure that the question wasn\u2019t a trap.\nBut it\u2019s likely that\neven gay people\nin the 1950\u2019s wouldn\u2019t have guessed \u201cone in every thirty adults,\u201d which was the rate of self-report in Gallup polls in 2012, or \u201cone in every fourteen adults\u201d (!) which was the rate in 2022.\u00a0 In 1950, if you had told someone that the would likely pass within nodding distance of\nmultiple\nnon-straight people\nevery single time they went to the grocery store,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "they would have assumed you were either crazy or slandering their hometown.\nThe conservative party line goes something like \u201csee??\u00a0 Once we stopped punishing people for bringing it up, it started happening more and more and now it\u2019s everywhere!\u201d\nAnd this is at least somewhat true, with regards to\nbehavior.\nTo how many people with a given inclination\nexpress\nthat inclination, through action (especially out in the open where everyone can see, as opposed to furtively, in private).\nBut there wouldn\u2019t have needed to\nbe\nso much glaring and shushing, if the proportion of the population with the inclination were vanishingly small.\nSocieties do not typically wage intense and ongoing pressure campaigns to prevent behavior that 0.0001% of people want to engage in.\nFor as long as there have been classrooms with thirty students in them, there have been two or three queer folk in those classrooms\u2014they simply didn\u2019t\ntalk\nabout it.\n(Just like we didn\u2019t talk about sexual assault.\u00a0 It\u2019s not\nnew.\nIt\u2019s just newly un-hidden.)\nIII. The Law of Prevalence\nThis\u2014\n(transness, queerness, sexual assault)\n\u2014is\nsocial dark matter\n.\u00a0 If you\u2019re both paying attention and also thinking carefully, you can conclude that it must be out there, and that there's probably actually quite a lot of it.\u00a0 But you don\u2019t see it.\u00a0 You don\u2019t hear it.\u00a0 You don\u2019t interact with it.\n(Unless you are a member of the relevant group, and members of that group in fact talk\nto each other,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "in which case you may have a somewhat less inaccurate picture than all of the oblivious outsiders.\u00a0 Many women were not shocked at the information that came out during #MeToo.\u00a0 Shocked\nthat\nit came out, perhaps, but not shocked by the stories themselves.)\nI\u2019d like to present two laws surrounding social dark matter in this piece, and at this point we\u2019re ready for the first one:\nThe Law of Prevalence:\nAnything people have a strong incentive to hide will be many times more prevalent than it seems.\nStraightforward, hopefully.\u00a0 People hide things they are encouraged to hide; people do not share things they are punished for sharing.\n(\nTwice\nnow, I have been told by a dear, close friend that they had developed and recovered from serious alcoholism, without me ever even knowing that they drank.\u00a0 Both friends formed the problem in private, and solved it in private, all without giving any outward sign.)\nHowever, while almost anyone would nod along with the above, people do not often stick around long enough to do the final step, and\napply\nit.\nHere are some examples of things that people are (or were) encouraged to hide, and nonzero punished for sharing or discussing or admitting to (starting with ones we\u2019ve already touched on):\nAlcoholism/alcohol abuse\nSexual assault\nHomosexuality (less so than two generations ago, but still)\nTransness (less so than a generation ago, sort of?\u00a0 It\u2019s depressingly complicated.)\nLeft-handedness (much less so than five generations ago)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Being autistic (much less so than a generation ago)\nBeing into BDSM or polyamory (somewhat less so than a generation ago)\nLiking the score from the movie \"Titanic\" as a seventh-grade boy in N.C in 1998.\nWanting to divorce your partner or thinking that your relationship is toxic/bad (commonplace now, was almost unspeakable five generations ago)\nHaving some small, deniable amount of Japanese heritage (in the United States, during WWII)\nHaving some small, deniable amount of black or brown heritage (in the United States, among other places, especially a few generations ago)\nHaving some small, deniable amount of Jewish heritage (in Europe, among other places)\nHaving some small, deniable amount of Dalit heritage (in India, among other places)\nNot feeling overwhelmingly positive feelings about your infant or toddler, or feeling active antipathy toward your infant or toddler, or frequently visualizing murdering your infant or toddler\nAgnosticism or atheism (in religious communities)\nFaith or \u201cwoo\u201d (in atheistic communities)\nDrug use (especially hard drugs)\nBeing a furry\nCross dressing/drag\nProviding or availing oneself of sex work\nPsychopathy or borderline personality disorder\nHearing voices or having hallucinations\nBeing on the verge of breakdown or suicide\nHaving violent thoughts or urges\nHaving once beaten a partner, or a child, or a dog\nMasturbating to fantasies of rape or torture\nHaving HIV\nBeing sexually interested in children, or family members, or animals",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Actually having sex with children, or family members, or animals\nHaving abortions (in red enclaves) or thinking abortion should be banned (in blue ones)\nHaving committed felonies\n\u2026 this is not an exhaustive list by any means.\u00a0 But it\u2019s worth pausing for a moment, for each of these, and\nactually applying\nthe Law of Prevalence to each, and realizing that there\u2019s\nreally quite a lot more of that going on than you think.\nNo, really.\nSeriously.\nYour\nperceptions\nof how rare something like incest is are\nsystematically\nskewed by the fact that virtually everyone engaging in it is\nalso\nengaging in a fairly effortful (and largely successful) campaign to prevent you from ever finding out about it. \u00a0Ditto everyone who habitually uses opium, or draws tentacle hentai to decompress after work, or thinks Jar-Jar Binks was what made\nThe Phantom Menace\na truly great movie.\nThe punishment our culture levied on left-handed people was fairly tame, relative to (say) the punishment our culture levies on pedophiles (even those who have never abused anyone).\u00a0 Yet even\nthat\npunishment was enough to hide/suppress\nseventy-five percent\nof left-handedness!\u00a0 Three in four left-handers or would-be left-handers were deterred, in the year 1900; incidence went up from 3% to 12% once the beatings began to slow\n[1]\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It\u2019s not crazy to hazard that some of the more-strongly-stigmatized things on the list above have incidence that\u2019s 10x or even 100x what is readily apparent, just like the number of trans folk in the population is\nwildly\ngreater than the median American would have guessed in the year 1980.\nIV. Interlude i: Nazis\nThere\u2019s something of a corollary to the Law of Prevalence: when the stigma surrounding a given taboo is\nremoved\n(or even just meaningfully decreased), this is almost always followed by a massive uptick in apparent prevalence.\nA very straightforward example of this is Nazi and white supremacist sentiment in the United States, Canada, and Europe over the previous decade.\u00a0 The internet is littered with commentary from people saying things like \"but where did all these Nazis even\ncome\nfrom?\"\nIn point of fact, most of them were already there.\u00a0 There has not been a 2x or 5x or 10x increase in\nactual\ntotal white supremacist sentiment\u2014\n(though there probably\nhas\nbeen some modest marginal increase, as more people openly discuss the claims and tenets of white supremacy and some people become convinced of those ideas for the first time, or as the culture war has polarized people to greater and greater forgiveness of those on their side, etc.)\n\u2014instead, what happened is that there has been a sudden rise in\nvisibility.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Prior to 2015, there was a fragile ban on discussing the possible superiority or inferiority of various races, similar to the ban on discussion of sexual assault pre-#MeToo. \u00a0In most venues, it simply Wasn't The Sort Of Thing One Would Mention.\nAnd many, many people, not getting the Law of Prevalence all the way down in their bones, mistook the resulting silence for an\nactual absence\nof white supremacist sentiment.\nBecause they never\nheard\npeople openly proselytizing Nazi ideology, they assumed that practically no one\nsympathized\nwith Nazi ideology.\nAnd then one Donald J. Trump smashed into the Overton window like a wrecking ball, and all of a sudden a bunch of sentiment that had been previously shamed into private settings was on display for the whole world to see, and millions of dismayed onlookers were disoriented at the lack of universal condemnation because\nsurely\nnone of us think that any of this is okay??\nBut in fact, a rather large fraction of people\ndo\nthink this is okay, and have\nalways\nfelt this was okay, and were only holding their tongues for fear of punishment, and only adding their voices to the punishment of others to avoid being punished as a sympathizer. When the threat of punishment\ndiminished,\nso too did their hesitation.\nThe key thing to recognize is that this is (mostly) not an uptick in\nactual\nprevalence.\u00a0 Changes to the memetic landscape can be quite bad in their own right\u2014the world\nis",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "somewhat worse, now that white supremacy is less afraid of the light, and discussions of it are happening out in the open where young and impressionable people can be recruited and radicalized, and the marginal racist is marginally bolder and more likely to act on their racism.\nBut it\u2019s nowhere near as bad as it\nlooks.\nJudging by visibility alone, it\nseems\nlike white supremacy has become 10x or 20x more prevalent than it was a decade ago, but if there had\nactually\nbeen a 10-20x increase in the number of white supremacists, the world would look very, very different.\nOr, to repurpose the words of Eugene Gendlin\n[2]\n: \u201cYou can afford to live in the world where there are this many white supremacists and white supremacist sympathizers among your neighbors and colleagues, because (to a first approximation) you were already doing so.\u201d\n(Ditto anti-Semites, ditto misogynists, ditto pedophiles, ditto people hearing voices, ditto people committing all those perfect crimes that you\u2019ve always been told don\u2019t exist but if you stop and think for twelve seconds you\u2019ll realize that they\ntotally\nexist,\nof course\nthey exist, they\u2019re probably happening\nall the time,\nand yet somehow the world keeps spinning and society doesn\u2019t collapse, it\nis\nworse than you thought but it\u2019s not as bad as you would naively\nexpect,\nDON'T PANIC.)\nV. Interlude ii: The Fearful Swerve\nThere is a substance called shellac that is made from the secretions of the female lac bug, formal name\nKerria lacca.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Out in the wild, shellac looks like this:\nThis substance has many uses.\u00a0 One such use is making the outer shells of jelly beans shiny and hard and melt-in-your-mouth soluble.\nMany people, upon discovering that the jelly beans they have been enjoying all their lives are made with something\nsecreted by a bug\u2014\n(I mean, what a viscerally evocative word!\nSecrete\n.\u00a0 Squelchy.)\n\u2014many people, upon discovering that the jelly beans they have been enjoying all their lives are coated in\nbug secretions\n, experience a\nsudden decrease in their desire to eat them\n.\nThis is the\nfearful swerve.\nHere is (roughly) how it works:\nThe human mind has something like\nbuckets,\nfor various pieces of information, and those buckets have valences, or karma scores.\nFor many people, \u201cjelly beans\u201d live in a bucket that is Very Good!\u00a0 The bucket has labels like \u201cdelicious\u201d and \u201cyes, please!\u201d and \u201cmmmmmmmmmmm\u201d and\n.\n\u201cBug secretions,\u201d on the other hand, live in a bucket that, for many people, is Very Bad\n[3]\n.\u00a0 Its labels are things like \u201cblecchh\u201d and \u201cgross\u201d and \u201cdefinitely do not put this inside your body in any way, shape, or form.\u201d\nWhen buckets collide\u2014when things that people\nthought",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "were in one bucket turn out to be in another, or when two buckets that people thought were different turn out to be the same bucket\u2014people do not usually react with slow, thoughtful deliberation.\u00a0 They usually do not think to themselves \u201chuh!\u00a0 Turns out that I was wrong about bug secretions being gross or unhygienic!\u00a0 I\u2019ve been eating jelly beans for years and years and it\u2019s always been delicious and rewarding\u2014\nbug secretions must be\ngood,\nactually, or at least they\ncan\nbe good!\u201d\nInstead, people tend to react with their guts. \u00a0The bad labels trumps the good ones, the baby goes out with the bathwater, and they reach for a Reese's instead.\n(In the case of shellac specifically, this reaction is often short-lived, and played up for comedic effect; there aren't\nthat\nmany people who give up on them long-term. \u00a0But the toy example is still useful for understanding the general pattern.)\nThis same effect often plays out when humans become suddenly aware of other humans\u2019 social dark matter.\u00a0 In the U.S. of the 1950\u2019s (and, rather depressingly, in many parts of the U.S. of the 2020\u2019s), the discovery that Person A was gay would often\nviolently\nrearrange Person B\u2019s perception of them, overriding (and often over\nwriting\n) all of Person B\u2019s other memories and experiences and conceptions of Person A.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The same thing happens in various parts of the world when people are outed as having been raped, or having had an abortion, or having lost their faith, or having been diagnosed with schizophrenia, or having a habit of taking LSD on weekends, or having a penchant for wearing diapers, or any number of other taboo things.\u00a0 The stereotype of a good and upstanding person is\nincompatible\nwith the stereotype of [dark matter], and rather than make a complicated and effortful update to a more nuanced stereotype, people often simply snap to \u201cwell, I guess they were Secretly Horrible All Along, all of my direct evidence to the contrary notwithstanding.\u201d\nThis is unfortunately complicated by the fact that this update is sometimes actually\ncorrect,\nas many people discovered during #MeToo. \u00a0There are, in fact, dealbreakers, and there are people who present a kind and friendly front while concealing horrific behavior, and one negative revelation can in fact be more important than everything else you knew about a person\n[4]\n.\u00a0 Such occurrences are (sadly) not even particularly rare.\nBut!\u00a0 There\u2019s a pretty damn important distinction between\nthoughts\nor\ninclinations,\non the one hand, and\nactions\non the other.\u00a0 Most of the horror that suburban parents in the 1950\u2019s would have felt upon discovering that their child\u2019s high school English teacher was gay would have been\nwrong\n\u2014wrong in the sense of being incorrect, unfounded, unjustified,\nconfused.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The mere fact of being gay (whilst being otherwise well-behaved and in compliance with all social norms and standards, such that most people never even noticed) is\nnot\na major risk factor for child molestation, and is\nnot\nevidence that Mr. So-and-So was actually a ticking time bomb all along and we simply never knew, thank God we got rid of him before he\nfiddled\nwith somebody all of a sudden after never doing anything of the sort for thirty years.\nThe fearful swerve is a\nreflex.\nIt\u2019s one thing to\nconclude\n, after conscious thought and careful evaluation, that the things you\u2019ve learned about a person mean that they are irretrievably Bad (especially if what you've learned is that they have actually taken harmful action out in the world\n[5]\n).\nIt\u2019s another thing altogether to\nleap\nto that conclusion unthinkingly, based on nothing more than stereotypes and pattern-matching.\u00a0 Deciding that \"welp, jelly beans are off limits now, because I'm an ethical vegan and shellac is an animal by-product\" is very, very different from just going \u201ceww, gross!\u201d and letting that impulse have final say.\nVI. The Law of Extremity\nWhen the categorical bucket \u201cautism\u201d was invented, it was built around people who were what we would now call\nprofoundly\nor\nseverely\nautistic\n[6]\n. \u00a0Many of them were nonverbal.\u00a0 Many of them had extreme tics.\u00a0 They were incapable of navigating ordinary human contexts, and required highly specialized and intensive assistance to survive even at home or in institutions.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Over time, the medical establishment began to pick up on autistic-spectrum traits in people whose support needs were not\nquite\nso great.\u00a0 In 1988, the movie\nRain Man\nintroduced the word \u201cautistic\u201d to the broader English-speaking public, and for a couple of decades Dustin Hoffman\u2019s character Raymond was the go-to archetype of a person with autism. \u00a0Yes, he was disabled, but he could talk, he could walk, he could feed himself, and not\nevery\nsurprising stimulus triggered a meltdown.\nIn 2010, as a middle school teacher, I encountered my own first bona fide, formally diagnosed autist, in the form of a student named Ethan.\u00a0 Ethan was fully capable of holding up his end of a completely normal conversation (though he much preferred that conversation be about\nSpore\nor\nMinecraft,\nand was not shy about saying so).\nOn December 19th, 2021, I ended a months-long process of discovering that\nI\nam autistic.\u00a0 These days, somewhere between 10% and 25% of my social circle wears that label as part of their identity.\nThe Law of Prevalence states that anything people have a strong incentive to hide will be many times more prevalent than it seems.\nPart of what makes this\npossible at all\nis the Law of Extremity:\nThe\nLaw of Extremity\n: if part of the incentive to hide something is that the cultural narrative treats it as damning, dangerous, or debilitating, the median and modal\ninstances\nof that thing will be many times less extreme than that narrative would have you believe.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The prevalence of autism in 1988 was\nbasically\nthe same as it is today, i.e. really quite startlingly common, overall less rare than red hair.\u00a0 Yes, there are some reasonable hypotheses that certain environmental changes (\nnot\nvaccines) are leading to an actual ground-level increase in the rate of autism, but if so, we\u2019re talking about something like a 1.5x-3x increase, not a 10x or 20x or 50x increase.\nBut this claim\u2014that something close to three percent of the population is autistic\u2014would sound\nabsolutely crazy\nto your everyday moviegoer leaving the theater in 1988 after watching\nRain Man.\nIt would sound crazy because that moviegoer\u2019s\npicture of autism\nwas rooted in an unusually extreme example of what an autistic person looks like. They would think you were claiming that one in thirty-three adults was Raymond Babbit, which is patently false.\nThe 1988 counterparts of [me] and [my autistic friends] and [my student Ethan] weren\u2019t\nquite\ntrying to hide their autism in the way that people actively try to hide things like psychopathy and pedophilia (though they probably\nwere\nspending a ton of personal resources in an attempt to seem more normal than they were on the inside).\nBut our 1988 counterparts were able to fly under the radar\nat all\nbecause everyone else's autism-detectors were calibrated on much more extreme and pathological\nvariants\nof the traits that they themselves carried.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "When I wrote a public Facebook post letting my social circle know that I am autistic, a friend of mine replied with the following (abridged):\nI've been uncomfortable with this label myself because this one time I went to a grocery store (to buy a bottle of a specific dish soap to make giant soap bubbles) and there was this guy meticulously re-stacking the product who I just ignored because I thought he was a store employee but then he grunted and shoved my hand away when I tried to take one. An older fellow identifying himself as the man's care taker showed up and had to explain the situation. Then I had to drive to another store to get that specific dish soap.\nBut, anyway, my point is, neither of us are like that guy, and that's the sort of case I thought the psychiatric label was designed for.\nTo which my partner\n@LoganStrohl\nreplied:\nbecause the characteristics you're mostly likely to observe in someone are the easy-to-see-from-the-outside ones (unusual social behaviors, motor problems, speaking problems), you'll probably by default see people with extreme versions of these more visible characteristics as \"more autistic\".",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "but there are autistic people who can neither walk nor talk but are unusually good *for a median human* at flirting. there are autistic people who have severe world-shattering meltdowns daily from sensory overload yet have built such convincing masking and coping mechanisms that their bosses do not know they're anything besides \"a bit quirky\". there are autistic people who react so violently and explosively to perceived rule violations that they need constant supervision to protect others and themselves, yet they find life in the middle New York City comfortable and exciting rather than overwhelming.\ni think there is a thing going on with intelligence, where people with higher intelligence tend to succeed more at going about normal life than people with lower intelligence. if two people share basically the same innate tendencies wrt information processing style, social processing architecture, sensory processing ability, etc.; but one is IQ 95 while the other is IQ 160; then the IQ 95 person may seem like a total disaster who can't dress himself let alone hold down a job, while the IQ 160 person has stable employment, a mortgage, and lots of friends who appreciate his quirky interests and moral passion. one of these people isn't more autistic than the other; they're just more *disabled* than the other.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "there's a very similar effect from support. imagine two very similar autistic people again. one of them is born into a wealthy progressive family, they go to the same school and live in the same town the whole time they're growing up, their parents organize a lot of structured interest-based socialization for them, and then they go to college where food is provided by resident services in the cafeteria while they study computer science until they move on to the Google campus. the other person grows up moving from foster home to foster home in a conservative area where differences are seen as threatening, and they're considered a problem child and put through ABA therapy that punishes their coping strategies and tries to cure their disorder. the first person will not stick out nearly as much to someone looking for obvious external signs of autism.\ngiven that Duncan is both very intelligent and has been very well supported in the relevant ways throughout his life, then on the hypothesis that he is autistic, we should expect his internal experiences and personal struggles to be WAY more similar to those of supermarket guy than they appear (much more similar to those of supermarket guy than to those of a randomly chosen person)\n\u2026which is in fact true; I was\nimmediately\nable to recognize and describe several ways in which the supermarket guy seemed to be something like a less-well-contained version of myself.\nThe first autistic people to\nrise to the level of attention",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "were those who:\nHad such\nintense\nversions of the traits that they outshone everything else\nLacked the support structures and coping strategies that allow people with the traits to overcome them and\nappear\nto not-have-them.\nAnd thus, when the stereotype (/archetype) of autism began to crystallize, it crystallized with those people at the center, when in fact they were well to the right side of the bell curve.\nThis (claims the Law of Extremity) is approximately\nalways\ngoing to be true.\u00a0 It doesn\u2019t matter if you\u2019re talking about autism or homosexuality or sexual assault or drug use\u2014when it comes to social dark matter, what you hear about on the news and see depicted in movies and shows will\nalways\nbe an unusually vivid version of [whatever it is].\u00a0 Someone who couldn\u2019t hold it together, couldn\u2019t keep up the facade\u2014someone whose [thing] is so intense, or whose self-control is so tattered, that they can\u2019t keep it under wraps like everybody else\n[7]\n.\nA term-of-art in this domain is\nrepresentativeness;\nthere\u2019s a difference between the\nmost memorable/most recognizable\nversion of a thing, and the actual most common or typical example.\u00a0 The\u00a0classic archetype of a basketball player is a thin, muscular, dark-skinned man, pushing seven feet tall and wearing a jersey with a number on it.\u00a0 But the vast majority of the people actually playing basketball at any given moment do not fit this description\n[8]\n.\nYou could call it the Law of Caricature, perhaps.\u00a0 The\npicture",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "of a drug user is a lank, twitchy, pale young man with greasy hair and disheveled clothes and wild, suspicious eyes\u2014but most drug users look nothing like that.\u00a0 A\nNew Yorker\ncartoon depicting a Jew might have characteristic long braids and dark clothes and a yarmulke and a prayer shawl with a star of David\u2014but most Jews\ndo not look like that\n, most of the time.\n(If #MeToo taught us anything, it taught us that you can't identify rapists and molesters simply by\nlooking for people who \"look like\" rapists and molesters.\n)\nJust as 95+% of people who play basketball look nothing like LeBron James, so too is it obvious (if you pause and actually take the time to add 17 to 26) that 95+% of psychopaths aren\u2019t serial killers and 95+% of pedophiles aren\u2019t child molesters and 95+% of drug users have never mugged a stranger on the street to pay for their next hit.\n(\"Even if most Dark Wizards are from Slytherin, very few Slytherins are Dark Wizards. There aren't all that many Dark Wizards, so not all Slytherins can be one.\")\nThe\nperception\nthat all of these traits are necessarily extreme and dangerous is explained by the fact that the\nvisible\nexamples\nof [psychopaths] and [pedophiles] and [drug users] are all drawn from the extreme end of the distribution.\u00a0 When the penalty for a certain kind of dark matter is steep enough, the only individuals you notice are the ones who\ncannot escape notice,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "and if you don't keep your brain on a fairly short leash, it will naturally default to a corresponding belief that the entire population of [psychopaths] and [pedophiles] and [drug users]\ncenters\non those individuals.\nBut in all likelihood, for every person wandering the street raving about freemasons, there are literally dozens who hear voices from time to time and never unravel enough that they even get diagnosed.\n(Or, to put it another way: you can\u2019t confidently deduce an\nabsence\nof people-hearing-voices in a given population from an absence of people raving on the street, just like you shouldn\u2019t be confident that no one is playing basketball because none of them are seven feet tall.)\n(Similarly, #MeToo showed us that it's a mistake to conclude that none of the people in our lives have been sexually assaulted simply because none of them are suffering nightmares and panic attacks and having trouble holding down a job and conspicuously avoiding being in the same room as their boss.\nMost\nrape victims don\u2019t look anything like the training data our society handed us.)\nVII. Surely You\u2019re Joking, Mr. Sabien\nI once got into a protracted disagreement with a colleague about the prevalence of a specific form of social dark matter.\u00a0 I used eight different independent lines of reasoning to try to reach an estimate of how commonplace it might be, each of which gave me back an answer somewhere in the ballpark of 0.5% - 5%.\n(Several of the lines of reasoning were\nvery",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "crude/coarse/rudimentary, such that if any one of them had been my sole source of intuition I would have been embarrassed to proffer my guess, and might have preferred to say \u201cI really don\u2019t know.\u201d\u00a0 But eight unrelated crappy methods all giving answers in the same order of magnitude felt like real reason for confidence.)\nMy colleague did not buy it\n[9]\n.\u00a0 They were insistent that prevalence had to be well below one in a thousand.\u00a0 They were someone in whom people frequently confided, on a fairly wide range of issues\u2014\"I\u2019ve been told equally damning things, multiple times; I find it hard to believe I would have simply\nnever\nheard this particular one, from anyone, unless it is in fact vanishingly rare.\"\nAnd yet\nI\nhad received (and continued to receive) confidences from\u00a0multiple such people\u2014more than one of whom was also close to my colleague and yet chose not to extend their trust in that direction.\nThis is no accident; most people\nsystematically\ndrive away such confidences (and I work rather hard, and rather deliberately, to earn them).\u00a0 The driving-away is usually not intentional; it\u2019s a vibe thing.\u00a0 It comes across in offhand comments and microexpressions and reflexive, cached responses that the speaker will hardly even notice themselves saying (because they\u2019re so self-evidently true).\nIf you walk around aggressively signaling the equivalent of \u201cWhat!?? There ain\u2019t no faggots in\nthis\ntown,\u201d then it should be no surprise to you that your\nlived experience\nis that\nnone",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "of the people around you are gay (even though base rates imply that quite a few of them are).\nIf you exude \u201cwe are all brothers and sisters in Christ,\u201d then you will generally not hear from atheists (except the unusually militant and confrontational ones), and you will likely underestimate their prevalence in your religious community.\n(And you may also underestimate the prevalence of nonbinary folk in your immediate vicinity!\u00a0 The thing that tells someone\u2019s gut that\nyou are not safe to confide in\nis often indirect or subtle, and speaking always of \u201cbrothers\u201d and \u201csisters\u201d and never of \u201csiblings\u201d or \u201cchildren\u201d is exactly the sort of subtle signal that puts the animal instincts inside of a marginalized person on high alert.)\nI have personally been in rooms where someone was trying to make a small accommodation for [some trait X], or push for being less alienating and offputting to [Xers], and heard colleagues push back, not merely against the proposed changes, but against\neven discussing\nthem, arguing \"look, this is all hypothetical, it's not like any of us here are actually [X], why are we even spending time on this?\"\nwhen I knew for a fact that at least one other person in the room was secretly [X].\nDwight\n: Okay, now who wrote\nthis\nhysterical one? 'Anal fissures'?\nKevin\n: That's a real thing.\nDwight\n: Yeah, but no one\nhere\nhas it.\nKevin\n:\n(softly) ...\nsomeone has it.\nIf you loudly assert that\nof course\nnone of the people in\nyour",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "circle are suppressing deeply violent fantasies, all of\nyour\nassociates are\ngood people\n\u2014then the good people around you who have their deeply violent fantasies well in hand are unlikely to disabuse you. If you would never hear from a pedophile in the world where there are none around you, and you would\nalso\nnever hear from a pedophile in the world where there were secretly a dozen in your social circle,\nthen you can't use the fact of having never heard from one to\ntell you which world you're in.\nIt's a bit\ndangerous, epistemically\n, to claim that a\nlack\nof observations of a thing constitutes\nevidence\nfor that thing,\nbut in the case of social dark matter, it really can be\n.\nThere's a spectrum ranging from \"I will actively harm white ravens if I see them\" to \"I will actively reward white ravens if I see them,\" and where a would-be observer sits on that spectrum strongly influences how many white ravens will allow themselves to be noticed in the first place.\n\"I won't actively harm white ravens, necessarily, but I react to claims of their prevalence with open incredulity and scoff at people who claim to see them regularly and outright ignore arguments that we should expect a lot of them on priors and also by the way I sometimes talk about how white ravens are kinda bad maybe, and make faces whenever they're brought up\" is a lot closer to the former than the latter. You don't want to fall into conspiracy-theory type thinking, but you don't have to observe all\nthat",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "many white ravens surreptitiously blacking their feathers with coal dust before you learn to roll your eyes at the people running around insisting that 99.9% of all ravens are definitely black (\"and thank goodness for that!\").\n(After all, what does one have to\ngain\nby saying something incendiary like \"I was introduced to sex by an adult, as a child, and I think that this was straightforwardly good for me, actually\"? The few times I\nhave\nseen an otherwise-apparently-happy-and-well-adjusted adult make a credible claim along those lines, they were subsequently screamed at, excoriated, gaslit, condescended-to, told that they were a victim, told that they were stupid, told that they were sick, told that they were evil, told that they were enabling child molesters, interrogated and pressured to give up the name of whoever-it-was, kicked out of various social and professional circles, and generally treated as liars or mocked for how they are\nobviously\nin denial about their own suffering and brokenness (their stable, fulfilling adult lives and livelihoods and relationships apparently notwithstanding). \u00a0And given that this is the obvious, predictable-in-advance outcome of any such disclosure,\nmost\npeople who might be in a position to say such a thing\nsimply don't bother\n. \u00a0To the extent that these people are out there, they have no incentive whatsoever to disillusion the rest of us. \u00a0Which is bad news for those of us who want to understand the\nactual\nstate of affairs!)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "You also don't have to observe all that many furtive white ravens to develop a strong default mistrust of the things that people\nconfidently claim to know\nabout the\nproperties\nof white ravens (since all of their conclusions are drawn from an extreme, impoverished, and wildly non-representative data set). \u00a0It's like trying to learn about the effects of various drugs by studying\nonly\nindividuals in court-ordered rehab. \u00a0The people who are most emphatic about knowing precisely how awful X is tend to be the very same ones who plug their ears and go\nlalalalala\nwhenever anyone pops up with counterevidence.\n(Which, again, isn't to say that X is secretly\ngood,\nnecessarily. \u00a0Just that the current situation\ndoesn't let you tell one way or the other\n. \u00a0If nobody ever talked about sex\nat all,\nand so you based your entire understanding of human sexuality on the study of convicted rapists, you would likely have some pretty weird beliefs about sex-writ-large, even if all of your\nindividual\nbits of evidence were true and accurate.)\nVIII. The Law of Maybe Calm The Fuck Down\nOkay, that one\u2019s not actually a law.\u00a0 But it\u2019s the right\nprescription,\nin the sense of being the correct action to take a majority of the time.\nCore claim\n: If you were a median American in the United States in the year 1950, and you discovered that your child's teacher or coach or mentor was secretly in a homosexual relationship with another adult, you would very likely have felt something like\npanic.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The median American in that era would have concluded that the teacher was\nobviously\na clear and present danger, and taken immediate steps to protect their child\nfrom\nthat danger, and the resulting fallout would have likely completely derailed that person's life.\nAnd this (I further claim) would almost certainly have been\nbad\u2014\na moral tragedy, a miscarriage of justice\u2014and at the root of the badness would have been people being\nreally really wrong, as a question of simple fact,\nabout what was implied and entailed by the mere presence of [homosexuality\n[10]\n].\nDitto if someone adopted the Socially Appropriate Level of Panic about, say, a female neighbor in the 1600s after spotting her stewing herbs in the moonlight.\nDitto if someone adopted the S.A.L.o.P about their neighbor in California in 1942, after discovering that they were not, as had been assumed, one-eighth Korean, but actually one-eighth Japanese.\nSince our moral evolution has\nprobably\nnot yet brought us to a state of overall perfect judgment, it seems quite likely to me that there are dozens of other tragedies of similar form taking place\nright now, in today's society\n\u2014conclusions being drawn, and consequences being meted out, which are overwhelmingly approved-of but which future generations will (correctly) assess as having been wildly unjustified and excessive and wrong.\n(That\u2019s the main thing I\u2019d like to see happen less.\u00a0 I'm\u00a0not exactly impressed with how the machine handles\nactual",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "rapists and racists and serial killers and child molesters, either, but I feel much less urgent about improving the situation for those people; even in situations where their actions weren't exactly choices and they\u2019re unfortunately subject to forces beyond their control, the majority of my sympathy and protective energy nevertheless goes to their victims and their potential future victims.)\nThe casus belli of this essay is \"can we maybe\nnot,\nthen?\" and this final section is about how.\nMuch of the subsequent advice will be incomplete, and useful only if laid atop some preexisting base of skill and motivation. \u00a0I'm going to try to gesture at the shape of the problem, and in the general vicinity of (what I suspect are) the solutions, but the actual nitty-gritty is up to you, and will require nonzero mental dexterity and field-ready metacognitive competence. \u00a0I\u2019ll be recommending a class of actions that are only available to you if you are at all capable of e.g.\nnoticing a thought while it\u2019s still forming\n, or\nrecognizing the difference between an observation and an interpretation\nof\nthat observation\n, or pausing and counting to ten and having the fact that you paused and counted to ten\nactually\nbreak\na runaway cascade of thoughts\n.\nI've sketched out The Bad Thing above in brief, but to add a little more detail, here's the problem as I understand it:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Your brain receives evidence that Person A has Trait X, which is social dark matter and is widely believed to be virulently awful.\nThis triggers a preexisting mental scheme that your society has helpfully gifted you, causing your brain to\ninterpret\nits observation that [Person A has Trait X] as\nmeaning\nthat [Person A has done, in the past, or inevitably will do, in the future, whatever awful thing\n[11]\nis characteristic of Trait-X-ers]. \u00a0In most cases, your brain does this without noticing that it is making any intuitive leaps at all\n[12]\n, just as you aren't usually consciously aware of the step between [observing certain specific patterns of shimmering light] and [recognizing that what you are observing is an instance of the class \"fish\"].\nYour brain, thus believing itself to be in the presence of a Clear And Tangible Threat, offers you up some form of Panic, so as to fill you with motive energy to Do What Must Be Done, which in most cases includes...\n...some form of \"outing\" the person as a Trait-X-er, whether via internet megaphone or whisper network or a call to the police or the rousing of an angry mob.\n(This part does not usually\nfeel\nlike outing someone\u2014from the inside, it feels more like Doing Your Due Diligence, or Warning The Potential Victims, or Making Sure Everyone Is Aware, or perhaps Letting Everyone Know They've Been\nLied\nTo, For\nYears",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": ". \u00a0In other words, there's usually some amount of righteousness or sense-of-responsibility, a feeling of engaging in a public good or protecting the vulnerable or fulfilling your civic duty.)\nAs the meme spreads, each person who comes into contact with it does their very own fearful swerve, and Person A's life is ultimately made substantially worse (they are fired, institutionalized, jailed, evicted, shunned, blacklisted, etc).\nIt seems to me that there are two basic strategies for replacing this story with a more moral, less wrong one. \u00a0One of them is sort of clunky and brute-force-y, but it has the virtue of being a thing you can do once, in the course of a single afternoon, and then be done with. \u00a0The other is more integrated, and has some ongoing attentional costs, but the particular kind of attention required\nhappens to be useful\nfor a bunch of other things and so might be worthwhile anyway.\nThe brute force strategy is to do something like\nwrite down\nand\nconsciously consider\nsome ten or twenty or thirty important flavors of dark matter, and to meditate on each for five minutes and figure out your personal policy around each of them. For example:\nWhat do I\nactually believe\nabout the base rate of suppressed rage in the general population, and why do I believe it? \u00a0How suppressible, in a fundamental sense, do I think rage\nis,\nand why? \u00a0What would I see if I were wrong? \u00a0What things do my current beliefs\nrule out,\nand are any of those things happening?\nWhat\nbehaviors",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "on the part of someone with suppressed rage are dealbreakers, assuming that the mere existence of (successfully) suppressed rage is not?\nWhat other traits are comorbid with suppressed rage turning out to be A Problem, Actually? \u00a0On the flip side, what other traits, found alongside suppressed rage, would be substantially reassuring?\nWhat events could I observe, along the lines of \"if their suppressed rage\nwere\nsomething I should actually be concerned about, I would have seen them doing X, Y, or Z; if I\nhaven't\never observed X, Y, or Z, then it's probably fine\"? (See also:\nSplit and Commit\n)\nIn what specific circumstances that are\nun\nlike all my previous experiences of them would I expect my preexisting models to fail? \u00a0What are the boundaries of my \"training distribution,\" and what lies outside of them? \u00a0For instance, have I already seen them drunk, and if so, did they seem meaningfully scarier or less stable, or was it actually just fine?\nIn short: to what extent, and for what reason, do you endorse\nactually\ndistrusting people who (it turns out) are carrying around vast quantities of hidden rage, or drug users whose drug use is invisible to outside observers, or perverts whose perversion has remained confined to their fantasies for decades, or people with an utter absence of empathy whose outward behavior has nevertheless passed everyone's emotional Turing tests?\n(By the way, the point here is not to presuppose the answer\u2014perhaps you\ndo",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "still endorse distrusting them! \u00a0Different people have different thresholds for risk, after all, and different people assign different moral weights to various harmful behaviors. \u00a0The thing I am advocating against is joining lynch mobs by reflex and then absolving yourself of responsibility because you were merely following the orders of your culture. \u00a0It's every moral person's job to do better than what was handed to them, even if only by a little, and\nactually bothering to try to answer this question at all\nis a damn good start.)\nOne hugely beneficial side-effect of thinking through specific kinds of dark matter that are likely pretty prevalent in the population is that it gets the initial shock out of the way. When you believe yourself to live in a world where no one around you regularly visualizes murdering their coworkers in grim detail, it is\nunderstandably unnerving\nto suddenly discover that someone on your team does!\nAnd in the resulting shock, confusion, and panic, it's easy to succumb to a generic, undirected\npressure-to-act.\nIt feels like an emergency. \u00a0It feels like you're in\ndanger.\nIt feels like something must be done.\n(And it's easy to\nforget\nthe Law of Prevalence, and that probably\nseveral\nof your coworkers have such thoughts, and it's easy to forget the Law of Extremity, and that such thoughts probably influence and control people's actual behavior much less than media has primed you to think.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "And people who are frightened and disoriented and triggered often\nflail,\ntaking actions that are desperate and disproportionate and poorly thought-out as they try to claw back some sense of safety and control\n[13]\n.\nRunning through considerations like the above for even just a handful of specific strains of social dark matter does something like\ninoculate\nyou against that sort of panic response, such that when you drop by your friend's house unexpectedly and discover them halfway into putting on a fursuit, you aren't\nwholly\nad-libbing, and have some previous prep work to fall back on.\n(It's sort of like how, at least in theory, having practiced kicks and punches in the context of martial arts kata means that your muscles have pretrained patterns that will fire if you end up in an actual fight. \u00a0Meditating on how you would ideally\nwant\nto respond to a given unmasking makes it more likely that you\nwill\nrespond that way, as opposed to doing something more frantic and mercurial.)\nThe other path to not-doing-the-thing is to catch it\nin the moment,\ni.e. to\nnotice\nthat your brain is going off the rails and intervene before it does anything drastic and irrevocable.\n(Often, literally just noticing what's happening\nat all,\nas in, popping out of the object-level flow and thinking/saying something like \"I notice I'm panicking\" is enough.)\nThe whole problem with social dark matter is that our underlying base of experience is deeply impoverished, which means that our\nunexamined beliefs",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "are largely wrong, which means that our\ndefault responses\ntend to be miscalibrated and broken. \u00a0It's like someone who's never driven on ice before reflexively slamming on the brakes, because slamming on the brakes\nusually\nmakes a car stop\u2014your mental autopilot isn't prepared for this, and so letting your default reactions guide you is a recipe for disaster.\nBut if you can successfully replace even one would-be link in the chain with the action [noticing what's actually happening in your brain], the odds of snapping out of it and\nwaking up\ngo through the roof.\n(For a dozen more essays on this, check out\nThe Nuts and Bolts of Naturalism\n.)\nSo here, for your perusal, is a(n incomplete) list of\nthings you might notice,\nif you are in the middle of something like a fearful swerve\u2014thoughts or feelings or sensations that could be a clue that you want to do something like slow down and start reasoning more explicitly.\nAs mentioned above, a sense of\nrighteousness\nor\nsteely resolve\nor\nmoral obligation.\nYou might feel like you have no choice, like you are\ncompelled\nto respond in a certain way, like the situation\ndemands\nit.\nAs mentioned above, a sense of\nurgency\n\u2014that the threat is imminent, and something must be done immediately.\nA sense of shock and horror\u2014that what you just learned is\nhuge,\nand\nvast,\nand\nimportant,\nand\nvery, very surprising.\nThe opposite of mundane.\nSignificant,\nin a dire sort of way.\nConfusion, disorientation, disbelief\u2014something about this is\nincoherent,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "surely not, it can't be true, this is so discordant with everything else I know about them.\nA feeling of\nreshuffling\n, or throwing-away\u2014that you're having to suddenly reassess and reevaluate everything you thought you knew about a person, that every memory is suspect, that maybe it was all a facade, that maybe you can't trust any of your previous, cached beliefs.\nA feeling of certainty or inevitability\u2014that you just\nknow\nwhat this revelation means, that its\nconsequences\nare obvious. \u00a0You can see the pattern, you know how the dominos will fall (or have already fallen). \u00a0That A necessarily and absolutely implies B, or even that A simply\nis\nB. \u00a0That what you've learned\nmeans\nsomething else, something more, something awful.\nThat this is a situation you've never actually encountered before and have no direct experience with, and yet (curiously) you feel like you ... kinda know a\nlot,\nactually? ...about what you're supposed to do next, and how to respond. \u00a0That there's a clear script, a prescribed set of Ways You're Supposed To React\u2014that you know just what you're supposed to say as soon as you can find a teacher to tell.\nA sense that others will\nback\nyou in your outrage or dismay; that you're feeling strong feelings that are\nperfectly normal\nand what\nanyone\nwould feel; that you have clear social consensus on your side.\nAll of these things tend to\npush people forward along the default track.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "They're the opposite of the standard, virtuous, building-block move of rationality: \"wait, hang on a second. \u00a0What do I think I know, and why do I think I know it?\"\nAnd if you manage to become\ncognizant\nof any of those things happening in your brain, then you've already created space for yourself to simply\nnot take the next step.\nTo break the cascade, and instead take any number of options that buy you thirty seconds of additional breathing room.\n(Counting slowly to ten, taking a dozen deep breaths, saying out loud \"pause\u2014I need to think,\" getting up and going to the bathroom,\nremembering your feet\n.)\nThe point is not so much to do some\nspecific\nwiser move as it is to simply avoid doing the\nknown\nbad thing, of blindly following your broken preconceptions to their obvious and tragic conclusion.\n(Remember: if, after thirty seconds of conscious awareness and deliberate thought, you come to the conclusion \"no, this actually\nis\nbad, I\nshould\nbe on the warpath,\" you can always\nramp right back up again!\nAny panic that can be destroyed by a mere thirty seconds of slow, deep breathing is probably panic you didn't want in the first place, and it's pretty rare that\nliterally immediate action\nis genuinely called-for, such that you can't afford to take the thirty seconds.)\nIX. Recap and Conclusion\nWhat is\nsocial dark matter\n?",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It's anything that people are strongly incentivized to hide from each other, and which therefore appears to be rare. \u00a0And given that our society disapproves-of and disincentivizes a\nwide\nvariety of things, there is a\nlot\nof it out there.\nBy the\nLaw of Prevalence\n, any given type of social dark matter is going to be much more common than your evidence would suggest, and by the\nLaw of Extremity\n, instances of that dark matter are going to tend to be much less extreme than you would naively guess.\nIf you lose track of these facts, you can go\ncrazy.\nYou can end up thinking absolutely insane things like \"all psychopaths are violent, manipulative bastards\" or \"pedophilia is an absolute, irresistible compulsion\" or \"none of the women\nI\nknow have ever been sexually assaulted\n[14]\n.\"\nYou can end up thinking these things because your mind takes, as its\ncentral, archetypal\nexamples of a thing, what are actually the most extreme, most visible, most unable-to-be-hidden instances of it.\nAs a result, when you glimpse something through a window or someone offers you a confidence or you stumble across an open journal page, you are in danger of making some pretty serious mistakes, chief among them concluding that\nthis real person\nis well-modeled by\nyour preexisting stereotype.\n(This is called \"bigotry\"\u2014reducing someone to a single demographic trait and then judging them based on your beliefs about that trait.)\nAnd as a result of\nthat,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "there are frighteningly high odds that you will fly off the handle in some fashion, and punish them for things they not only haven't done, but also very well might never have done in the future, either.\nAnd that's kind of a bummer, and maybe something that you don't want to do, on reflection! \u00a0Fortunately, you have other options, such as \"think through such situations in advance\" and \"develop the general ability to notice your brain going off the rails, and when you do, maybe just ... slow down for a bit?\"\nI will reiterate that I think we have different moral responsibilities toward people with dark matter\ntraits,\nversus toward people with dark matter\nactions.\nThe advice \"think it through in advance\" and \"slow down for a bit in the moment\" applies equally well to discovering that your partner\nfantasizes about violently raping people\nas it does to discovering that your partner\nhas violently raped someone,\nbut in the latter case, the correct thing to do is (pretty often, at least) to out and report them. \u00a0The reason you don't\nnecessarily\nout the person in the former case is because our society is\nsystematically confused\nabout how rare and how damning the existence of a rape fetish really is\u2014\n(largely by virtue of making it extraordinarily difficult to go gather the data, although some heroes are\ntrying anyway\n)\n\u2014and so you're causing many of their future interactions with employers and friends and potential romantic prospects to be\nunfairly dominated",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "by this thing that may actually have very little to do with their core character and their actual behavior.\nAnd that's bad! \u00a0It's bad to punish the innocent, and when it happens, it just further reinforces the problem, making\nother\ninnocents with the same traits hide even harder in a\nspiral of silence\n.\nIf you would like to think of yourself as the sort of person who would hide Jews from the Nazis, because the things that the Nazis believed about Jews being fundamentally and irretrievably toxic and horrible and corrosive and evil\nwere simply wrong\u2014\nIf you would like to think of yourself as the sort of person who\nwouldn't\nhave destroyed the career of a high-school English teacher in the 1950's, because the things that society believed about gay men being universally corrupting sex-craved child molesters\nwere simply wrong\n\u2014\nIf you would like to think of yourself as the sort of person who wouldn't have burned witches, or ostracized prostitutes, or stigmatized the mentally ill, or fought to maintain segregation, then you should apply the same kind of higher perspective\nnow,\nand see if you can sniff out the places where the\ncurrent\nsocial consensus is malignantly confused, just as the Jew-hiders and slave-rescuers and queer allies of the past saw past\ntheir\nsocieties' unquestioned beliefs\n[15]\n.\nMy parting advice, in that endeavor: take seriously the probability that you have not, in fact, internalized this lesson. \u00a0This essay, by itself, was\nprobably not enough\nto get you to",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "really truly grok\njust how many sadists and coke users and schizophrenics and brotherfuckers and closeted Randian conservatives and Nickelback fans you have in your very own social circle.\nThey were always there, and (in many cases) things turned out okay anyway. \u00a0When you have an unpleasant revelation and it\nfeels\nlike the sky is falling, check and see whether there's an\nactual\nproblem that requires you to take action, or whether it's just your preconceptions shattering\n[16]\n.\nMuch of the time, it's the latter, and acting\nanyway\nis what leads to tragedy.\nGood luck.\n^\nPart of what gives me confidence to name the Law of Prevalence the\nLaw\nof Prevalence, rather than the Guess of Prevalence or the Maybe of Prevalence, is that this happens basically every single time a given stigma is reduced.\n^\n\"\nWhat is true is already so.\nOwning up to it doesn't make it worse.\nNot being open about it doesn't make it go away.\nAnd because it's true, it is what is there to be interacted with.\nAnything untrue isn't there to be lived.\nPeople can stand what is true,\nfor they are already enduring it.\"\n^\nWith a special exception for honey, somehow; never underestimate the power of\nnormalization.\n^\nWhen a single revelation triggers a massive update, it's usually (in my experience) because one's model of the other person\nruled out\nthat they would ever do X, and so discovering even a single instance of X is a Big Deal because it means that one's model of X being\nfirmly off limits",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "was fundamentally flawed.\n^\nIt's a reflex people do\nat themselves,\ntoo. \u00a0Social dark matter is often dark to the person possessing it; people often dread finding out or admitting (even just to themselves) that they are queer or lack empathy or are turned on by violence or were sexually assaulted, etc.\n^\nOr perhaps\nhighly disabled people with autism;\nsorry, I\u2019m not sure where we currently stand with respect to which terminology manages to be neutral.\n^\nOr someone who\nchooses not to,\nfor various reasons, but when the incentives are steep, few people make that choice and so you're still looking at someone who is Notably Nonstandard.\n^\nI recall one of my partners (who worked as a stripper) mentioning that their father remarked \"strip clubs are where dreams go to die,\" to which my partner's knee-jerk response was \"what? No\u2014strip clubs are where pre-meds go to pay off their student loans.\"\n^\nThey also, for the record, did not rebut or even substantively engage with any of my eight distinct lines of reasoning. \u00a0They ignored that part of my argument completely, as if I hadn't said anything at all.\n^\nThere\nare\nchild molesters out there! \u00a0Extremely distressingly many! \u00a0Something like\nten percent of all minors\nare sexually abused, and often by teachers and pastors and scoutmasters and other Trusted Authority Figures! \u00a0It would be\ngreat",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "if we could figure out how to actually preemptively identify such people. But \"they're gay\" just ... turned out not to be a marker, in the way that people in the 1950's wanted it to be. \u00a0And being overconfident in your belief that you've found a proxy or correlate, when you haven't,\nmakes things worse for victims.\nIt makes you wrongly attack trustworthy and innocent people who\nhave\nthe trait, and it makes you wrongly trust dangerous people who lack it.\n^\nSome various examples: they will betray the United States to Japan, or brainwash your children into thinking that they should cut off their own genitals, or steal your spouse's valuables and sell them to a pawnshop to fuel their addiction, or try to play doctor with an unsuspecting child, or lose control in a traffic jam and grab their gun and kill someone.\n^\nWhile in the process of writing this very essay, I made a Facebook post in which I went out of my way to be careful with the label \"psychopath,\" and this triggered the following real-world example of the thing, happening in real time:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I am stuck at the idea that it's bigoted to tell someone not to date or get close to psychopaths. Like, dude, would it be bigoted to say \"don't marry a woman who will baby trap you, or one who will\u00a0immediately run away with your baby\"? Like, these are obvious no goes in the mating pool, and all parents should tell their kids. \"Once upon a time, we were in Egypt. Oh, and also avoid psychopaths. People who take pleasure in hurting others make bad social connections on any level.\"\nAlso, where on the surface of the earth can you be if not engaging with psychopaths is a difficult problem? 'I would lose so many potential social connections by filtering out (checks notes) enjoys hurting people.' Jeez, do you work for the police or for a bank? Are your coworkers doing lines of coke in the bathroom or straight up at their office desk?\nThe person replying to me seems\nunaware\nthat they are making a leap from \"psychopath\" to \"liar/manipulator/sadist.\" \u00a0They don't have distinct buckets for separately assessing [A], [B], and [A \u2192 B]. \u00a0The archetype\nis\nthe whole bell curve, as far as they know. \u00a0It's analogous to the (also often completely elided) differences between [pedophiles] and [child molesters], or between [people with violent sexual fantasies] and [rapists].\n^",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This is true when reacting to epiphanies about your own dark matter, too; it's likely that at least some queer suicides are not just about predictions of how others will treat them, but about a desperate attempt to solve the problem from the inside. Please just breathe.\n^\nWith the\nDifferent Worlds\ncaveat that bubbles do exist and there probably\nis\nsomebody out there for whom this statement is actually true. But you can't be (responsibly)\nconfident\nin such a belief simply because the women around you never talk about it.\n^\nI will note that one of my beta readers helpfully provided the psychopathic perspective that these empathetic and ethical perspectives are not particularly motivating. \u00a0I think that's a solid point! \u00a0I would bet on the existence of a selfish, purely pragmatic argument for these actions, but I have not actually gone out and located it. A short gesture in the direction of my intuition is \"on long enough timelines and with broad enough horizons, the actions taken by an\nMTG black\nagent often converge with those taken by an MTG white one.\"\n(There's\nalso\na purely pragmatic argument for going with the flow and not rocking the boat, though, and I ruefully acknowledge that it may be correct, according to their values, for most of my psychopathic readers to shrug and not help with this one.)\n^\nThis is not to say that the situation is always\ngood\nor even\nfine.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As #MeToo demonstrated, often things are very, very bad, and society just ... ignores it, and keeps on churning. \u00a0Often it would be nice if the world were\nmore\nderailed by ongoing atrocity, but whether the dark matter is \"good\" or \"bad,\"\neither way\nwe can't get to a place of commensurate, appropriate reaction until we can manage to look straight at reality without distortion and talk clearly and frankly about what's\nactually going on.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Recently someone pointed out to me that there was no good canonical post that explained the use of common knowledge in society. Since I wanted to be able to link to such a post, I decided to try to write it.\nThe epistemic status of this post is that I hoped to provide an explanation for a standard, mainstream idea, in a concrete way that could be broadly understood rather than in a mathematical/logical fashion, and so the definitions should all be correct, though the examples in the latter half are more speculative and likely contain some inaccuracies.\nLet's start with a puzzle. What do these three things have in common?\nDictatorships all through history have attempted to suppress freedom of the press and freedom of speech. Why is this? Are they just very sensitive? On the other side, the leaders of the Enlightenment fought for freedom of speech, and would not budge an inch against this principle.\nWhen two people are on a date and want to sleep with each other, the conversation will often move towards but never\nexplicitly\ndiscuss having sex. The two may discuss going back to the place of one of theirs, with a different explicit reason discussed (e.g. \"to have a drink\"),\neven\nif both want to have sex.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Throughout history, communities have had religious rituals that look very similar. Everyone in the village has to join in. There are repetitive songs, repetitive lectures on the same holy books, chanting together. Why, of all the possible community events (e.g. dinner, parties, etc) is this the most common type?\nWhat these three things have in common, is\ncommon knowledge\n- or at least, the attempt to create it.\nBefore I spell that out, we\u2019ll take a brief look into game theory so that we have the language to describe clearly what\u2019s going on. Then we\u2019ll be able to see concretely in a bunch of examples, how common knowledge is necessary to understand and build institutions.\nPrisoner's Dilemmas vs Coordination Problems\nTo understand why common knowledge is useful, I want to contrast two types of situations in game theory: Prisoner\u2019s Dilemmas and Coordination Problems. They look similar at first glance, but their payoff matrices have important differences.\nThe Prisoner's Dilemma (PD)\nYou\u2019ve probably heard of it - two players have the opportunity to cooperate, or defect against each other, based on a\nstory about two prisoners being offered a deal if they testify against the other\n.\nIf they do nothing they will put them both away for a short time; if one of them snitches on the other, the snitch gets off free and the snitched gets a long sentence. However if they\nboth\nsnitch they get pretty bad sentences (though neither are as long as when only one snitches on the other).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "In game theory, people often like to draw little boxes that show two different people's choices, and how much they like the outcome. Such a diagram is called a\ndecision matrix\n, and the numbers are called the players'\npayoffs\n.\nTo describe the Prisoner's Dilemma, below is a decision matrix where Anne and Bob each have the same two choices, labelled\nC\nand\nD\n. These are colloquially called \u2018cooperate\u2019 and \u2018defect\u2019. Each box contains two numbers, for Anne and Bob's payoffs respectively. \u200b\nIf the prisoner \u2018defects\u2019 on his partner, this means he snitches, and if he \u2018cooperates\u2019 with his partner, he doesn\u2019t snitch. They\u2019d both prefer that\nboth\nof them cooperate\n(\nC\n,\nC\n)\nto both of them defecting\n(\nD\n,\nD\n)\n, but each of them has an incentive to stab each other in the back to reap the most reward\n(\nD\n,\nC\n)\n.\nDo you see in the matrix how they both would prefer no snitching to both snitching, but they also have an incentive to stab each other in the back?\nReal World Examples\nNuclear disarmament is a prisoner\u2019s dilemma. Both the Soviet Union and the USS wanted to have nuclear bombs while the opponent doesn't, but they'd probably both prefer a world where nobody had bombs than a world where they were both pointing massive weapons at each others heads. Unfortunately in our world, we failed to solve the problem, and ended up pointing massive weapons at each others' heads for decades.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Military budget spending more broadly can be a prisoner\u2019s dilemma. Suppose two neighbouring countries are determining how much to spend on the military. Well, they don\u2019t want to go to war with each other, and so they\u2019d each like to spend a small amount of money on their military, and spend the rest of the money on running the country - infrastructure, healthcare, etc. However, if one country spends a small amount and the other country spends a lot, then the second country can just walk in and take over the first. So, they both spend lots of money on the military with no intention of using it, just so the other one can\u2019t take over.\nAnother prisoner\u2019s dilemma is tennis players figuring out whether to take performance enhancing drugs. Naturally, they'd like to dope and the opposing player not, but they'd rather both not dope than both dope.\nFree-Rider Problems\nDid you notice how there are more than two tennis players in the doping situation? When deciding whether to take drugs, not only do you have to worry about whether your opponent in the match today will dope, but also whether your opponent tomorrow will, and the day after, and so on. We\u2019re all wondering whether\nall\nof us will dope. In society there are loads of these scaled up versions of the prisoner\u2019s dilemma.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "For example, according to many political theories, everyone is better off if the government takes some taxes and uses them to provide public goods (e.g. transportation, military, hospitals). As a population, it's in everyone's interest if everyone cooperates, and takes a small personal sacrifice of wealth.\nHowever, if most people are doing it, you can defect, and this is great for you - you get the advantage of a government providing public goods, and also you keep your own money. But if everyone defects, then nobody gets the important public goods, and this is worse for each person than if they'd all cooperated.\nWhether you\u2019re two robbers, one of many tennis players, or a whole country fighting another country, you will run into a prisoner\u2019s dilemma. In the scaled-up version, a person who defects while everyone else cooperates is known as a\nfree-rider\n, and the scaled up prisoner\u2019s dilemma is called the\nfree-rider problem\n.\nCoordination Problems\nWith that under our belt, let\u2019s look at a new decision matrix. Can you identify what\u2019s importantly different about this matrix? Make a prediction about how you think this will change the players\u2019 strategies.\u200b\nDon't mix this up with the Prisoners' Dilemma - it's quite different. In the PD, if you cooperate and I defect, I get 4. What\u2019s important about the new decision-matrix, is that nobody has an incentive to backstab! If you cooperate and I defect, I get\nzero\n, instead of four.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "We all want the same thing. Both players' preference ordering is:\n(\nC\n,\nC\n)\n>\n(\nD\n,\nD\n)\n>\n[\n(\nC\n,\nD\n)\n=\n(\nD\n,\nC\n)\n]\nSo, you might be confused: Why is this a problem at all? Why doesn\u2019t everyone just pick C?\nLet me give an example from Michael Chwe\u2019s\nclassic book\non the subject\nRational Ritual: Culture, Coordination and Common Knowledge\n.\nSay you and I are co-workers who ride the same bus home. Today the bus is completely packed and somehow we get separated. Because you are standing near the front door of the bus and I am near the back door, I catch a glimpse of you only at brief moments. Before we reach our usual stop, I notice a mutual acquaintance, who yells from the sidewalk, \u201cHey you two! Come join me for a drink!\u201d Joining this acquaintance would be nice, but we care mainly about each other\u2019s company. The bus doors open; separated by the crowd, we must decide independently whether to get off.\nSay that when our acquaintance yells out, I look for you but cannot find you; I\u2019m not sure whether you notice her or not and thus decide to stay on the bus. How exactly does the communication process fail? There are two possibilities. The first is simply that you do not notice her; maybe you are asleep. The second is that you do in fact notice her. But I stay on the bus because I don\u2019t know whether you notice her or not. In this case we both know that our acquaintance yelled but I do not know that you know.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Successful communication sometimes is not simply a matter of whether a given message is received. It also depends on whether people are aware that other people also receive it. In other words, it is not just about people\u2019s knowledge of the message; it is also about people knowing that other people know about it, the \u201cmetaknowledge\u201d of the message.\nSay that when our acquaintance yells, I see you raise your head and look around for me, but I\u2019m not sure if you manage to find me. Even though I know about the yell, and I know that you know since I see you look up, I still decide to stay on the bus because I do not know that you know that I know. So just one \u201clevel\u201d of metaknowledge is not enough.\nTaking this further, one soon realizes that every level of metaknowledge is necessary: I must know about the yell, you must know, I must know that you know, you must know that I know, I must know that you know that I know, and so on; that is, the yell must be \u201ccommon knowledge.\u201d\nThe term \u201ccommon knowledge\u201d is used in many ways but here we stick to a precise definition. We say that an event or fact is common knowledge among a group of people if everyone knows it, everyone knows that everyone knows it, everyone knows that everyone knows that everyone knows it, and so on.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Two people can create these many levels of metaknowledge simply through eye contact: say that when our acquaintance yells I am looking at you and you are looking at me, [and we exchange a brief glance at our mutual friend and nod]. Thus I know you know about the yell, you know that I know that you know (you see me looking at you), and so on. If we do manage to make eye contact, we get off the bus; communication is successful.\nCoordination problems are only ever problems when everyone is\ncurrently\nchoosing\nD\n, and we need to\ncoordinate\nall choosing\nC\nat the same time. To do that, we need common knowledge.\n(The specific definition of common knowledge (\n\"I know that you know that I know that....\"\n) is often confusing, but for now the concrete examples below should help get a solid intuition for the idea.)\nCompare you and I on the bus to the coordination game payoff matrix: If we\nboth get off the train\n(\nC\n,\nC\n)\n,\nwe get to hang out with each other\nand\nspend some time with a mutual acquaintance. If only one of us does, we both miss out on the opportunity to hang out with each other (the thing we want least -\n(\nC\n,\nD\n)\nor\n(\nD\n,\nC\n)\n). If neither of us gets off the train, we get to hang out with each other, but in a less interesting way\n(\nD\n,\nD\n)\n.\nA Stable State\nThe reason that it\u2019s a difficult\ncoordination problem\n, is because the state\n(\nD\n,\nD\n)\nis an equilibrium state; neither of us alone can improve it by getting off the bus - only if we\u2019re able to coordinate us\nboth",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "getting off the bus does this work. You can think of it like a\nlocal optimum\n: if you take one step in any direction (if any single one of us changes our actions) we lose utility on net.\nThe name for such an equilibrium is taken from mathematician\nJohn Nash\n(who the film\nA Beautiful Mind\nwas based on), and is called a\nNash equilibrium.\nBoth\n(\nC\n,\nC\n)\nand\n(\nD\n,\nD\n)\nare Nash equilibria in a\ncoordination problem\n. Can you see how many Nash equilibria there are in the Prisoner's Dilemma?\nSolving problems and resolving dilemmas\nA good way to contrast\ncoordination problem\ns and free rider problems is to think about these equilibrium states. In the free rider problem, the situation where everyone cooperates is not a Nash equilibrium - everyone is incentivised to defect while the others cooperate, and so occasionally some people do. While the PD only has one Nash equilibrium however, a\ncoordination problem\nhas got two! The challenge is moving from the current one, to one we all prefer.\nFree rider problems are solved by creating new incentives against defecting. For example, the government punishes you if you don't pay your taxes. In sports, the practice of doping is punished, and what's more it's made out to be\ndishonourable\n. People tell stories of the evil people that dope and how we all look down on them; even if you could dope and probably get away with it, there's no\nplausible deniability",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "in your mind - you know you're being a bad person and would be judged by everyone of your colleagues.\nCoordination problems can be solved by creating such incentives, but they can also be solved just by improving information flow. We'll see that below.\nThree Coordination Problems\nThat situation when you and I lock eyes, nod, and get off the bus? That\u2019s\nhaving\ncommon knowledge\n. It\u2019s the confidence to take the step, because you\u2019re not worried about what I might do. Because you know I\u2019m getting off the bus with you.\nNow we\u2019ve got a handle on what common knowledge is, we can turn back to the three puzzling phenomena from the beginning.\nDictators and freedom of speech\nDictatorships all through history have attempted to suppress freedom of the press and freedom of speech. Why is this? Are they just very sensitive? On the other side, the leaders of the Enlightenment fought for freedom of speech, and would not budge an inch against this principle.\nMany people under a dictatorship want a revolution - but rebelling only makes sense if enough\nother\npeople want to rebel. The people as a whole are much more powerful than the government. But you alone won\u2019t be any match for the local police force. You have to\nknow\nthat the others are willing to rebel (as long as you rebel),\nand\nyou have to know that they know that\nyou\u2019re\nwilling to rebel.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "People in a dictatorship are all trying to move to a better nash equilibrium without going via the corners of the box (i.e. where some people rebel, but not enough, and then you have some pointless death instead of a revolution).\nThat feeling of worrying whether the people around you will support you, if you attack the police. That\u2019s what it\u2019s like\nnot\nto have common knowledge. When a dictator gets ousted by the people, it's often in the form of a riot, because you can\nsee the other people around you\nwho are poised on the brink of violence. They can see you, and you all know that if you moved as one you might accomplish something. That\u2019s the feeling of common knowledge.\nThe dictator is trying to suppress the people\u2019s ability to create common knowledge that jumps them straight to\n(\nC\n,\nC\n)\n- and so they attempt to suppress the news media. Preventing common knowledge being formed among the populace means that large factions cannot coordinate - this is a successful divide and conquer strategy, and is why dictators are able to lead with so little support (often <1% of the population).\nUncertainty in Romance\nWhen two people are on a date and want to sleep with each other, the conversation will often move towards but never\nexplicitly\ndiscuss having sex. The two may discuss going back to the place of one of theirs, with a different explicit reason discussed (e.g. \"to have a drink\"),\neven\nif both want to have sex.\nNotice the difference between",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Walking up to someone cold at a bar and starting a conversation\nWalking up to someone at a bar, after you noticed them stealing glances at you\nWalking up to someone at a bar, after you glanced at them, they glanced at you, and your eyes\nlocked\nIt\u2019s easiest to approach confidently in the last case, since you have clear evidence that you\u2019re both at least interested in a flirtatious conversation.\nIn dating, getting\nexplicitly\nrejected is a loss of status, so people are incentivised to put a lot of effort into preserving\nplausible deniability\n.\nNo really, I just came up to your flat to listen to your vinyl records!\nSimilarly, we know other people don\u2019t like getting rejected, so we rarely explicitly ask either.\nAre you trying to have sex with me?\nSo with sex, romance, or even deep friendships, people are often trying to get to\n(\nC\n,\nC\n)\nwithout\ncommon knowledge, up until the moment that they\u2019re both very confident that both parties are interested in raising their level of intimacy.\n(Scott Alexander wrote about this attempt to avoid rejection and the confusion it entails in his post\nConversation Deliberately Skirts the Border of Incomprehensibility\n.)\nThis problem of\navoiding\ncommon knowledge as we try to move to better Nash equilibrium also shows up in negotiations and war, where you might make a threat, and not\nwant\nthere to be common knowledge of whether you\u2019ll actually follow through on that threat.\n(Added: After listening to a podcast with Robin Hanson,",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "I realise that I've simplified too much here.\nIt's also the case that each member of the couple might not have figured out whether they want to have sex, and so\nplausible deniability\ngives them an out if they decide not to, without the explicit status hit/attack.\nI definitely have the sense that if someone very bluntly states subtext when they notice it, this means\nI can't play the game with\nthem even if I wanted to\n, as when they state it explicitly I have to say \"No!\" else admit that I was slightly flirting / exploring a romance with them, and significantly increase\nthe change\nI will immediately receive an explicit rejection.)\nCommunal/Religious Rituals\nThroughout history, communities have had religious rituals that look very similar. Everyone in the village has to join in. There are repetitive songs, repetitive lectures on the same holy books, chanting together. Why, of all the possible community events (e.g. dinner, parties, etc) is this the most common type?\nMichael Chwe wrote a whole\nbook\non this topic. To simplify massively: rituals are a space to create common knowledge in a community.\nYou don\u2019t just listen to a pastor talk about virtue and sin. You listen\ntogether,\nwhere you know that everyone else was listening too. You say \u2018\namen\n\u2019 together after each prayer the pastor speaks, and you all know that you\u2019re listening along and paying attention. You speak the Lord\u2019s Prayer or some Buddhist chant together, and you know that\neveryone\nknows the words.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Rituals create common knowledge about what in the community is is rewarded, what is punished. This is why religions are so powerful (and why the state likes to control religion). It\u2019s not just a part of life like other institutions everyone uses like a market or a bank - this is an institution that builds common knowledge about\nall\nareas of life, especially the most important communal norms.\nTo flesh out the punishment part of that: When someone does something sinful by the standards of the community, you know that\nthey\nknow they\u2019re not supposed to, and they know that you know that they know. This makes it easier to punish people - they can\u2019t claim they didn\u2019t know they weren\u2019t supposed to do something. And making it easier to punish people also makes people less likely to sin in the first place.\nThe rituals have been gradually improved and changed over time, and often the trade-offs have been towards helping coordinate a community. This is why the words in the chants or songs that everyone sings are simple, repetitive, and often rhyme - so you know that everyone knows exactly what they are. This is why rituals often occur seated in a circle - not only can you see the performance, but you can see\nme\nseeing the performance, and I you, and we have common knowledge.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Common knowledge is often much easier to build in small groups - in the example about getting off the bus, the two need only to look at each other, share a nod, and common knowledge is achieved. Building common knowledge between hundreds or thousands of people is significantly harder, and the fact that religion has such a significant ability to do so is why it has historically had so much connection to politics.\nCommon Knowledge Production in Society at Large\nCommon knowledge is a very common state of affairs that humans had to reason about naturally in the ancestral environment; there is no explicit mathematical calculation being done when two people lock eyes on a bus then coordinate getting off and seeing their friend.\nWe\u2019ve looked at how religions help create common knowledge of norms. Here\u2019s a few other common knowledge producing mechanisms that exist in the world today.\nThe News\nThe main way common knowledge is built is by having everyone in the same room, in silence, while somebody speaks. Another way (in the modern world) is official channels of communication that you know everyone listens to.\nThis is actually one of the good reasons to discuss news so much - we\u2019ve built trust that what the NYT says is common knowledge, and so can coordinate around it. Sometimes an official document is advertised widely and is known to be known as common knowledge, even if we ourselves often haven\u2019t read it (e.g. Will MacAskill\u2019s book, the NYT).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Nowadays there is no such single news source, and we\u2019ve lost that coordination mechanism. We all have Facebook, but Facebook is entirely built out of bubbles. Facebook\ncould\nchoose to create common knowledge by making something appear in everyone\u2019s feed, but they choose not to (and this is in fact a fairly restrained use of power that I appreciate).\nOne time facebook slipped up on this, was when they built their 'Marked Safe' feature. If a dangerous event (big fire, terrorist attack, earthquake) happened near you, you could 'mark yourself safe' and then all of your friends would get a notification saying you were safe.\nNow, it was clear that everyone else was seeing the notifications you were seeing, and so if your nearby friend marked themselves safe and you didn\u2019t, your friends would all notice that conspicuous absence of a notification, and know that you had chosen not to click it. This creates a pressure for all people to always notify their friends whenever there\u2019s been a dangerous event near them, even if the odds of them being involved were miniscule. This is a clear waste of time and attention,\nand the feature was removed\nthe feature continues to be a piece of security theatre in our lives.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "A related point about the power of media that creates common knowledge: in Michael Chwe's book, he does some data analysis of the marketing strategies of multiple different industries. He classifies products that are 'social goods' - those you want to buy if you expect other people like them. For example, you want to buy wines that you know your guests like, or bring beer to parties that others like; you want to use popular computer brands that people have developed software for; etc.\nHe then shows that social brands typically pay more\nper viewer\nfor advertising; not necessarily more total, but that they'll pay a higher amount for opportunities to broadcast in places that generate common knowledge. Rather than buy 10 opportunities to broadcast to 2 million people on various channels, they'll pay a premium for 20 million people to view their ad during the superbowl, to create stronger common knowledge.\nAcademic Research\nThe central place where common knowledge is generated in science is in journals. These are where researchers can discover the new insights of the field, and build off them. Conferences can also help in this regard.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "A more interesting case is textbooks (I borrow this example from Oliver Habryka). There was once a time in the history of physics where the basics of quantum mechanics were known, and yet to study them required reading the right journal articles, in the right order. When you went to a convention of physicists, you likely had to explain many of the basics of the field\nbefore\nyou could express your new idea.\nThen, some people decided to aggregate it into textbooks, which were then all taught to the undergraduates of the next generation, until the point where you could walk into the room and start using all the jargon and\ntrust that everyone knew what you meant\n. Having common knowledge of the basics of a field is necessary for a field to make progress - to make the 201 the 101, and then build new insights on top.\nIn my life, even if 90% of the people around have the idea, when I\u2019m not confident that 100% do then I often explain the basic idea for everyone. This often costs a lot of time - for example, after you read this post, I\u2019ll be able to say to you a sentence like \u2018the undergrad textbook system is a mechanism to create the common knowledge that allows the field as a whole to jump to the new Nash equilibrium of using advanced concepts\u2019.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Paragraphs can be reduced to sentences, and you can get even more powerful returns with more abstract ideas - in mathematics, pages of symbols can be turned into a couple of lines (with the right abstractions e.g. calculus, linear algebra, probability theory, etc).\nStartups\nA startup is a very small group of people building detailed models of a product. They\u2019re able to create a lot of common knowledge due to their small size. However, one of the reasons why they need to put a lot of thought into the long-term of the company, is because they will\nlose\nthis common knowledge producing mechanism as they scale, and the only things they\u2019ll be able to coordinate on are the things they already learned together.\nThe fact that they\u2019re able to build common knowledge when they\u2019re small is why they\u2019re able to make so much more progress than big companies, and is also why big companies that innovate tend to compartmentalise their teams into small groups. As the company grows, there are far fewer things that can be retained as common knowledge amongst the employees. You can have intensive on-boarding processes for the first 20 hires, but it really doesn\u2019t scale to 100 employees.\nHere are three things that can sustain at very large scales:\nName:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Y Combinator says that the name of your company should tell people what you do - cf. AirBnb, InstaCart, DoorDash, OpenAI, Lyft, etc. Contrast with companies like Palantir, where even I don\u2019t know exactly what they work on day-to-day, and I\u2019ve got friends who work there.\nMission:\nIt is possible to predict the output of an organisation very well by what their mission statement concretely communicates. For example, the company SpaceX has their mission statement at the top of all hiring documents (cf. the application forms to be a\nrocket scientist\n,\nbusiness analyst\n, or\nbarista\n).\nValues:\nAffects hiring and decision-making long into the future. YC specifically says to pick 4-8 core values, have a story associated with each value, and tell each story\nevery\nday (e.g. in meetings). That may seem like way too much, but in fact that\u2019s how much it can take to make the values common knowledge (especially as your company scales).\nAt what cost?\nA standard response to coordination failures is one of\nexasperation\n- a feeling that we\nshould\nbe able to solve this if only we\ntried\n.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Imagine you\u2019re trying to coordinate you and a few friends to move some furniture, and they keep getting in each other\u2019s way. You might shout \u201cHey guys! Look, Pete and Laurie have to move the couch first, then John and Pauline can move the table!\u201d And then things just start working. Or even just between two of you - when a friend is late for skype calls because she messes up her calendar app, you might express irritation, and she might try extra hard to fix the problem.\nWe also feel this when we look at society at large, for example when we look at coordination failures in politics.\nWhy does everyone continue voting for silly-no-good politicians? Why can\u2019t we all just vote for someone sane?!\nIn the book\nInadequate Equilibria\nby Eliezer Yudkowsky, the character\nSimplicio\nrepresents this feeling. Here is the character discussing a (real) coordination failure in the US healthcare system that causes a few dozen newborn children to die every year:\nsimplicio:\nThe first thing you have to understand, Visitor, is that the folk in this world are hypocrites, cowards, psychopaths, and sheep.\nI mean, I certainly care about the the lives of newborn children. Hearing about their plight certainly makes me want to do something about it. When I see the problem continuing in spite of that, I can only conclude that other people don\u2019t feel the level of moral indignation that I feel when staring at a heap of dead babies.\n[...]",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Regardless, I\u2019m not seeing what the grand obstacle is to people solving these problems by, you know, coordinating. If people would just act in unity, so much could be done!\nI feel like you\u2019re placing too much blame on system-level issues, Cecie, when the simpler hypothesis is just that the people in the system are terrible: bad at thinking, bad at caring, bad at coordinating. You claim to be a \u201ccynic,\u201d but your whole world-view sounds rose-tinted to me.\nOne of the final points to deeply understand about common knowledge in society, is how\ncostly\nit is to create at scale.\nBig companies get to pick only a few sentences to become common knowledge. To have a community rally around a more complex set of values and ideals (i.e. a significant function of religion) each and every member of that community must give up half of each Sunday, to repeat ideas\nthey already know\nover and over - nothing new, just with the goal of creating common knowledge.\nThere used to be news programmes everybody in a country would tune in for. Notice how the New York Times used to be something people would read once per week or once per\nday\n, and discuss it with friends, even though most of the info has no direct effect on their lives.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Our intuitions were developed for tribes of size 150 or less (cf. Dunbar\u2019s number) and as such, our intuitions around coordination are often terribly off. Simplicio is someone who has not noticed the cost of creating common knowledge at scale. He believes that society could easily vote for good politicians\nif only we coordinated\n, and because we don\u2019t he infers we\nmust\nbe stupid and/or evil.\nThe feeling of\nindignation\nat people for failing to coordinate can be thought of as creating an incentive to solve the\ncoordination problem\n. I\u2019m letting my skype partner know that I will punish them if they fail again. But today, this feeling toward people for failing to coordinate is almost always misguided.\nThink of it this way: many small\ncoordination problem\ns are sufficiently small that you\u2019ll solve them quickly; many\ncoordination problem\ns are sufficiently big that you have no chance of solving them via normal means, and you will feel indignation every time you notice them (e.g. think politics/twitter). Basically, when you feel like being indignant in the modern world, 99% of the time it\u2019s wasted motion.\nSimplicio\u2019s intuitions are a great fit for a hunter-gatherer tribe. When he gets indignant, it would be proportional to the problem, the problem would get solved, and everyone would be happy. At a later point in the book Simplicio calls for political revolution - the sort of mechanism that works if you\u2019re able to get everyone to gather in a single place.\nThe solution to",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "coordination problem\ns at scale is much harder, and requires thinking about incentives structures and information flows rather than emotions directed at individuals in your social environment. Or in other words, building a civilization.\nvisitor:\nIndeed. Moving from bad equilibria to better equilibria is the whole point of having a civilization in the first place.\n- Another character in\nInadequate Equilibria\n, by Eliezer Yudkowsky\nSo, what's common knowledge\nfor\n?\nSummary of this post:\nA\ncoordination problem\nis when everyone is taking some action A, and we\u2019d rather all be taking action B, but it\u2019s bad if we don\u2019t all move to B at the same time.\nCommon knowledge is the name for the epistemic state we\u2019re collectively in, when we know we can all start choosing action B - and trust everyone else to do the same.\nWe\u2019re intuitively very good at navigating such problems when we\u2019re in small groups (size < 150).\nWe\u2019re intuitively very bad at navigating such problems in the modern world, and require the building of new, microeconomic intuitions in order to build a successful society.\nThere is a great deal more subtlety to how common knowledge gets built and propagates. This post has given but a glimpse through the lens of game-theory, and hopefully you now see the light that this lens sheds on a great variety of phenomena.\nLinks to explore more on this subject:\nMoloch\u2019s Toolbox (Inadequate Equilibria, Ch 3)\n(\nlink\n)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "A guide to the ways our current institutions fail to coordinate. Largely applying standard microeconomics, and a great post to read after this one.\nMeditations on Moloch\n(\nlink\n)\nAn original idea about coordination failures, which the above book chapter formalised. It's a great post, and it's good to follow the intellectual heritage of ideas.\nRational Ritual: Culture, Coordination and Common Knowledge\n(\nlink\n)\nSolid book with lots of detail.\nScott Aaronson on Common Knowledge and Aumann's Agreement Theorem\n(\nlink\n)\nThis post caused me to spend a bunch more time thinking about these topics. I found the explanations personally to be fairly abstract, which inspired me to write this post.\nScott Alexander\u2019s sequence on Game Theory\n(\nlink\n)\nAfter writing this post, I found Scott Alexander had also written about some of the examples (especially the dictatorship one) in detail 7 years ago\n(link)\n.\nAndrew Critch on 'Unrolling Social Metacognition: Three levels of meta are not enough'\n(\nlink\n)\nThis is a great post going into the details of how my modelling of you modelling me modelling you... works in practice. Highly recommended if the definition of common knowledge presented above seemed confusing.\nThanks\nto Raymond Arnold, Jacob Lagerros and Oliver Habryka for extensive feedback and comments, and to Hadrien Pouget for proofreading an early draft. A further special mention to Raymond for pointing out this term ought to be a standard piece of\nexpert jargon",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "[Previously in sequence:\nEpistemic Learned Helplessness\n]\nI.\n\u201cCulture is the secret of humanity\u2019s success\u201d sounds like the most vapid possible thesis.\nThe Secret Of Our Success\nby anthropologist Joseph Heinrich manages to be an amazing book anyway.\nHeinrich wants to debunk (or at least clarify) a popular view where humans succeeded because of our raw intelligence. In this view, we are smart enough to invent neat tools that help us survive and adapt to unfamiliar environments.\nAgainst such theories: we cannot actually do this. Heinrich walks the reader through many stories about European explorers marooned in unfamiliar environments. These explorers usually starved to death. They starved to death in the middle of endless plenty. Some of them were in Arctic lands that the Inuit considered among their richest hunting grounds. Others were in jungles, surrounded by edible plants and animals. One particularly unfortunate group was in Alabama, and would have perished entirely if they hadn\u2019t been captured and enslaved by local Indians first.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "These explorers had many advantages over our hominid ancestors. For one thing, their exploration parties were made up entirely of strong young men in their prime, with no need to support women, children, or the elderly. They were often selected for their education and intelligence. Many of them were from Victorian Britain, one of the most successful civilizations in history, full of geniuses like Darwin and Galton. Most of them had some past experience with wilderness craft and survival. But despite their big brains, when faced with the task our big brains supposedly evolved for \u2013 figuring out how to do hunting and gathering in a wilderness environment \u2013 they failed pathetically.\nNor is it surprising that they failed. Hunting and gathering is actually really hard. Here\u2019s Heinrich\u2019s description of how the Inuit hunt seals:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "You first have to find their breathing holes in the ice. It\u2019s important that the area around the hole be snow-covered\u2014otherwise the seals will hear you and vanish. You then open the hole, smell it to verify it\u2019s still in use (what do seals smell like?), and then assess the shape of the hole using a special curved piece of caribou antler. The hole is then covered with snow, save for a small gap at the top that is capped with a down indicator. If the seal enters the hole, the indicator moves, and you must blindly plunge your harpoon into the hole using all your weight. Your harpoon should be about 1.5 meters (5ft) long, with a detachable tip that is tethered with a heavy braid of sinew line. You can get the antler from the previously noted caribou, which you brought down with your driftwood bow.\nThe rear spike of the harpoon is made of extra-hard polar bear bone (yes, you also need to know how to kill polar bears; best to catch them napping in their dens). Once you\u2019ve plunged your harpoon\u2019s head into the seal, you\u2019re then in a wrestling match as you reel him in, onto the ice, where you can finish him off with the aforementioned bear-bone spike.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Now you have a seal, but you have to cook it. However, there are no trees at this latitude for wood, and driftwood is too sparse and valuable to use routinely for fires. To have a reliable fire, you\u2019ll need to carve a lamp from soapstone (you know what soapstone looks like, right?), render some oil for the lamp from blubber, and make a wick out of a particular species of moss. You will also need water. The pack ice is frozen salt water, so using it for drinking will just make you dehydrate faster. However, old sea ice has lost most of its salt, so it can be melted to make potable water. Of course, you need to be able to locate and identify old sea ice by color and texture. To melt it, make sure you have enough oil for your soapstone lamp.\nNo surprise that stranded explorers couldn\u2019t figure all this out. It\u2019s more surprising that the Inuit\ndid\n. And although the Arctic is an unusually hostile place for humans, Heinrich makes it clear that hunting-gathering techniques of this level of complexity are standard everywhere. Here\u2019s how the Indians of Tierra del Fuego make arrows:\nAmong the Fuegians, making an arrow requires a 14-step procedure that involves using seven different tools to work six different materials. Here are some of the steps:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "\u2013 The process begins by selecting the wood for the shaft, which preferably comes from chaura, a bushy, evergreen shrub. Though strong and light, this wood is a non-intuitive choice since the gnarled branches require extensive straightening (why not start with straighter branches?).\n\u2013 The wood is heated, straightened with the craftsman\u2019s teeth, and eventually finished with a scraper. Then, using a pre-heated and grooved stone, the shaft is pressed into the grooves and rubbed back and forth, pressing it down with a piece of fox skin. The fox skin become impregnated with the dust, which prepares it for the polishing stage (Does it have to be fox skin?).\n\u2013 Bits of pitch, gathered from the beach, are chewed and mixed with ash (What if you don\u2019t include the ash?).\n\u2013 The mixture is then applied to both ends of a heated shaft, which must then be coated with white clay (what about red clay? Do you have to heat it?). This prepares the ends for the fletching and arrowhead.\n\u2013 Two feathers are used for the fletching, preferably from upland geese (why not chicken feathers?).\n\u2013 Right-handed bowman must use feathers from the left wing of the bird, and vice versa for lefties (Does this really matter?).\n\u2013 The feathers are lashed to the shaft using sinews from the back of the guanaco, after they are smoothed and thinned with water and saliva (why not sinews from the fox that I had to kill for the aforementioned skin?).",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Next is the arrowhead, which must be crafted and then attached to the shaft, and of course there is also the bow, quiver and archery skills. But, I\u2019ll leave it there, since I think you get the idea.\nHow do hunter-gatherers know how to do all this? We usually summarize it as \u201cculture\u201d. How did it form? Not through some smart Inuit or Fuegian person reasoning it out; if that had been it, smart European explorers should have been able to reason it out too.\nThe obvious answer is\n\u201ccultural evolution\u201d\n, but Heinrich isn\u2019t much better than anyone else at taking the mystery out of this phrase. Trial and error must have been involved, and less successful groups/people imitating the techniques of more successful ones. But is that really a satisfying explanation?\nI found the chapter on language a helpful reminder that we already basically accept something like this is true. How did language get invented? I\u2019m especially interested in this question because of my brief interactions with conlanging communities \u2013 people who try to construct their own languages as a hobby or as part of a fantasy universe, like Tolkien did with Elvish. Most people are\nterrible",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "at this; their languages are either unusable, or exact clones of English. Only people who (like Tolkien) already have years of formal training in linguistics can do a remotely passable job. And you\u2019re telling me the original languages were invented by cavemen? Surely there was no committee of Proto-Indo-European nomads that voted on whether to have an inflecting or agglutinating tongue? Surely nobody ran out of their cave shouting \u201cEureka!\u201d after having discovered the interjection? We just kind of accept that after cavemen working really hard to communicate with each other, eventually language \u2013 still one of the most complicated and impressive productions of the human race \u2013 just sort of happened.\nTaking the generation of culture as secondary to this kind of mysterious process, Heinrich turns to its transmission. If cultural generation happens at a certain rate, then the fidelity of transmission determines whether a given society advances, stagnates, or declines.\nFor Heinrich, humans started becoming more than just another species of monkey when we started transmitting culture with high fidelity. Some anthropoligsts talk about the\nMachiavellian Intelligence Hypothesis\n\u2013 the theory that humans evolved big brains in order to succeed at social manuevering and climbing dominance hierarchies.\nHeinrich counters with his own Cultural Intelligence Hypothesis \u2013 humans evolved big brains in order to be able to maintain things like Inuit seal hunting techniques.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Everything that separates us from the apes is part of an evolutionary package designed to help us maintain this kind of culture, exploit this kind of culture, or adjust to the new abilities that this kind of culture gave us.\nII.\nSecret\ngives many examples of many culture-related adaptations, and not all are in the brain.\nOne of the most important differences between man and ape is our puny digestive tracts:\nOur mouths are the size of the squirrel monkey\u2019s, a species that weighs less than three pounds. Chimpanzees can open their mouths twice as ide as we can and hold substantial amounts of food compressed between their lips and large teeth. We also have puny jaw muscles that reach up only to just below our ears. Other primates\u2019 jaw muscles stretch to the top of their heads, where they sometimes even latch onto a central bony ridge. Our stomachs are small, having only a third of the surface area that we\u2019d expect for a primate of our size, and our colons are too short, being only 60% of their expected mass.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Compared to other animals, we have such atrophied digestive tracts that we shouldn\u2019t be able to live. What saves us? All of our food processing techniques, especially cooking, but also chopping, rinsing, boiling, and soaking. We\u2019ve done much of the work of digestion before food even enters our mouths. Our culture teaches us how to do this, both in broad terms like \u201chold things over fire to cook them\u201d and in specific terms like \u201cthis plant needs to be soaked in water for 24 hours to leach out the toxins\u201d. Each culture has its own cooking knowledge related to the local plants and animals; a frequent cause of death among European explorers was cooking things in ways that didn\u2019t unlock any of the nutrients, and so starving while apparently well-fed.\nAll of this is cultural. Heinrich is kind of cruel in his insistence on this. He recommends readers go outside and try to start a fire. He even gives some helpful hints \u2013 flint is involved, rubbing two sticks together works for some people, etc. He predicts \u2013 and stories I\u2019ve heard from unfortunate campers confirm \u2013 that you will not be able to do this, despite an IQ far beyond that of most of our hominid ancestors. In fact, some groups (most notably the aboriginal Tasmanians) seem to have lost the ability to make fire, and never rediscovered it. Fire-making was discovered a small number of times, maybe once, and has been culturally transmitted since then.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "And food processing techniques are even more complicated. Nixtamalization of corn, necessary to prevent vitamin deficiencies, involves soaking the corn in a solution containing ground-up burnt seashells. The ancient Mexicans discovered this and lived off corn just fine for millennia. When the conquistadors took over, they ignored it and ate corn straight. For four hundred years, Europeans and Americans ate unnixtamalized corn. By official statistics, three million Americans came down with corn-related vitamin deficiencies during this time, and up to a hundred thousand died. It wasn\u2019t until 1937 that Western scientists discovered which vitamins were involved and developed an industrial version of nixtamalization that made corn safe. Early 1900s Americans were very smart and had lots of advantages over ancient Mexicans. But the ancient Mexicans\u2019 culture got this one right in a way it took Westerners centuries to match.\nHumans are persistence hunters: they cannot run as fast as gazelles, but they can keep running for longer than gazelles (or almost anything else). Why did we evolve into that niche? The secret is our ability to carry water. Every hunter-gatherer culture has invented its own water-carrying techniques, usually some kind of waterskin. This allowed humans to switch to perspiration-based cooling systems, which allowed them to run as long as they want.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "And humans are consumate tool users. In some cases, we evolved in order to use tools better; our hands outclass those of any other ape in terms of finesse. In other cases, we devolved systems that were no longer necessary once tools took over. We are vastly weaker than any other ape. Heinrich describes a circus act of the 1940s where the ringmaster would challenge strong men in the audience to wrestle a juvenile chimpanzee. The chimpanzee was tied up, dressed in a mask that prevented it from biting, and wearing soft gloves that prevented it from scratching. No human ever lasted more than five seconds. Our common ancestor with other apes grew weaker and weaker as we became more and more reliant on artificial weapons to give us an advantage.\nIII.\nBut most of our differences from other apes are indeed in the brain. They\u2019re just not necessarily where you would expect.\nTomasello et al tested human toddlers vs. apes on a series of traditional IQ type questions. The match-up was surprisingly fair; in areas like memory, logic, and spatial reasoning, the three species did about the same. But in ability to learn from another person, humans wiped the floor with the other two ape species:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Remember, Heinrich thinks culture accumulates through random mutation. Humans don\u2019t have control over how culture gets generated. They have more control over how much of it gets transmitted to the next generation. If 100% gets transmitted, then as more and more mutations accumulate, the culture becomes better and better. If less than 100% gets transmitted, then at some point new culture gained and old culture lost fall into equilibrium, and your society stabilizes at some higher or lower technological level. This means that transmitting culture to the next generation is maybe the core human skill. The human brain is optimized to make this work as well as possible.\nHuman children are obsessed with learning things. And they don\u2019t learn things randomly. There seem to be \u201cbiases in cultural learning\u201d, ie slots in an infant\u2019s mind that they know need to be filled with knowledge, and which they preferentially seek out the knowledge necessary to fill.\nOne slot is for language. Human children naturally listen to speech (as early as in the womb). They naturally prune the phonemes they are able to produce and distinguish to the ones in the local language. And they naturally figure out how to speak and understand what people are saying, even though learning a language is hard even for smart adults.\nAnother slot is for animals. In a world where megafauna has been relegated to zoos, we\nstill\nteach children their ABCs with \u201cL is for lion\u201d and \u201cB is for bear\u201d, and children\nstill",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "read picture books about Mr. Frog and Mrs. Snake holding tea parties. Heinrich suggests that just as the young brain is hard-coded to want to learn language, so it is hard-coded to want to learn the local animal life (little boys\u2019 vehicle obsession may be a weird outgrowth of this; buses and trains are the closest thing to local megafauna that most of them will encounter).\nAnother slot is for plants:\nTo see this system in operation, let\u2019s consider how infants respond to unfamiliar plants. Plants are loaded with prickly thorns, noxious oils, stinging nettles and dangerous toxins, all genetically evolved to prevent animals like us from messing with them. Given our species wide geographic range and diverse use of plants as foods, medicines and construction materials, we ought to be primed to both learn about plants and avoid their dangers. To explore this idea in the lab, the psychologists Annie Wertz and Karen Wynn first gave infants, who ranged in age from eight to eighteen months, an opportunity to touch novel plants (basil and parsley) and artifacts, including both novel objects and common ones, like wooden spoons and small lamps.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "The results were striking. Regardless of age, many infants flatly refused to touch the plants at all. When they did touch them, they waited substantially longer than they did with the artifacts. By contrast, even with the novel objects, infants showed none of this reluctance. This suggests that well before one year of age infants can readily distinguish plants from other things, and are primed for caution with plants. But, how do they get past this conservative predisposition?\nThe answer is that infants keenly watch what other people do with plants, and are only inclined to touch or eat the plants that other people have touched or eaten. In fact, once they get the \u2018go ahead\u2019 via cultural learning, they are suddenly interested in eating plants. To explore this, Annie and Karen exposed infants to models who both picked fruit from plants and also picked fruit-like things from an artifact of similar size and shape to the plant. The models put both the fruit and the fruit-like things in their mouths. Next, the infants were given a choice to go for the fruit (picked from the plant) or the fruit-like things picked from the object. Over 75% of the time the infants went for the fruit, not the fruit-like things, since they\u2019d gotten the \u2018go ahead\u2019 via cultural learning.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "As a check, the infants were also exposed to models putting the fruit or fruit-like things behind their ears(not in their mouths). In this case, the infants went for the fruit or fruit-like things in equal measure. It seems that plants are most interesting if you can eat them, but only if you have some cultural learning cues that they aren\u2019t toxic.\nAfter Annie first told me about her work while I was visiting Yale in 2013, I went home to test it on my 6-month-old son, Josh. Josh seemed very likely to overturn Annie\u2019s hard empirical work, since he immediately grasped anything you gave him and put it rapidly in his mouth. Comfortable in his mom\u2019s arms, I first offered Josh a novel plastic cube. He delighted in grapping it and shoving it directly into his mouth, without any hesitation. Then, I offered him a sprig of arugula. He quickly grabbed it, but then paused, looked with curious uncertainty at it, and then slowly let it fall from his hand while turning to hug his mom.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "It\u2019s worth pointing out how rich the psychology is here. Not only do infants have to recognize that plants are different from objects of similar size, shape and color, but they need to create categories for types of plants, like basil and parsley, and distinguish \u2018eating\u2019 from just \u2018touching\u2019. It does them little good to code their observation of someone eating basil as \u2018plants are good to eat\u2019 since that might cause them to eat poisonous plants as well as basil. But, it also does them little good to narrowly code the observation as \u2018that particular sprig of basil is good to eat\u2019 since that particular sprig has just been eaten by the person they are watching. This another content bias in cultural learning.\nThis ties into the more general phenomenon of figuring out what\u2019s edible. Most Westerners learn insects aren\u2019t edible; some Asians learn that they are. This feels deeper than just someone telling you insects aren\u2019t edible and you believing them. When I was in Thailand, my guide offered me a giant cricket, telling me it was delicious. I believed him when he said it was safe to eat, I even believed him when he said it tasted good to him, but my conditioning won out \u2013 I didn\u2019t eat the cricket. There seems to be some process where a child\u2019s brain learns what is and isn\u2019t locally edible, then hard-codes it against future change.\n(Or so they say; I\u2019ve never been able to eat shrimp either.)",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "Another slot is for gender roles. By now we\u2019ve all heard the stories of progressives who try to raise their children without any exposure to gender. Their failure has sometimes been taken as evidence that gender is hard-coded. But it can\u2019t be quite that simple: some modern gender roles, like girls = pink, are far from obvious or universal. Instead, it looks like children have a hard-coded slot that gender roles go into, work hard to figure out what the local gender roles are (even if their parents are trying to confuse them), then latch onto them and don\u2019t let go.\nIn the Cultural Intelligence Hypothesis, humans live in obligate symbiosis with a culture. A brain without an associated culture is incomplete and not very useful. So the infant brain is adapted to seek out the important aspects of its local culture almost from birth and fill them into the appropriate slots in order to become whole.\nIV.\nThe next part of the book discusses post-childhood learning. This plays an important role in hunter-gatherer tribes:\nWhile hunters reach their peak strength and speed in their twenties, individual hunting success does not peak until around age 30, because success depends more on know-how and refined skills than on physical prowess.",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  },
  {
    "text": "This part of the book made most sense in the context of examples like the Inuit seal-hunting strategy which drove home just how complicated and difficult hunting-gathering was. Think less \u201cBoy Scouts\u201d and more \u201cPhD\u201d; a primitive tribesperson\u2019s life requires mastery of various complicated technologies and skills. And the difference between \u201cmediocre hunter\u201d and \u201cgreat hunter\u201d can be the difference between high status (and good mating opportunities) and low status, or even between life and death. Hunter-gatherers really want to learn the essentials of their hunter-gatherer lifestyle, and learning it is really hard. Their heuristics are:\nLearn from people who are good at things and/or widely-respected\n. If you haven\u2019t already read about the difference between dominance and prestige hierarchies, check out\nKevin Simler\u2019s blog post\non the topic. People will fear and obey authority figures like kings and chieftains, but they give a different kind of respect (\u201cprestige\u201d) to people who seem good at things. And since it\u2019s hard to figure out who\u2019s good at things (can a non-musician who wants to start learning music tell the difference between a merely good performer and one of the world\u2019s best?) most people use the heuristic of respecting the people who other people respect. Once you identify someone as respect-worthy, you strongly consider copying them in, well, everything:",
    "concept": "lesswrong",
    "source": "lesswrong-scrape"
  }
]